{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Step 1: Loading All Necessary Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data loaded successfully.\n",
      "------------------------------ \n",
      "\n",
      "# Step 2: Building Location Resolvers\n",
      "Location resolvers created.\n",
      "------------------------------ \n",
      "\n",
      "# Step 3: Loading Hugging Face Model for NER\n",
      "GPU found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER pipeline loaded.\n",
      "------------------------------ \n",
      "\n",
      "# Step 4: Hybrid Geotagging (Article & Sentence Levels)\n",
      "Found 244 unique articles for context.\n",
      "Extracted article-level locations.\n",
      "Extracted sentence-level locations.\n",
      "------------------------------ \n",
      "\n",
      "# Step 5: Applying Hierarchical Logic and Finalizing Data\n",
      "Created 1,924 final, high-precision risk-location pairs.\n",
      "Sample of final data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>risk_factor</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>location_id</th>\n",
       "      <th>location_name_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>169</td>\n",
       "      <td>2024-06-27</td>\n",
       "      <td>english</td>\n",
       "      <td>\".</td>\n",
       "      <td>without international aid</td>\n",
       "      <td>0.960398</td>\n",
       "      <td>iq_da_2</td>\n",
       "      <td>dahuk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>169</td>\n",
       "      <td>2024-06-27</td>\n",
       "      <td>english</td>\n",
       "      <td>\".</td>\n",
       "      <td>without international aid</td>\n",
       "      <td>0.960398</td>\n",
       "      <td>iq_ni_7</td>\n",
       "      <td>shingal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>169</td>\n",
       "      <td>2024-06-27</td>\n",
       "      <td>english</td>\n",
       "      <td>\".</td>\n",
       "      <td>without international aid</td>\n",
       "      <td>0.960398</td>\n",
       "      <td>iq_bg</td>\n",
       "      <td>baghdad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>169</td>\n",
       "      <td>2024-06-27</td>\n",
       "      <td>english</td>\n",
       "      <td>\".</td>\n",
       "      <td>without international aid</td>\n",
       "      <td>0.960398</td>\n",
       "      <td>iq_ni_1</td>\n",
       "      <td>akre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>169</td>\n",
       "      <td>2024-06-27</td>\n",
       "      <td>english</td>\n",
       "      <td>\".</td>\n",
       "      <td>without international aid</td>\n",
       "      <td>0.960398</td>\n",
       "      <td>iq_ts_4</td>\n",
       "      <td>kirkuk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id        date language sentence_text                risk_factor  \\\n",
       "0         169  2024-06-27  english            \".  without international aid   \n",
       "0         169  2024-06-27  english            \".  without international aid   \n",
       "0         169  2024-06-27  english            \".  without international aid   \n",
       "0         169  2024-06-27  english            \".  without international aid   \n",
       "0         169  2024-06-27  english            \".  without international aid   \n",
       "\n",
       "   confidence_score location_id location_name_english  \n",
       "0          0.960398     iq_da_2                 dahuk  \n",
       "0          0.960398     iq_ni_7               shingal  \n",
       "0          0.960398       iq_bg               baghdad  \n",
       "0          0.960398     iq_ni_1                  akre  \n",
       "0          0.960398     iq_ts_4                kirkuk  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Loading All Necessary Data\n",
    "print(\"# Step 1: Loading All Necessary Data\")\n",
    "DATA_DIR = '../data'\n",
    "MODELS_DIR = os.path.join(DATA_DIR, '03_models')\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, '02_processed')\n",
    "\n",
    "df_risk_sentences = pd.read_csv(os.path.join(MODELS_DIR, 'risk_mentions_SAMPLE_FINAL.csv'))\n",
    "df_eng = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, 'news_eng_processed.pkl'))\n",
    "df_eng['language'] = 'english'\n",
    "df_ara = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, 'news_ara_processed.pkl'))\n",
    "df_ara['language'] = 'arabic'\n",
    "if 'article_id' not in df_eng: df_eng['article_id'] = df_eng.index\n",
    "if 'article_id' not in df_ara: df_ara['article_id'] = df_ara.index\n",
    "df_articles = pd.concat([df_eng, df_ara])\n",
    "with open('../data/01_raw/id_english_location_name.pkl', 'rb') as f: eng_locations = pickle.load(f)\n",
    "with open('../data/01_raw/id_arabic_location_name.pkl', 'rb') as f: ara_locations = pickle.load(f)\n",
    "print(\"All data loaded successfully.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# Step 2: Building Location Resolvers\n",
    "print(\"# Step 2: Building Location Resolvers\")\n",
    "def create_name_to_id_lookup(location_dict):\n",
    "    lookup = {}\n",
    "    for loc_id, names in location_dict.items():\n",
    "        for name in names: lookup[name.lower()] = loc_id\n",
    "    return lookup\n",
    "location_lookup = create_name_to_id_lookup(eng_locations)\n",
    "location_lookup.update(create_name_to_id_lookup(ara_locations))\n",
    "id_to_english_name_lookup = {loc_id: names[0] for loc_id, names in eng_locations.items()}\n",
    "print(\"Location resolvers created.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# Step 3: Initialize Hugging Face NER Pipeline\n",
    "print(\"# Step 3: Loading Hugging Face Model for NER\")\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "if device == 0: print(\"GPU found.\")\n",
    "else: print(\"No GPU found.\")\n",
    "ner_pipeline = pipeline(\"ner\", model=\"Babelscape/wikineural-multilingual-ner\", aggregation_strategy=\"simple\", device=device)\n",
    "print(\"NER pipeline loaded.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# Step 4: Hybrid Geotagging (Article and Sentence Level)\n",
    "print(\"# Step 4: Hybrid Geotagging (Article & Sentence Levels)\")\n",
    "articles_with_risks_ids = df_risk_sentences['article_id'].unique()\n",
    "df_articles_with_risks = df_articles[df_articles['article_id'].isin(articles_with_risks_ids)][['article_id', 'body', 'language']].copy()\n",
    "print(f\"Found {len(df_articles_with_risks)} unique articles for context.\")\n",
    "\n",
    "def resolve_locations(text, pipeline, lookup):\n",
    "    \"\"\"Shared function to extract and resolve locations from any text.\"\"\"\n",
    "    entities = pipeline(text)\n",
    "    found_ids = set()\n",
    "    for entity in entities:\n",
    "        if entity['entity_group'] == 'LOC':\n",
    "            loc_name_lower = entity['word'].lower()\n",
    "            if loc_name_lower in lookup: found_ids.add(lookup[loc_name_lower])\n",
    "    return list(found_ids)\n",
    "\n",
    "# 4a: Get ARTICLE-level locations (the broad context)\n",
    "df_articles_with_risks['article_locations'] = df_articles_with_risks['body'].apply(\n",
    "    lambda text: resolve_locations(text, ner_pipeline, location_lookup)\n",
    ")\n",
    "print(\"Extracted article-level locations.\")\n",
    "\n",
    "# 4b: Get SENTENCE-level locations (the specific context)\n",
    "df_risk_sentences['sentence_locations'] = df_risk_sentences['sentence_text'].apply(\n",
    "    lambda text: resolve_locations(text, ner_pipeline, location_lookup)\n",
    ")\n",
    "print(\"Extracted sentence-level locations.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# Step 5: Merge and Apply Hierarchical Logic\n",
    "print(\"# Step 5: Applying Hierarchical Logic and Finalizing Data\")\n",
    "# Merge article context (language and locations) into the sentence dataframe\n",
    "df_merged = pd.merge(\n",
    "    df_risk_sentences,\n",
    "    df_articles_with_risks[['article_id', 'language', 'article_locations']],\n",
    "    on='article_id'\n",
    ")\n",
    "\n",
    "# HIERARCHICAL LOGIC: Choose the most specific location available\n",
    "def choose_locations(row):\n",
    "    # Prioritize specific (longer ID) sentence-level locations\n",
    "    sentence_specific = {loc for loc in row['sentence_locations'] if len(loc) > 2}\n",
    "    if sentence_specific:\n",
    "        return list(sentence_specific)\n",
    "    \n",
    "    # Fallback to any sentence-level locations\n",
    "    if row['sentence_locations']:\n",
    "        return row['sentence_locations']\n",
    "        \n",
    "    # Fallback to specific article-level locations\n",
    "    article_specific = {loc for loc in row['article_locations'] if len(loc) > 2}\n",
    "    if article_specific:\n",
    "        return list(article_specific)\n",
    "        \n",
    "    # Finally, use any article-level location as the last resort\n",
    "    return row['article_locations']\n",
    "\n",
    "df_merged['final_locations'] = df_merged.apply(choose_locations, axis=1)\n",
    "\n",
    "# Explode, clean, and add the English name\n",
    "df_final_exploded = df_merged.explode('final_locations').rename(columns={'final_locations': 'location_id'})\n",
    "df_final_exploded = df_final_exploded.dropna(subset=['location_id'])\n",
    "df_final_exploded['location_name_english'] = df_final_exploded['location_id'].map(id_to_english_name_lookup)\n",
    "\n",
    "# Reorder columns for final output\n",
    "final_columns = [\n",
    "    'article_id', 'date', 'language', 'sentence_text', 'risk_factor',\n",
    "    'confidence_score', 'location_id', 'location_name_english'\n",
    "]\n",
    "df_final_exploded = df_final_exploded[final_columns]\n",
    "\n",
    "print(f\"Created {len(df_final_exploded):,} final, high-precision risk-location pairs.\")\n",
    "print(\"Sample of final data:\")\n",
    "display(df_final_exploded.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
