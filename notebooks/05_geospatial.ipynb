{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Step 1: Loading All Necessary Data & SAMPLING\n",
      "--- RUNNING ON A SAMPLE of 100 rows ---\n",
      "All data loaded successfully.\n",
      "------------------------------ \n",
      "\n",
      "# Step 2: Building Location Resolvers\n",
      "Location resolvers created.\n",
      "------------------------------ \n",
      "\n",
      "# Step 3: Loading Hugging Face Model for NER\n",
      "GPU found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER pipeline loaded.\n",
      "------------------------------ \n",
      "\n",
      "# Step 4: Hybrid Geotagging (Optimized with Batching)\n",
      "Found 148 unique articles for context.\n",
      "Batch processing article-level locations...\n",
      "Extracted article-level locations.\n",
      "Batch processing sentence-level locations...\n",
      "Extracted sentence-level locations.\n",
      "------------------------------ \n",
      "\n",
      "# Step 5: Applying Hierarchical Logic and Finalizing Data\n",
      "Created 232 final, high-precision risk-location pairs.\n",
      "Sample of final data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>risk_factor</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>location_id</th>\n",
       "      <th>location_name_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26712</td>\n",
       "      <td>2024-06-27</td>\n",
       "      <td>english</td>\n",
       "      <td>\"</td>\n",
       "      <td>without international aid</td>\n",
       "      <td>0.916260</td>\n",
       "      <td>iq</td>\n",
       "      <td>iraq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26712</td>\n",
       "      <td>2024-06-27</td>\n",
       "      <td>arabic</td>\n",
       "      <td>\"</td>\n",
       "      <td>without international aid</td>\n",
       "      <td>0.916260</td>\n",
       "      <td>sy_dy_2</td>\n",
       "      <td>dayr az-zawr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26712</td>\n",
       "      <td>2024-06-27</td>\n",
       "      <td>arabic</td>\n",
       "      <td>\"</td>\n",
       "      <td>without international aid</td>\n",
       "      <td>0.916260</td>\n",
       "      <td>lb_ba_1</td>\n",
       "      <td>beirut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26712</td>\n",
       "      <td>2024-06-27</td>\n",
       "      <td>arabic</td>\n",
       "      <td>\"</td>\n",
       "      <td>without international aid</td>\n",
       "      <td>0.916260</td>\n",
       "      <td>sy_hl</td>\n",
       "      <td>aleppo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32920</td>\n",
       "      <td>2024-07-22</td>\n",
       "      <td>english</td>\n",
       "      <td>\" The West, Abulhawa reminds us, has a long an...</td>\n",
       "      <td>destructive pattern</td>\n",
       "      <td>0.995453</td>\n",
       "      <td>ps_gz_2</td>\n",
       "      <td>gaza</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id        date language  \\\n",
       "0       26712  2024-06-27  english   \n",
       "1       26712  2024-06-27   arabic   \n",
       "1       26712  2024-06-27   arabic   \n",
       "1       26712  2024-06-27   arabic   \n",
       "2       32920  2024-07-22  english   \n",
       "\n",
       "                                       sentence_text  \\\n",
       "0                                                  \"   \n",
       "1                                                  \"   \n",
       "1                                                  \"   \n",
       "1                                                  \"   \n",
       "2  \" The West, Abulhawa reminds us, has a long an...   \n",
       "\n",
       "                 risk_factor  confidence_score location_id  \\\n",
       "0  without international aid          0.916260          iq   \n",
       "1  without international aid          0.916260     sy_dy_2   \n",
       "1  without international aid          0.916260     lb_ba_1   \n",
       "1  without international aid          0.916260       sy_hl   \n",
       "2        destructive pattern          0.995453     ps_gz_2   \n",
       "\n",
       "  location_name_english  \n",
       "0                  iraq  \n",
       "1          dayr az-zawr  \n",
       "1                beirut  \n",
       "1                aleppo  \n",
       "2                  gaza  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Step 6: Saving the Final Geotagged Data\n",
      "Successfully saved 232 geotagged risk mentions to: ../data/03_models/risk_mentions_geotagged_FINAL.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Step 1: Loading All Necessary Data & SAMPLING ---\n",
    "print(\"# Step 1: Loading All Necessary Data & SAMPLING\")\n",
    "DATA_DIR = '../data'\n",
    "MODELS_DIR = os.path.join(DATA_DIR, '03_models')\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, '02_processed')\n",
    "\n",
    "df_risk_sentences = pd.read_csv(os.path.join(MODELS_DIR, 'risk_mentions_SAMPLE_FINAL.csv'))\n",
    "\n",
    "# --- THIS IS THE NEW LINE FOR SAMPLING ---\n",
    "# Process only the first 100 rows for quick testing\n",
    "df_risk_sentences = df_risk_sentences.head(100).copy()\n",
    "print(f\"--- RUNNING ON A SAMPLE of {len(df_risk_sentences)} rows ---\")\n",
    "# -----------------------------------------\n",
    "\n",
    "df_eng = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, 'news_eng_processed.pkl'))\n",
    "df_eng['language'] = 'english'\n",
    "df_ara = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, 'news_ara_processed.pkl'))\n",
    "df_ara['language'] = 'arabic'\n",
    "if 'article_id' not in df_eng: df_eng['article_id'] = df_eng.index\n",
    "if 'article_id' not in df_ara: df_ara['article_id'] = df_ara.index\n",
    "df_articles = pd.concat([df_eng, df_ara])\n",
    "with open('../data/01_raw/id_english_location_name.pkl', 'rb') as f: eng_locations = pickle.load(f)\n",
    "with open('../data/01_raw/id_arabic_location_name.pkl', 'rb') as f: ara_locations = pickle.load(f)\n",
    "print(\"All data loaded successfully.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- Step 2: Building Location Resolvers ---\n",
    "print(\"# Step 2: Building Location Resolvers\")\n",
    "def create_name_to_id_lookup(location_dict):\n",
    "    lookup = {}\n",
    "    for loc_id, names in location_dict.items():\n",
    "        for name in names: lookup[name.lower()] = loc_id\n",
    "    return lookup\n",
    "location_lookup = create_name_to_id_lookup(eng_locations)\n",
    "location_lookup.update(create_name_to_id_lookup(ara_locations))\n",
    "id_to_english_name_lookup = {loc_id: names[0] for loc_id, names in eng_locations.items()}\n",
    "print(\"Location resolvers created.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- Step 3: Initialize Hugging Face NER Pipeline ---\n",
    "print(\"# Step 3: Loading Hugging Face Model for NER\")\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "if device == 0: print(\"GPU found.\")\n",
    "else: print(\"No GPU found.\")\n",
    "ner_pipeline = pipeline(\"ner\", model=\"Babelscape/wikineural-multilingual-ner\", aggregation_strategy=\"simple\", device=device)\n",
    "print(\"NER pipeline loaded.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- Step 4: Hybrid Geotagging (Optimized with Batching) ---\n",
    "print(\"# Step 4: Hybrid Geotagging (Optimized with Batching)\")\n",
    "articles_with_risks_ids = df_risk_sentences['article_id'].unique()\n",
    "df_articles_with_risks = df_articles[df_articles['article_id'].isin(articles_with_risks_ids)][['article_id', 'body', 'language']].copy()\n",
    "print(f\"Found {len(df_articles_with_risks)} unique articles for context.\")\n",
    "\n",
    "# Helper function to resolve locations from pre-computed entities\n",
    "def resolve_locations_from_entities(entities, lookup):\n",
    "    found_ids = set()\n",
    "    for entity in entities:\n",
    "        if entity['entity_group'] == 'LOC':\n",
    "            loc_name_lower = entity['word'].lower()\n",
    "            if loc_name_lower in lookup:\n",
    "                found_ids.add(lookup[loc_name_lower])\n",
    "    return list(found_ids)\n",
    "\n",
    "# 4a: BATCH process ARTICLE-level locations\n",
    "print(\"Batch processing article-level locations...\")\n",
    "article_bodies = df_articles_with_risks['body'].fillna('').tolist()\n",
    "article_entities_list = ner_pipeline(article_bodies, batch_size=128)\n",
    "article_locations = [resolve_locations_from_entities(entities, location_lookup) for entities in article_entities_list]\n",
    "df_articles_with_risks['article_locations'] = article_locations\n",
    "print(\"Extracted article-level locations.\")\n",
    "\n",
    "# 4b: BATCH process SENTENCE-level locations\n",
    "print(\"Batch processing sentence-level locations...\")\n",
    "sentence_texts = df_risk_sentences['sentence_text'].fillna('').tolist()\n",
    "sentence_entities_list = ner_pipeline(sentence_texts, batch_size=128)\n",
    "sentence_locations = [resolve_locations_from_entities(entities, location_lookup) for entities in sentence_entities_list]\n",
    "df_risk_sentences['sentence_locations'] = sentence_locations\n",
    "print(\"Extracted sentence-level locations.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- Step 5: Merge and Apply Hierarchical Logic ---\n",
    "print(\"# Step 5: Applying Hierarchical Logic and Finalizing Data\")\n",
    "df_merged = pd.merge(\n",
    "    df_risk_sentences,\n",
    "    df_articles_with_risks[['article_id', 'language', 'article_locations']],\n",
    "    on='article_id'\n",
    ")\n",
    "\n",
    "def choose_locations(row):\n",
    "    sentence_specific = {loc for loc in row['sentence_locations'] if len(loc) > 2}\n",
    "    if sentence_specific: return list(sentence_specific)\n",
    "    if row['sentence_locations']: return row['sentence_locations']\n",
    "    article_specific = {loc for loc in row['article_locations'] if len(loc) > 2}\n",
    "    if article_specific: return list(article_specific)\n",
    "    return row['article_locations']\n",
    "\n",
    "df_merged['final_locations'] = df_merged.apply(choose_locations, axis=1)\n",
    "df_final_exploded = df_merged.explode('final_locations').rename(columns={'final_locations': 'location_id'})\n",
    "df_final_exploded = df_final_exploded.dropna(subset=['location_id'])\n",
    "df_final_exploded['location_name_english'] = df_final_exploded['location_id'].map(id_to_english_name_lookup)\n",
    "\n",
    "final_columns = [\n",
    "    'article_id', 'date', 'language', 'sentence_text', 'risk_factor',\n",
    "    'confidence_score', 'location_id', 'location_name_english'\n",
    "]\n",
    "df_final_exploded = df_final_exploded[final_columns]\n",
    "\n",
    "print(f\"Created {len(df_final_exploded):,} final, high-precision risk-location pairs.\")\n",
    "print(\"Sample of final data:\")\n",
    "display(df_final_exploded.head())\n",
    "\n",
    "\n",
    "# --- Step 6: Save the Geotagged Results ---\n",
    "print(\"\\n# Step 6: Saving the Final Geotagged Data\")\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, '03_models')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "output_path = os.path.join(OUTPUT_DIR, 'risk_mentions_geotagged_FINAL.csv')\n",
    "df_final_exploded.to_csv(output_path, index=False)\n",
    "print(f\"Successfully saved {len(df_final_exploded):,} geotagged risk mentions to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
