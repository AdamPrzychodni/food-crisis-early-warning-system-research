{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 10:04:50,946 - INFO - GPU available: NVIDIA L4\n",
      "2025-09-13 10:04:50,948 - INFO - Starting news article processing pipeline\n",
      "2025-09-13 10:04:50,949 - INFO - Geographic filtering is ENABLED\n",
      "2025-09-13 10:05:02,855 - INFO - Loaded 86660 English and 85511 Arabic articles\n",
      "2025-09-13 10:05:02,895 - INFO - Loaded 918 unique location aliases\n",
      "2025-09-13 10:05:02,897 - INFO - Loading NER model: Babelscape/wikineural-multilingual-ner\n",
      "Device set to use cuda:0\n",
      "2025-09-13 10:05:03,792 - INFO - NER pipeline initialized successfully\n",
      "2025-09-13 10:05:04,069 - INFO - Running NER on 172,171 articles...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 379\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# Usage example with geographic filtering enabled (default)\u001b[39;00m\n\u001b[1;32m    373\u001b[0m processor \u001b[38;5;241m=\u001b[39m NewsArticleProcessor(\n\u001b[1;32m    374\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    375\u001b[0m     filter_by_geography\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Set to False to disable filtering\u001b[39;00m\n\u001b[1;32m    376\u001b[0m     max_text_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m,\n\u001b[1;32m    377\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m\n\u001b[1;32m    378\u001b[0m )\n\u001b[0;32m--> 379\u001b[0m df_eng_processed, df_ara_processed \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# Example of accessing processed data\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_eng_processed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m English articles\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 356\u001b[0m, in \u001b[0;36mNewsArticleProcessor.run_pipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeographic filtering is DISABLED\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# Load data (with geographic filtering if enabled)\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m df_eng, df_ara \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# Process each dataset\u001b[39;00m\n\u001b[1;32m    359\u001b[0m df_eng \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_dataframe(df_eng, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 201\u001b[0m, in \u001b[0;36mNewsArticleProcessor.load_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_locations()\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_ner()\n\u001b[0;32m--> 201\u001b[0m     df_combined \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_by_geography\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_combined\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Split back into language-specific dataframes\u001b[39;00m\n\u001b[1;32m    204\u001b[0m df_eng_filtered \u001b[38;5;241m=\u001b[39m df_combined[df_combined[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[0;32mIn[1], line 143\u001b[0m, in \u001b[0;36mNewsArticleProcessor._filter_by_geography\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    140\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning NER on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts_to_process)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m articles...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Extract entities\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m all_entities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mner_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts_to_process\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Check for relevant locations\u001b[39;00m\n\u001b[1;32m    146\u001b[0m relevance_mask \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:279\u001b[0m, in \u001b[0;36mTokenClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset_mapping:\n\u001b[1;32m    277\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m offset_mapping\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/base.py:1448\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1445\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1446\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1447\u001b[0m     )\n\u001b[0;32m-> 1448\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:127\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    126\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 127\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:396\u001b[0m, in \u001b[0;36mTokenClassificationPipeline.postprocess\u001b[0;34m(self, all_outputs, aggregation_strategy, ignore_labels)\u001b[0m\n\u001b[1;32m    393\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    394\u001b[0m     offset_mapping \u001b[38;5;241m=\u001b[39m offset_mapping\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mif\u001b[39;00m offset_mapping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m pre_entities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather_pre_entities\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffset_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspecial_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mword_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mword_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mword_to_chars_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mword_to_chars_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m grouped_entities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate(pre_entities, aggregation_strategy)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# Filter anything that is in self.ignore_labels\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:460\u001b[0m, in \u001b[0;36mTokenClassificationPipeline.gather_pre_entities\u001b[0;34m(self, sentence, input_ids, scores, offset_mapping, special_tokens_mask, aggregation_strategy, word_ids, word_to_chars_map)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m special_tokens_mask[idx]:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 460\u001b[0m word \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset_mapping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     start_ind, end_ind \u001b[38;5;241m=\u001b[39m offset_mapping[idx]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "News article processing pipeline for multilingual text data.\n",
    "Filters by geographic relevance, then cleans and tokenizes English and Arabic news articles.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Set, Tuple\n",
    "from transformers import pipeline, Pipeline\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class NewsArticleProcessor:\n",
    "    \"\"\"Processes and cleans news articles in multiple languages with geographic filtering.\"\"\"\n",
    "    \n",
    "    # Regex patterns as class constants for reusability\n",
    "    HTML_PATTERN = re.compile(r'<.*?>')\n",
    "    URL_PATTERN = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    WHITESPACE_PATTERN = re.compile(r'\\s+')\n",
    "    SENTENCE_SPLIT_PATTERN = re.compile(r'(?<=[.!?۔])\\s+')\n",
    "    LETTER_PATTERN = re.compile(r'[a-zA-Zء-ي]')\n",
    "    \n",
    "    def __init__(self, data_dir: str = '../data', \n",
    "                 filter_by_geography: bool = True,\n",
    "                 max_text_length: int = 2000,\n",
    "                 batch_size: int = 128,\n",
    "                 ner_model: str = \"Babelscape/wikineural-multilingual-ner\"):\n",
    "        \"\"\"\n",
    "        Initialize the processor with data directory path and filtering options.\n",
    "        \n",
    "        Args:\n",
    "            data_dir: Root directory containing raw and processed data folders\n",
    "            filter_by_geography: Whether to apply geographic filtering\n",
    "            max_text_length: Maximum text length for NER processing\n",
    "            batch_size: Batch size for NER processing\n",
    "            ner_model: Model name for NER pipeline\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.raw_dir = self.data_dir / '01_raw'\n",
    "        self.processed_dir = self.data_dir / '02_processed'\n",
    "        \n",
    "        # Geographic filtering settings\n",
    "        self.filter_by_geography = filter_by_geography\n",
    "        self.max_text_length = max_text_length\n",
    "        self.batch_size = batch_size\n",
    "        self.ner_model = ner_model\n",
    "        \n",
    "        # Initialize components\n",
    "        self.location_lookup: Dict[str, str] = {}\n",
    "        self.ner_pipeline: Optional[Pipeline] = None\n",
    "        self.device = self._get_device()\n",
    "        \n",
    "        # Ensure processed directory exists\n",
    "        self.processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def _get_device(self) -> int:\n",
    "        \"\"\"Determine the best available device for processing.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            logger.info(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "            return 0\n",
    "        else:\n",
    "            logger.info(\"No GPU found, using CPU\")\n",
    "            return -1\n",
    "    \n",
    "    def _load_locations(self) -> None:\n",
    "        \"\"\"Load location dictionaries for geographic filtering.\"\"\"\n",
    "        if not self.filter_by_geography:\n",
    "            return\n",
    "            \n",
    "        eng_path = self.raw_dir / 'id_english_location_name.pkl'\n",
    "        ara_path = self.raw_dir / 'id_arabic_location_name.pkl'\n",
    "        \n",
    "        try:\n",
    "            with open(eng_path, 'rb') as f:\n",
    "                eng_locations = pickle.load(f)\n",
    "            with open(ara_path, 'rb') as f:\n",
    "                ara_locations = pickle.load(f)\n",
    "            \n",
    "            # Build lookup dictionary\n",
    "            for location_dict in [eng_locations, ara_locations]:\n",
    "                for loc_id, names in location_dict.items():\n",
    "                    for name in names:\n",
    "                        self.location_lookup[name.lower()] = loc_id\n",
    "            \n",
    "            logger.info(f\"Loaded {len(self.location_lookup):,} unique location aliases\")\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            logger.error(f\"Location file not found: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _initialize_ner(self) -> None:\n",
    "        \"\"\"Initialize NER pipeline for geographic filtering.\"\"\"\n",
    "        if not self.filter_by_geography:\n",
    "            return\n",
    "            \n",
    "        logger.info(f\"Loading NER model: {self.ner_model}\")\n",
    "        \n",
    "        try:\n",
    "            self.ner_pipeline = pipeline(\n",
    "                \"ner\",\n",
    "                model=self.ner_model,\n",
    "                aggregation_strategy=\"simple\",\n",
    "                device=self.device\n",
    "            )\n",
    "            logger.info(\"NER pipeline initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load NER model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _filter_by_geography(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Filter articles containing target geographic locations.\n",
    "        \n",
    "        Args:\n",
    "            df: Combined dataframe of articles\n",
    "            \n",
    "        Returns:\n",
    "            Filtered dataframe with location metadata\n",
    "        \"\"\"\n",
    "        if not self.filter_by_geography:\n",
    "            logger.info(\"Geographic filtering disabled, using all articles\")\n",
    "            df['matched_locations'] = [[] for _ in range(len(df))]\n",
    "            return df\n",
    "        \n",
    "        # Prepare texts for NER\n",
    "        article_bodies = df['body'].fillna('').tolist()\n",
    "        texts_to_process = [text[:self.max_text_length] for text in article_bodies]\n",
    "        \n",
    "        logger.info(f\"Running NER on {len(texts_to_process):,} articles...\")\n",
    "        \n",
    "        # Extract entities\n",
    "        all_entities = self.ner_pipeline(texts_to_process, batch_size=self.batch_size)\n",
    "        \n",
    "        # Check for relevant locations\n",
    "        relevance_mask = []\n",
    "        location_matches = []\n",
    "        \n",
    "        for article_entities in tqdm(all_entities, desc=\"Checking location relevance\"):\n",
    "            found_locations = []\n",
    "            \n",
    "            for entity in article_entities:\n",
    "                if (entity.get('entity_group') == 'LOC' and \n",
    "                    entity.get('word', '').lower() in self.location_lookup):\n",
    "                    found_locations.append(entity['word'])\n",
    "            \n",
    "            relevance_mask.append(len(found_locations) > 0)\n",
    "            location_matches.append(found_locations)\n",
    "        \n",
    "        # Add metadata and filter\n",
    "        df['matched_locations'] = location_matches\n",
    "        df_filtered = df[relevance_mask].copy()\n",
    "        \n",
    "        logger.info(f\"Geographic filtering complete:\")\n",
    "        logger.info(f\"  - Original articles: {len(df):,}\")\n",
    "        logger.info(f\"  - Relevant articles: {len(df_filtered):,}\")\n",
    "        logger.info(f\"  - Retention rate: {len(df_filtered)/len(df)*100:.1f}%\")\n",
    "        \n",
    "        return df_filtered\n",
    "    \n",
    "    def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Load English and Arabic news datasets.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (english_df, arabic_df)\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If dataset files don't exist\n",
    "        \"\"\"\n",
    "        eng_path = self.raw_dir / 'news-articles-eng.csv'\n",
    "        ara_path = self.raw_dir / 'news-articles-ara.csv'\n",
    "        \n",
    "        try:\n",
    "            df_eng = pd.read_csv(eng_path)\n",
    "            df_ara = pd.read_csv(ara_path)\n",
    "            \n",
    "            # Add language column for tracking\n",
    "            df_eng['language'] = 'english'\n",
    "            df_ara['language'] = 'arabic'\n",
    "            \n",
    "            logger.info(f\"Loaded {len(df_eng)} English and {len(df_ara)} Arabic articles\")\n",
    "            \n",
    "            # Combine for geographic filtering\n",
    "            df_combined = pd.concat([df_eng, df_ara], ignore_index=True)\n",
    "            \n",
    "            # Apply geographic filtering if enabled\n",
    "            if self.filter_by_geography:\n",
    "                self._load_locations()\n",
    "                self._initialize_ner()\n",
    "                df_combined = self._filter_by_geography(df_combined)\n",
    "            \n",
    "            # Split back into language-specific dataframes\n",
    "            df_eng_filtered = df_combined[df_combined['language'] == 'english'].copy()\n",
    "            df_ara_filtered = df_combined[df_combined['language'] == 'arabic'].copy()\n",
    "            \n",
    "            # Remove the language column as it's no longer needed\n",
    "            df_eng_filtered = df_eng_filtered.drop('language', axis=1)\n",
    "            df_ara_filtered = df_ara_filtered.drop('language', axis=1)\n",
    "            \n",
    "            return df_eng_filtered, df_ara_filtered\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            logger.error(f\"Dataset file not found: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data: {e}\")\n",
    "            raise\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_text(text: Optional[str]) -> str:\n",
    "        \"\"\"\n",
    "        Clean raw text by removing HTML tags, URLs, and normalizing whitespace.\n",
    "        \n",
    "        Args:\n",
    "            text: Raw text string to clean\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned text string\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = NewsArticleProcessor.HTML_PATTERN.sub('', text)\n",
    "        # Remove URLs\n",
    "        text = NewsArticleProcessor.URL_PATTERN.sub('', text)\n",
    "        # Normalize whitespace\n",
    "        text = NewsArticleProcessor.WHITESPACE_PATTERN.sub(' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize_sentences(text: Optional[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into sentences using regex and filter non-textual results.\n",
    "        \n",
    "        Args:\n",
    "            text: Text to tokenize\n",
    "            \n",
    "        Returns:\n",
    "            List of sentence strings\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str) or not text:\n",
    "            return []\n",
    "        \n",
    "        # Split on sentence-ending punctuation\n",
    "        sentences = NewsArticleProcessor.SENTENCE_SPLIT_PATTERN.split(text)\n",
    "        \n",
    "        # Filter empty strings and non-textual content\n",
    "        valid_sentences = [\n",
    "            s for s in sentences \n",
    "            if s and NewsArticleProcessor.LETTER_PATTERN.search(s)\n",
    "        ]\n",
    "        \n",
    "        return valid_sentences\n",
    "    \n",
    "    def process_dataframe(self, df: pd.DataFrame, language: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply cleaning and tokenization to a dataframe.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with 'body' column containing article text\n",
    "            language: Language identifier for logging\n",
    "            \n",
    "        Returns:\n",
    "            Processed DataFrame with cleaned text and sentences\n",
    "        \"\"\"\n",
    "        logger.info(f\"Processing {language} articles...\")\n",
    "        \n",
    "        # Clean text\n",
    "        df['body_cleaned'] = df['body'].apply(self.clean_text)\n",
    "        logger.info(f\"Cleaned {len(df)} {language} articles\")\n",
    "        \n",
    "        # Tokenize into sentences\n",
    "        df['sentences'] = df['body_cleaned'].apply(self.tokenize_sentences)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        sentence_counts = df['sentences'].apply(len)\n",
    "        logger.info(f\"Tokenized {language} articles: \"\n",
    "                   f\"avg {sentence_counts.mean():.1f} sentences per article\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def save_processed_data(self, df_eng: pd.DataFrame, df_ara: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Save processed dataframes to pickle files.\n",
    "        \n",
    "        Args:\n",
    "            df_eng: Processed English dataframe\n",
    "            df_ara: Processed Arabic dataframe\n",
    "        \"\"\"\n",
    "        eng_path = self.processed_dir / 'news_eng_processed.pkl'\n",
    "        ara_path = self.processed_dir / 'news_ara_processed.pkl'\n",
    "        \n",
    "        try:\n",
    "            df_eng.to_pickle(eng_path)\n",
    "            df_ara.to_pickle(ara_path)\n",
    "            logger.info(f\"Saved English data to: {eng_path}\")\n",
    "            logger.info(f\"Saved Arabic data to: {ara_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving processed data: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def display_sample(self, df: pd.DataFrame, num_sentences: int = 3) -> None:\n",
    "        \"\"\"\n",
    "        Display sample processed sentences for verification.\n",
    "        \n",
    "        Args:\n",
    "            df: Processed dataframe\n",
    "            num_sentences: Number of sentences to display\n",
    "        \"\"\"\n",
    "        if df.empty or 'sentences' not in df.columns:\n",
    "            logger.warning(\"No sentences to display\")\n",
    "            return\n",
    "        \n",
    "        first_article = df.iloc[0]\n",
    "        sentences = first_article['sentences']\n",
    "        \n",
    "        print(f\"\\n--- Sample Tokenization Results ---\")\n",
    "        print(f\"Article split into {len(sentences)} sentences\")\n",
    "        print(f\"First {min(num_sentences, len(sentences))} sentences:\")\n",
    "        \n",
    "        for i, sentence in enumerate(sentences[:num_sentences], 1):\n",
    "            print(f\"  {i}. {sentence}\")\n",
    "        \n",
    "        # Show matched locations if available\n",
    "        if 'matched_locations' in df.columns and first_article['matched_locations']:\n",
    "            print(f\"\\nMatched locations: {', '.join(first_article['matched_locations'])}\")\n",
    "    \n",
    "    def run_pipeline(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Execute the complete processing pipeline.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of processed (english_df, arabic_df)\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting news article processing pipeline\")\n",
    "        \n",
    "        if self.filter_by_geography:\n",
    "            logger.info(\"Geographic filtering is ENABLED\")\n",
    "        else:\n",
    "            logger.info(\"Geographic filtering is DISABLED\")\n",
    "        \n",
    "        # Load data (with geographic filtering if enabled)\n",
    "        df_eng, df_ara = self.load_data()\n",
    "        \n",
    "        # Process each dataset\n",
    "        df_eng = self.process_dataframe(df_eng, \"English\")\n",
    "        df_ara = self.process_dataframe(df_ara, \"Arabic\")\n",
    "        \n",
    "        # Display sample for verification\n",
    "        self.display_sample(df_eng)\n",
    "        \n",
    "        # Save processed data\n",
    "        self.save_processed_data(df_eng, df_ara)\n",
    "        \n",
    "        logger.info(\"Pipeline completed successfully\")\n",
    "        return df_eng, df_ara\n",
    "\n",
    "\n",
    "# Usage example with geographic filtering enabled (default)\n",
    "processor = NewsArticleProcessor(\n",
    "    data_dir='../data',\n",
    "    filter_by_geography=True,  # Set to False to disable filtering\n",
    "    max_text_length=2000,\n",
    "    batch_size=128\n",
    ")\n",
    "df_eng_processed, df_ara_processed = processor.run_pipeline()\n",
    "\n",
    "# Example of accessing processed data\n",
    "print(f\"\\nProcessed {len(df_eng_processed)} English articles\")\n",
    "print(f\"Processed {len(df_ara_processed)} Arabic articles\")\n",
    "\n",
    "# Check if location data is available\n",
    "if 'matched_locations' in df_eng_processed.columns:\n",
    "    total_locations = sum(len(locs) for locs in df_eng_processed['matched_locations'])\n",
    "    print(f\"Total location matches in English articles: {total_locations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
