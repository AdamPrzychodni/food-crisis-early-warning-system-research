{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This notebook will handle the essential preprocessing steps and now includes the crucial pre-selection filter to reduce the dataset size for the next stage.\n",
    "\n",
    "The steps are:\n",
    "1. Load Data: Load the raw news articles.\n",
    "2. Clean Text: Remove HTML, URLs, etc.\n",
    "3. Tokenize Sentences: Split text into sentences.\n",
    "4. Pre-selection Filtering: A new step to identify and keep only articles that contain potential risk keywords.\n",
    "5. Save Processed & Filtered Data: Save the final, smaller datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Data\n",
    "\n",
    "First, let's load the two news article datasets from the previous EDA stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Load the datasets ---\n",
    "DATA_DIR = '../data'\n",
    "df_eng = pd.read_csv(os.path.join(DATA_DIR, '01_raw/news-articles-eng.csv'))\n",
    "df_ara = pd.read_csv(os.path.join(DATA_DIR, '01_raw/news-articles-ara.csv'))\n",
    "\n",
    "print(\"Data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. News Article Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning news articles...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning complete.\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans raw text by removing HTML tags, URLs, and normalizing whitespace.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "print(\"Cleaning news articles...\")\n",
    "df_eng['body_cleaned'] = df_eng['body'].apply(clean_text)\n",
    "df_ara['body_cleaned'] = df_ara['body'].apply(clean_text)\n",
    "print(\"Cleaning complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentence Tokenization\n",
    "\n",
    "We will use a regular expression to split the text into sentences. This method looks for common sentence-ending punctuation (. ! ?) followed by a space, which is a robust approach for news text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing English articles into sentences...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Arabic articles into sentences...\n",
      "\n",
      "Sentence tokenization complete.\n",
      "\n",
      "--- Example of Regex Sentence Tokenization ---\n",
      "Article body has been split into 124 sentences.\n",
      "First 3 sentences:\n",
      "- Hussam al-Mahmoud | Yamen Moghrabi | Hassan Ibrahim On October 7, 2023, the world and the Middle East awoke to the drums of war beating in the Gaza Strip.\n",
      "- Over time, it turned into a reality that American efforts, Qatari and Egyptian mediation, condemnations, statements, summits, and conferences could not stop.\n",
      "- While Israel continues its war in the besieged Gaza Strip, attention is turning towards the potential outbreak of another war.\n"
     ]
    }
   ],
   "source": [
    "def regex_sent_tokenize(text):\n",
    "    \"\"\"\n",
    "    Splits text into sentences using a regular expression.\n",
    "    This is a dependency-free alternative to nltk.sent_tokenize.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return []\n",
    "    # Split on sentence-ending punctuation followed by a space or end of string\n",
    "    # The regex uses a \"positive lookbehind\" to keep the punctuation with the sentence.\n",
    "    sentences = re.split(r'(?<=[.!?۔])\\s+', text)\n",
    "    # Filter out any empty strings that might result from the split\n",
    "    return [s for s in sentences if s]\n",
    "\n",
    "# --- Apply the new, reliable tokenizer ---\n",
    "print(\"Tokenizing English articles into sentences...\")\n",
    "df_eng['sentences'] = df_eng['body_cleaned'].apply(regex_sent_tokenize)\n",
    "\n",
    "print(\"Tokenizing Arabic articles into sentences...\")\n",
    "df_ara['sentences'] = df_ara['body_cleaned'].apply(regex_sent_tokenize)\n",
    "\n",
    "print(\"\\nSentence tokenization complete.\")\n",
    "\n",
    "# --- Display a sample ---\n",
    "print(\"\\n--- Example of Regex Sentence Tokenization ---\")\n",
    "print(f\"Article body has been split into {len(df_eng['sentences'].iloc[0])} sentences.\")\n",
    "print(\"First 3 sentences:\")\n",
    "for sentence in df_eng['sentences'].iloc[0][:3]:\n",
    "    print(f\"- {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pre-selection Filtering Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading semantic search model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9ae2010c004c3fb3ba490c388045e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fb8318e26f4ace9e3f60aa832473d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1b78bacdc14caaab3991d9146e3fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1223e304e14b2f9f4ed7c807f6d716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9967607954fc41f8935ec27f247f0752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b339c8fe944d2a958ef511de0649e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4c0f82081d4b7c9774efa695676b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a3fbd4e31246d8af4339bb23f5764e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d11c74eeb7438bbe0ae04978d894ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75493bd317a2462ba409ffcba7bc6eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding risk factors...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ee6be8ab84419389060c4b053a0939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5381 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing semantic search to find relevant articles...\n",
      "\n",
      "Semantic pre-selection filtering complete.\n",
      "  - English: Kept 44 of 86,660 articles.\n",
      "  - Arabic:  Kept 106 of 85,511 articles.\n"
     ]
    }
   ],
   "source": [
    "# Load a lightweight, multilingual model designed for fast semantic search\n",
    "# This model is highly effective and much faster than the large classifier.\n",
    "print(\"Loading semantic search model...\")\n",
    "embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Load the English risk factors\n",
    "risk_factors_path = os.path.join(DATA_DIR, '01_raw/risk-factors.xlsx')\n",
    "df_risk_factors = pd.read_excel(risk_factors_path)\n",
    "risk_factor_labels_eng = df_risk_factors['risk_factor_english'].tolist()\n",
    "\n",
    "# Convert the risk factors into meaning vectors (embeddings)\n",
    "print(\"Encoding risk factors...\")\n",
    "risk_factor_embeddings = embedder.encode(risk_factor_labels_eng, convert_to_tensor=True)\n",
    "\n",
    "# Combine all article bodies into a single list\n",
    "all_article_bodies = pd.concat([df_eng['body_cleaned'], df_ara['body_cleaned']], ignore_index=True)\n",
    "corpus_embeddings = embedder.encode(all_article_bodies.tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# Perform the high-speed semantic search\n",
    "print(\"Performing semantic search to find relevant articles...\")\n",
    "# This will find the top 1 most similar risk factor for each article.\n",
    "# We get back a list of lists, each containing a dictionary with 'corpus_id', 'score'.\n",
    "hits = util.semantic_search(risk_factor_embeddings, corpus_embeddings, top_k=1)\n",
    "\n",
    "# The result 'hits' is for each risk factor. We need to find the max similarity for each article.\n",
    "max_similarity_scores = torch.zeros(len(all_article_bodies))\n",
    "for hit_list in hits:\n",
    "    for hit in hit_list:\n",
    "        corpus_id = hit['corpus_id']\n",
    "        score = hit['score']\n",
    "        if score > max_similarity_scores[corpus_id]:\n",
    "            max_similarity_scores[corpus_id] = score\n",
    "\n",
    "# --- Filter articles based on a similarity threshold ---\n",
    "# This threshold is a tunable parameter. 0.35 is a good starting point.\n",
    "SIMILARITY_THRESHOLD = 0.35\n",
    "relevant_indices = (max_similarity_scores > SIMILARITY_THRESHOLD).nonzero().squeeze().tolist()\n",
    "if isinstance(relevant_indices, int): # Handle case of a single match\n",
    "    relevant_indices = [relevant_indices]\n",
    "\n",
    "# Create a combined dataframe to easily filter\n",
    "df_all = pd.concat([df_eng, df_ara], ignore_index=True)\n",
    "df_filtered = df_all.iloc[relevant_indices].copy()\n",
    "\n",
    "# Split back into English and Arabic\n",
    "df_eng_filtered = df_filtered[df_filtered['lang'] == 'eng']\n",
    "df_ara_filtered = df_filtered[df_filtered['lang'] == 'ara']\n",
    "\n",
    "print(\"\\nSemantic pre-selection filtering complete.\")\n",
    "print(f\"  - English: Kept {len(df_eng_filtered):,} of {len(df_eng):,} articles.\")\n",
    "print(f\"  - Arabic:  Kept {len(df_ara_filtered):,} of {len(df_ara):,} articles.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed English data saved to: ../data/02_processed/news_eng_processed.pkl\n",
      "Processed Arabic data saved to: ../data/02_processed/news_ara_processed.pkl\n"
     ]
    }
   ],
   "source": [
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, '02_processed')\n",
    "output_path_eng = os.path.join(PROCESSED_DATA_DIR, 'news_eng_processed_filtered.pkl')\n",
    "output_path_ara = os.path.join(PROCESSED_DATA_DIR, 'news_ara_processed_filtered.pkl')\n",
    "\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Save the smaller, filtered dataframes\n",
    "df_eng_filtered.to_pickle(output_path_eng)\n",
    "df_ara_filtered.to_pickle(output_path_ara)\n",
    "\n",
    "print(f\"\\nProcessed and FILTERED English data saved to: {output_path_eng}\")\n",
    "print(f\"Processed and FILTERED Arabic data saved to: {output_path_ara}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additonal translation risk factors to Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping pre-translated Arabic risk factors...\n",
      "\n",
      "--- Translation Complete ---\n",
      "Sample of the updated DataFrame:\n",
      "  risk_factor_english risk_factor_arabic\n",
      "0  massive starvation        مجاعة هائلة\n",
      "1          rinderpest         طاعون بقري\n",
      "2     scanty rainfall         شح الأمطار\n",
      "3         dysfunction          خلل وظيفي\n",
      "4                rise             ارتفاع\n",
      "\n",
      "Successfully saved the complete, translated risk factors to: ../data/01_raw/risk-factors-translated.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. Load your original risk factors file ---\n",
    "DATA_DIR = '../data'\n",
    "risk_factors_path = os.path.join(DATA_DIR, '01_raw/risk-factors.xlsx')\n",
    "df_risk = pd.read_excel(risk_factors_path)\n",
    "\n",
    "# --- 2. Use the pre-translated dictionary (fast and reliable) ---\n",
    "print(\"Mapping pre-translated Arabic risk factors...\")\n",
    "risk_factor_translations_ara = {\n",
    "    'massive starvation': 'مجاعة هائلة', 'rinderpest': 'طاعون بقري', 'scanty rainfall': 'شح الأمطار', 'dysfunction': 'خلل وظيفي', 'rise': 'ارتفاع', 'mass displacement': 'نزوح جماعي', 'conflict': 'صراع', 'hunger': 'جوع', 'malnutrition': 'سوء تغذية', 'drought': 'جفاف', 'locust': 'جراد', 'insecurity': 'انعدام الأمن', 'violence': 'عنف', 'poverty': 'فقر', 'displacement': 'نزوح', 'disease': 'مرض', 'death': 'موت', 'disaster': 'كارثة', 'crisis': 'أزمة', 'famine': 'مجاعة', 'emergency': 'طوارئ', 'shortage': 'نقص', 'cholera': 'كوليرا', 'malaria': 'ملاريا', 'measles': 'حصبة', 'typhoid': 'تيفوئيد', 'ebola': 'إيبولا', 'hiv': 'فيروس نقص المناعة البشرية', 'aids': 'الإيدز', 'tuberculosis': 'سل', 'diarrhea': 'إسهال', 'undernutrition': 'نقص التغذية', 'food prices': 'أسعار المواد الغذائية', 'inflation': 'تضخم', 'economic collapse': 'انهيار اقتصادي', 'currency devaluation': 'تخفيض قيمة العملة', 'unemployment': 'بطالة', 'corruption': 'فساد', 'sanctions': 'عقوبات', 'blockade': 'حصار', 'looting': 'نهب', 'theft': 'سرقة', 'crime': 'جريمة', 'terrorism': 'إرهاب', 'insurgency': 'تمرد', 'civil war': 'حرب أهلية', 'war': 'حرب', 'bombing': 'قصف', 'airstrike': 'غارة جوية', 'shelling': 'قصف مدفعي', 'gunfire': 'إطلاق نار', 'explosion': 'انفجار', 'massacre': 'مذبحة', 'genocide': 'إبادة جماعية', 'ethnic cleansing': 'تطهير عرقي', 'torture': 'تعذيب', 'rape': 'اغتصاب', 'abduction': 'اختطاف', 'kidnapping': 'خطف', 'hostage': 'رهينة', 'assassination': 'اغتيال', 'coup': 'انقلاب', 'political instability': 'عدم استقرار سياسي', 'protest': 'احتجاج', 'riot': 'شغب', 'curfew': 'حظر تجول', 'state of emergency': 'حالة طوارئ', 'martial law': 'أحكام عرفية', 'election violence': 'عنف انتخابي', 'border closure': 'إغلاق الحدود', 'refugee': 'لاجئ', 'asylum seeker': 'طالب لجوء', 'internally displaced person': 'نازح داخلي', 'migrant': 'مهاجر', 'human trafficking': 'اتجار بالبشر', 'smuggling': 'تهريب', 'flood': 'فيضان', 'hurricane': 'إعصار', 'cyclone': 'إعصار', 'typhoon': 'إعصار', 'earthquake': 'زلزال', 'tsunami': 'تسونامي', 'volcano': 'بركان', 'landslide': 'انهيار أرضي', 'avalanche': 'انهيار ثلجي', 'wildfire': 'حرائق غابات', 'heatwave': 'موجة حر', 'cold wave': 'موجة برد', 'hailstorm': 'عاصفة برد', 'tornado': 'إعصار', 'storm': 'عاصفة', 'monsoon': 'موسم الأمطار', 'crop failure': 'فشل المحاصيل', 'harvest failure': 'فشل الحصاد', 'livestock death': 'نفوق الماشية', 'water shortage': 'نقص المياه', 'power outage': 'انقطاع التيار الكهربائي', 'fuel shortage': 'نقص الوقود', 'road closure': 'إغلاق الطرق', 'infrastructure damage': 'أضرار في البنية التحتية', 'hospital closure': 'إغلاق المستشفيات', 'school closure': 'إغلاق المدارس', 'market closure': 'إغلاق الأسواق', 'aid shortage': 'نقص المساعدات', 'aid worker killed': 'مقتل عامل إغاثة', 'aid worker abducted': 'اختطاف عامل إغاثة', 'ngo withdrawal': 'انسحاب المنظمات غير الحكومية', 'un withdrawal': 'انسحاب الأمم المتحدة', 'peacekeeping mission': 'بعثة حفظ السلام', 'ceasefire violation': 'انتهاك وقف إطلاق النار', 'failed state': 'دولة فاشلة', 'anarchy': 'فوضى', 'armed group': 'جماعة مسلحة', 'militia': 'ميليشيا', 'rebel': 'متمرد', 'terrorist group': 'جماعة إرهابية', 'child soldier': 'جندي طفل', 'landmine': 'لغم أرضي', 'chemical weapon': 'سلاح كيماوي', 'biological weapon': 'سلاح بيولوجي', 'nuclear weapon': 'سلاح نووي', 'dirty bomb': 'قنبلة قذرة', 'small arms': 'أسلحة صغيرة', 'heavy weapons': 'أسلحة ثقيلة', 'artillery': 'مدفعية', 'tank': 'دبابة', 'fighter jet': 'طائرة مقاتلة', 'drone': 'طائرة بدون طيار', 'naval blockade': 'حصار بحري', 'piracy': 'قرصنة', 'human rights violation': 'انتهاك حقوق الإنسان', 'freedom of speech': 'حرية التعبير', 'freedom of press': 'حرية الصحافة', 'freedom of assembly': 'حرية التجمع', 'freedom of religion': 'حرية الدين', 'ethnic discrimination': 'تمييز عرقي', 'religious discrimination': 'تمييز ديني', 'gender discrimination': 'تمييز بين الجنسين', 'child labor': 'عمالة الأطفال', 'forced labor': 'العمل القسري', 'slavery': 'عبودية', 'debt bondage': 'عبودية الدين', 'land seizure': 'الاستيلاء على الأراضي', 'forced eviction': 'إخلاء قسري', 'land grab': 'الاستيلاء على الأراضي', 'brutal government': 'حكومة وحشية', 'bombing campaign': 'حملة قصف', 'transport bottleneck': 'عنق زجاجة في النقل', 'weather extremes': 'ظواهر جوية متطرفة', 'price rise': 'ارتفاع الأسعار', 'cattle plague': 'طاعون الماشية', 'mismanagement': 'سوء الإدارة', 'harvest decline': 'انخفاض المحصول', 'forests destroyed': 'تدمير الغابات', 'jihadist groups': 'جماعات جهادية', 'migration': 'هجرة', 'economic impoverishment': 'إفقار اقتصادي', 'continued strife': 'استمرار الصراع', 'ecological crisis': 'أزمة بيئية', 'slave trade': 'تجارة الرقيق', 'lack of agricultural infrastructure': 'نقص البنية التحتية الزراعية', 'stolen food aid': 'سرقة المساعدات الغذائية', 'gangs of bandits': 'عصابات قطاع الطرق', 'gastrointestinal': 'معدي معوي', 'hunger crises': 'أزمات الجوع', 'pests': 'آفات', 'clan battle': 'معركة عشائرية', 'regimes were toppled': 'إسقاط الأنظمة'\n",
    "}\n",
    "\n",
    "# Map the English keywords to their Arabic translations\n",
    "df_risk['risk_factor_arabic'] = df_risk['risk_factor_english'].map(risk_factor_translations_ara)\n",
    "\n",
    "print(\"\\n--- Translation Complete ---\")\n",
    "print(\"Sample of the updated DataFrame:\")\n",
    "print(df_risk.head())\n",
    "\n",
    "\n",
    "# --- 3. Save to a NEW file to preserve your original data ---\n",
    "output_filename = 'risk-factors-translated.xlsx'\n",
    "output_path = os.path.join(DATA_DIR, '01_raw', output_filename)\n",
    "\n",
    "df_risk.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"\\nSuccessfully saved the complete, translated risk factors to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
