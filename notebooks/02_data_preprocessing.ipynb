{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This notebook prepares the raw news data for the main analysis by cleaning, structuring, and intelligently filtering it. The key innovation here is a **state-of-the-art semantic filter** that dramatically reduces the dataset size. This \"coarse-to-fine\" approach ensures our resource-intensive models focus only on the most relevant articles, which is crucial for completing the analysis within a tight timeframe.\n",
    "\n",
    "The preprocessing pipeline consists of the following steps:\n",
    "\n",
    "1.  **Load Data**: Import the raw English and Arabic news articles.\n",
    "2.  **Clean Text**: Remove noise from the text, such as HTML tags, URLs, and extra whitespace.\n",
    "3.  **Tokenize Sentences**: Break down the cleaned text into individual sentences.\n",
    "4.  **Semantic Pre-selection Filtering**: Use a powerful, multilingual sentence-embedding model to identify and keep only articles that are **semantically related** to the 167 known risk factors. This is superior to a simple keyword search as it understands the *meaning* and *context* of the text, not just exact word matches.\n",
    "5.  **Save Processed & Filtered Data**: Store the final, smaller, and analysis-ready datasets for the modeling stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Data\n",
    "\n",
    "First, let's load the two news article datasets from the previous EDA stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Load the datasets ---\n",
    "DATA_DIR = '../data'\n",
    "df_eng = pd.read_csv(os.path.join(DATA_DIR, '01_raw/news-articles-eng.csv'))\n",
    "df_ara = pd.read_csv(os.path.join(DATA_DIR, '01_raw/news-articles-ara.csv'))\n",
    "\n",
    "print(\"Data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. News Article Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning news articles...\n",
      "Cleaning complete.\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans raw text by removing HTML tags, URLs, and normalizing whitespace.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "print(\"Cleaning news articles...\")\n",
    "df_eng['body_cleaned'] = df_eng['body'].apply(clean_text)\n",
    "df_ara['body_cleaned'] = df_ara['body'].apply(clean_text)\n",
    "print(\"Cleaning complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentence Tokenization\n",
    "\n",
    "We will use a regular expression to split the text into sentences. This method looks for common sentence-ending punctuation (. ! ?) followed by a space, which is a robust approach for news text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing English articles into sentences...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Arabic articles into sentences...\n",
      "\n",
      "Sentence tokenization complete.\n",
      "\n",
      "--- Example of Regex Sentence Tokenization ---\n",
      "Article body has been split into 124 sentences.\n",
      "First 3 sentences:\n",
      "- Hussam al-Mahmoud | Yamen Moghrabi | Hassan Ibrahim On October 7, 2023, the world and the Middle East awoke to the drums of war beating in the Gaza Strip.\n",
      "- Over time, it turned into a reality that American efforts, Qatari and Egyptian mediation, condemnations, statements, summits, and conferences could not stop.\n",
      "- While Israel continues its war in the besieged Gaza Strip, attention is turning towards the potential outbreak of another war.\n"
     ]
    }
   ],
   "source": [
    "def regex_sent_tokenize(text):\n",
    "    \"\"\"\n",
    "    Splits text into sentences using a regular expression.\n",
    "    This is a dependency-free alternative to nltk.sent_tokenize.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return []\n",
    "    # Split on sentence-ending punctuation followed by a space or end of string\n",
    "    # The regex uses a \"positive lookbehind\" to keep the punctuation with the sentence.\n",
    "    sentences = re.split(r'(?<=[.!?۔])\\s+', text)\n",
    "    # Filter out any empty strings that might result from the split\n",
    "    return [s for s in sentences if s]\n",
    "\n",
    "# --- Apply the new, reliable tokenizer ---\n",
    "print(\"Tokenizing English articles into sentences...\")\n",
    "df_eng['sentences'] = df_eng['body_cleaned'].apply(regex_sent_tokenize)\n",
    "\n",
    "print(\"Tokenizing Arabic articles into sentences...\")\n",
    "df_ara['sentences'] = df_ara['body_cleaned'].apply(regex_sent_tokenize)\n",
    "\n",
    "print(\"\\nSentence tokenization complete.\")\n",
    "\n",
    "# --- Display a sample ---\n",
    "print(\"\\n--- Example of Regex Sentence Tokenization ---\")\n",
    "print(f\"Article body has been split into {len(df_eng['sentences'].iloc[0])} sentences.\")\n",
    "print(\"First 3 sentences:\")\n",
    "for sentence in df_eng['sentences'].iloc[0][:3]:\n",
    "    print(f\"- {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pre-selection: Semantic Filtering\n",
    "To efficiently handle the large dataset, we use a \"coarse-to-fine\" strategy. This step serves as the \"coarse\" filter, using a fast and powerful semantic search to intelligently identify a smaller, more relevant subset of articles. This avoids running the much slower, more resource-intensive classification model on the entire dataset.\n",
    "\n",
    "The process involves:\n",
    "\n",
    "Loading a lightweight, multilingual sentence-embedding model that converts text into numerical vectors representing its meaning.\n",
    "\n",
    "Encoding both the 167 English risk factors and all ~172,000 articles into this shared, multilingual \"meaning space.\"\n",
    "\n",
    "Performing a high-speed similarity search to find articles whose meaning is conceptually close to the risk factors.\n",
    "\n",
    "Filtering the articles based on a similarity score to create the final, smaller dataset for the modeling stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting semantic pre-selection to filter for relevant articles...\n",
      "Loading semantic search model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding risk factors...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e44de6480e48cfa8f60e89ac383d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding all article bodies (this may take some time)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feaec70b429843f1a1560e9711fdec8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5381 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing semantic search to find relevant articles...\n",
      "\n",
      "Found 150 potentially relevant articles with a threshold of 0.25.\n",
      "\n",
      "Semantic pre-selection filtering complete.\n",
      "  - English: Kept 44 of 86,660 articles.\n",
      "  - Arabic:  Kept 106 of 85,511 articles.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting semantic pre-selection to filter for relevant articles...\")\n",
    "\n",
    "# --- Load a lightweight, multilingual model for fast semantic search ---\n",
    "print(\"Loading semantic search model...\")\n",
    "embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# --- Load the English risk factors to search for ---\n",
    "risk_factors_path = os.path.join(DATA_DIR, '01_raw/risk-factors.xlsx')\n",
    "df_risk_factors = pd.read_excel(risk_factors_path)\n",
    "risk_factor_labels_eng = df_risk_factors['risk_factor_english'].tolist()\n",
    "\n",
    "# --- Convert the risk factors into meaning vectors (embeddings) ---\n",
    "print(\"Encoding risk factors...\")\n",
    "risk_factor_embeddings = embedder.encode(risk_factor_labels_eng, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# --- Convert all article bodies into meaning vectors ---\n",
    "# Combine English and Arabic articles into one list for batch processing\n",
    "print(\"Encoding all article bodies (this may take some time)...\")\n",
    "all_article_bodies = pd.concat([df_eng['body_cleaned'], df_ara['body_cleaned']], ignore_index=True)\n",
    "corpus_embeddings = embedder.encode(all_article_bodies.tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# --- Perform the high-speed semantic search ---\n",
    "print(\"Performing semantic search to find relevant articles...\")\n",
    "# This will find the single most similar risk factor for each article in the corpus.\n",
    "hits = util.semantic_search(risk_factor_embeddings, corpus_embeddings, top_k=1)\n",
    "\n",
    "# --- Calculate the maximum similarity score for each article ---\n",
    "# We create a tensor to hold the highest score found for each article.\n",
    "max_similarity_scores = torch.zeros(len(all_article_bodies))\n",
    "# The 'hits' result is a list for each risk factor. We iterate through it to find the max score for each article.\n",
    "for hit_list in hits:\n",
    "    for hit in hit_list:\n",
    "        corpus_id = hit['corpus_id']\n",
    "        score = hit['score']\n",
    "        # If this hit's score is higher than the current max for that article, update it.\n",
    "        if score > max_similarity_scores[corpus_id]:\n",
    "            max_similarity_scores[corpus_id] = score\n",
    "\n",
    "# --- Filter articles based on the similarity threshold ---\n",
    "# This is the key parameter to tune. A lower value keeps more articles.\n",
    "# 0.25 is a good starting point to balance speed and recall.\n",
    "SIMILARITY_THRESHOLD = 0.25\n",
    "relevant_indices = (max_similarity_scores > SIMILARITY_THRESHOLD).nonzero().squeeze().tolist()\n",
    "\n",
    "# Handle the case where only one article is found\n",
    "if isinstance(relevant_indices, int):\n",
    "    relevant_indices = [relevant_indices]\n",
    "\n",
    "print(f\"\\nFound {len(relevant_indices):,} potentially relevant articles with a threshold of {SIMILARITY_THRESHOLD}.\")\n",
    "\n",
    "# --- Create the final filtered DataFrames ---\n",
    "# Combine the original dataframes to easily select rows by their original index\n",
    "df_all = pd.concat([df_eng, df_ara], ignore_index=True)\n",
    "df_filtered = df_all.iloc[relevant_indices].copy()\n",
    "\n",
    "# Split back into English and Arabic dataframes\n",
    "df_eng_filtered = df_filtered[df_filtered['lang'] == 'eng']\n",
    "df_ara_filtered = df_filtered[df_filtered['lang'] == 'ara']\n",
    "\n",
    "print(\"\\nSemantic pre-selection filtering complete.\")\n",
    "print(f\"  - English: Kept {len(df_eng_filtered):,} of {len(df_eng):,} articles.\")\n",
    "print(f\"  - Arabic:  Kept {len(df_ara_filtered):,} of {len(df_ara):,} articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed and FILTERED English data saved to: ../data/02_processed/news_eng_processed_filtered.pkl\n",
      "Processed and FILTERED Arabic data saved to: ../data/02_processed/news_ara_processed_filtered.pkl\n"
     ]
    }
   ],
   "source": [
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, '02_processed')\n",
    "output_path_eng = os.path.join(PROCESSED_DATA_DIR, 'news_eng_processed_filtered.pkl')\n",
    "output_path_ara = os.path.join(PROCESSED_DATA_DIR, 'news_ara_processed_filtered.pkl')\n",
    "\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Save the smaller, filtered dataframes\n",
    "df_eng_filtered.to_pickle(output_path_eng)\n",
    "df_ara_filtered.to_pickle(output_path_ara)\n",
    "\n",
    "print(f\"\\nProcessed and FILTERED English data saved to: {output_path_eng}\")\n",
    "print(f\"Processed and FILTERED Arabic data saved to: {output_path_ara}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Additional Task: Completing the Risk Factor Data**\n",
    "\n",
    "For good data governance and to aid future analysis, it's a best practice to have a complete dataset. The original `risk-factors.xlsx` file was missing the Arabic translations. A separate, one-time script was used to create a new file, `risk-factors-translated.xlsx`, which contains both the English and Arabic terms. This ensures our core data is complete and bilingual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping pre-translated Arabic risk factors...\n",
      "\n",
      "--- Translation Complete ---\n",
      "Sample of the updated DataFrame:\n",
      "  risk_factor_english risk_factor_arabic\n",
      "0  massive starvation        مجاعة هائلة\n",
      "1          rinderpest         طاعون بقري\n",
      "2     scanty rainfall         شح الأمطار\n",
      "3         dysfunction          خلل وظيفي\n",
      "4                rise             ارتفاع\n",
      "\n",
      "Successfully saved the complete, translated risk factors to: ../data/01_raw/risk-factors-translated.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. Load your original risk factors file ---\n",
    "DATA_DIR = '../data'\n",
    "risk_factors_path = os.path.join(DATA_DIR, '01_raw/risk-factors.xlsx')\n",
    "df_risk = pd.read_excel(risk_factors_path)\n",
    "\n",
    "# --- 2. Use the pre-translated dictionary (fast and reliable) ---\n",
    "print(\"Mapping pre-translated Arabic risk factors...\")\n",
    "risk_factor_translations_ara = {\n",
    "    'massive starvation': 'مجاعة هائلة', 'rinderpest': 'طاعون بقري', 'scanty rainfall': 'شح الأمطار', 'dysfunction': 'خلل وظيفي', 'rise': 'ارتفاع', 'mass displacement': 'نزوح جماعي', 'conflict': 'صراع', 'hunger': 'جوع', 'malnutrition': 'سوء تغذية', 'drought': 'جفاف', 'locust': 'جراد', 'insecurity': 'انعدام الأمن', 'violence': 'عنف', 'poverty': 'فقر', 'displacement': 'نزوح', 'disease': 'مرض', 'death': 'موت', 'disaster': 'كارثة', 'crisis': 'أزمة', 'famine': 'مجاعة', 'emergency': 'طوارئ', 'shortage': 'نقص', 'cholera': 'كوليرا', 'malaria': 'ملاريا', 'measles': 'حصبة', 'typhoid': 'تيفوئيد', 'ebola': 'إيبولا', 'hiv': 'فيروس نقص المناعة البشرية', 'aids': 'الإيدز', 'tuberculosis': 'سل', 'diarrhea': 'إسهال', 'undernutrition': 'نقص التغذية', 'food prices': 'أسعار المواد الغذائية', 'inflation': 'تضخم', 'economic collapse': 'انهيار اقتصادي', 'currency devaluation': 'تخفيض قيمة العملة', 'unemployment': 'بطالة', 'corruption': 'فساد', 'sanctions': 'عقوبات', 'blockade': 'حصار', 'looting': 'نهب', 'theft': 'سرقة', 'crime': 'جريمة', 'terrorism': 'إرهاب', 'insurgency': 'تمرد', 'civil war': 'حرب أهلية', 'war': 'حرب', 'bombing': 'قصف', 'airstrike': 'غارة جوية', 'shelling': 'قصف مدفعي', 'gunfire': 'إطلاق نار', 'explosion': 'انفجار', 'massacre': 'مذبحة', 'genocide': 'إبادة جماعية', 'ethnic cleansing': 'تطهير عرقي', 'torture': 'تعذيب', 'rape': 'اغتصاب', 'abduction': 'اختطاف', 'kidnapping': 'خطف', 'hostage': 'رهينة', 'assassination': 'اغتيال', 'coup': 'انقلاب', 'political instability': 'عدم استقرار سياسي', 'protest': 'احتجاج', 'riot': 'شغب', 'curfew': 'حظر تجول', 'state of emergency': 'حالة طوارئ', 'martial law': 'أحكام عرفية', 'election violence': 'عنف انتخابي', 'border closure': 'إغلاق الحدود', 'refugee': 'لاجئ', 'asylum seeker': 'طالب لجوء', 'internally displaced person': 'نازح داخلي', 'migrant': 'مهاجر', 'human trafficking': 'اتجار بالبشر', 'smuggling': 'تهريب', 'flood': 'فيضان', 'hurricane': 'إعصار', 'cyclone': 'إعصار', 'typhoon': 'إعصار', 'earthquake': 'زلزال', 'tsunami': 'تسونامي', 'volcano': 'بركان', 'landslide': 'انهيار أرضي', 'avalanche': 'انهيار ثلجي', 'wildfire': 'حرائق غابات', 'heatwave': 'موجة حر', 'cold wave': 'موجة برد', 'hailstorm': 'عاصفة برد', 'tornado': 'إعصار', 'storm': 'عاصفة', 'monsoon': 'موسم الأمطار', 'crop failure': 'فشل المحاصيل', 'harvest failure': 'فشل الحصاد', 'livestock death': 'نفوق الماشية', 'water shortage': 'نقص المياه', 'power outage': 'انقطاع التيار الكهربائي', 'fuel shortage': 'نقص الوقود', 'road closure': 'إغلاق الطرق', 'infrastructure damage': 'أضرار في البنية التحتية', 'hospital closure': 'إغلاق المستشفيات', 'school closure': 'إغلاق المدارس', 'market closure': 'إغلاق الأسواق', 'aid shortage': 'نقص المساعدات', 'aid worker killed': 'مقتل عامل إغاثة', 'aid worker abducted': 'اختطاف عامل إغاثة', 'ngo withdrawal': 'انسحاب المنظمات غير الحكومية', 'un withdrawal': 'انسحاب الأمم المتحدة', 'peacekeeping mission': 'بعثة حفظ السلام', 'ceasefire violation': 'انتهاك وقف إطلاق النار', 'failed state': 'دولة فاشلة', 'anarchy': 'فوضى', 'armed group': 'جماعة مسلحة', 'militia': 'ميليشيا', 'rebel': 'متمرد', 'terrorist group': 'جماعة إرهابية', 'child soldier': 'جندي طفل', 'landmine': 'لغم أرضي', 'chemical weapon': 'سلاح كيماوي', 'biological weapon': 'سلاح بيولوجي', 'nuclear weapon': 'سلاح نووي', 'dirty bomb': 'قنبلة قذرة', 'small arms': 'أسلحة صغيرة', 'heavy weapons': 'أسلحة ثقيلة', 'artillery': 'مدفعية', 'tank': 'دبابة', 'fighter jet': 'طائرة مقاتلة', 'drone': 'طائرة بدون طيار', 'naval blockade': 'حصار بحري', 'piracy': 'قرصنة', 'human rights violation': 'انتهاك حقوق الإنسان', 'freedom of speech': 'حرية التعبير', 'freedom of press': 'حرية الصحافة', 'freedom of assembly': 'حرية التجمع', 'freedom of religion': 'حرية الدين', 'ethnic discrimination': 'تمييز عرقي', 'religious discrimination': 'تمييز ديني', 'gender discrimination': 'تمييز بين الجنسين', 'child labor': 'عمالة الأطفال', 'forced labor': 'العمل القسري', 'slavery': 'عبودية', 'debt bondage': 'عبودية الدين', 'land seizure': 'الاستيلاء على الأراضي', 'forced eviction': 'إخلاء قسري', 'land grab': 'الاستيلاء على الأراضي', 'brutal government': 'حكومة وحشية', 'bombing campaign': 'حملة قصف', 'transport bottleneck': 'عنق زجاجة في النقل', 'weather extremes': 'ظواهر جوية متطرفة', 'price rise': 'ارتفاع الأسعار', 'cattle plague': 'طاعون الماشية', 'mismanagement': 'سوء الإدارة', 'harvest decline': 'انخفاض المحصول', 'forests destroyed': 'تدمير الغابات', 'jihadist groups': 'جماعات جهادية', 'migration': 'هجرة', 'economic impoverishment': 'إفقار اقتصادي', 'continued strife': 'استمرار الصراع', 'ecological crisis': 'أزمة بيئية', 'slave trade': 'تجارة الرقيق', 'lack of agricultural infrastructure': 'نقص البنية التحتية الزراعية', 'stolen food aid': 'سرقة المساعدات الغذائية', 'gangs of bandits': 'عصابات قطاع الطرق', 'gastrointestinal': 'معدي معوي', 'hunger crises': 'أزمات الجوع', 'pests': 'آفات', 'clan battle': 'معركة عشائرية', 'regimes were toppled': 'إسقاط الأنظمة'\n",
    "}\n",
    "\n",
    "# Map the English keywords to their Arabic translations\n",
    "df_risk['risk_factor_arabic'] = df_risk['risk_factor_english'].map(risk_factor_translations_ara)\n",
    "\n",
    "print(\"\\n--- Translation Complete ---\")\n",
    "print(\"Sample of the updated DataFrame:\")\n",
    "print(df_risk.head())\n",
    "\n",
    "\n",
    "# --- 3. Save to a NEW file to preserve your original data ---\n",
    "output_filename = 'risk-factors-translated.xlsx'\n",
    "output_path = os.path.join(DATA_DIR, '01_raw', output_filename)\n",
    "\n",
    "df_risk.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"\\nSuccessfully saved the complete, translated risk factors to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
