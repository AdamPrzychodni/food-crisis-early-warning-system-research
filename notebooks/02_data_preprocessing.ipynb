{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This notebook handles the essential preprocessing steps for the news article data. The goal is to clean the raw text and structure it in a way that is suitable for advanced NLP analysis.\n",
    " \n",
    "The two main steps are:\n",
    "1.  **News Article Cleaning:** Removing noise like HTML tags, extra whitespace, and URLs.\n",
    "2.  **Sentence Tokenization:** Segmenting the cleaned text into individual sentences using a reliable method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Data\n",
    "\n",
    "First, let's load the two news article datasets from the previous EDA stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# --- Load the datasets ---\n",
    "DATA_DIR = '../data'\n",
    "df_eng = pd.read_csv(os.path.join(DATA_DIR, '01_raw/news-articles-eng.csv'))\n",
    "df_ara = pd.read_csv(os.path.join(DATA_DIR, '01_raw/news-articles-ara.csv'))\n",
    "\n",
    "print(\"Data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. News Article Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning news articles...\n",
      "Cleaning complete.\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans raw text by removing HTML tags, URLs, and normalizing whitespace.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "print(\"Cleaning news articles...\")\n",
    "df_eng['body_cleaned'] = df_eng['body'].apply(clean_text)\n",
    "df_ara['body_cleaned'] = df_ara['body'].apply(clean_text)\n",
    "print(\"Cleaning complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentence Tokenization\n",
    "\n",
    "We will use a regular expression to split the text into sentences. This method looks for common sentence-ending punctuation (. ! ?) followed by a space, which is a robust approach for news text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing English articles into sentences...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Arabic articles into sentences...\n",
      "\n",
      "Sentence tokenization complete.\n",
      "\n",
      "--- Example of Regex Sentence Tokenization ---\n",
      "Article body has been split into 124 sentences.\n",
      "First 3 sentences:\n",
      "- Hussam al-Mahmoud | Yamen Moghrabi | Hassan Ibrahim On October 7, 2023, the world and the Middle East awoke to the drums of war beating in the Gaza Strip.\n",
      "- Over time, it turned into a reality that American efforts, Qatari and Egyptian mediation, condemnations, statements, summits, and conferences could not stop.\n",
      "- While Israel continues its war in the besieged Gaza Strip, attention is turning towards the potential outbreak of another war.\n"
     ]
    }
   ],
   "source": [
    "def regex_sent_tokenize(text):\n",
    "    \"\"\"\n",
    "    Splits text into sentences using a regular expression.\n",
    "    This is a dependency-free alternative to nltk.sent_tokenize.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return []\n",
    "    # Split on sentence-ending punctuation followed by a space or end of string\n",
    "    # The regex uses a \"positive lookbehind\" to keep the punctuation with the sentence.\n",
    "    sentences = re.split(r'(?<=[.!?Û”])\\s+', text)\n",
    "    # Filter out any empty strings that might result from the split\n",
    "    return [s for s in sentences if s]\n",
    "\n",
    "# --- Apply the new, reliable tokenizer ---\n",
    "print(\"Tokenizing English articles into sentences...\")\n",
    "df_eng['sentences'] = df_eng['body_cleaned'].apply(regex_sent_tokenize)\n",
    "\n",
    "print(\"Tokenizing Arabic articles into sentences...\")\n",
    "df_ara['sentences'] = df_ara['body_cleaned'].apply(regex_sent_tokenize)\n",
    "\n",
    "print(\"\\nSentence tokenization complete.\")\n",
    "\n",
    "# --- Display a sample ---\n",
    "print(\"\\n--- Example of Regex Sentence Tokenization ---\")\n",
    "print(f\"Article body has been split into {len(df_eng['sentences'].iloc[0])} sentences.\")\n",
    "print(\"First 3 sentences:\")\n",
    "for sentence in df_eng['sentences'].iloc[0][:3]:\n",
    "    print(f\"- {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed English data saved to: ../data/02_processed/news_eng_processed.pkl\n",
      "Processed Arabic data saved to: ../data/02_processed/news_ara_processed.pkl\n"
     ]
    }
   ],
   "source": [
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, '02_processed')\n",
    "output_path_eng = os.path.join(PROCESSED_DATA_DIR, 'news_eng_processed.pkl')\n",
    "output_path_ara = os.path.join(PROCESSED_DATA_DIR, 'news_ara_processed.pkl')\n",
    "\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "df_eng.to_pickle(output_path_eng)\n",
    "df_ara.to_pickle(output_path_ara)\n",
    "\n",
    "print(f\"Processed English data saved to: {output_path_eng}\")\n",
    "print(f\"Processed Arabic data saved to: {output_path_ara}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
