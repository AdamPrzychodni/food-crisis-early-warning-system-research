{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading Data ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full preprocessed data loaded successfully.\n",
      "  - English: 86,660 articles\n",
      "  - Arabic:  85,511 articles\n",
      "------------------------------ \n",
      "\n",
      "--- Step 2: Loading Risk Factors ---\n",
      "Loaded 167 English risk factors.\n",
      "------------------------------ \n",
      "\n",
      "--- Step 3: Initializing Models ---\n",
      "GPU found. Models will run on the GPU for maximum speed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main classifier initialized: MoritzLaurer/deberta-v3-xsmall-zeroshot-v1.1-all-33\n",
      "Fast pre-filtering model initialized: paraphrase-multilingual-MiniLM-L12-v2\n",
      "------------------------------ \n",
      "\n",
      "--- Step 4: Defining the Extraction Function ---\n",
      "Risk factor embeddings pre-computed.\n",
      "\n",
      "--- Step 5: Running on a SAMPLE ---\n",
      "Processing a sample of 200 articles...\n",
      "\n",
      "Original sentence count: 13,394\n",
      "Pre-filtering sentences with threshold: 0.55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826c61e328a040d089ead1a8739eb578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/419 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced to 1,001 sentences after filtering.\n",
      "Running classifier with confidence threshold: 0.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13aeb253a694f73ac9d195bf7bf49ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample risk factor extraction complete.\n",
      "Found 3,345 potential risk mentions in the sample.\n",
      "------------------------------ \n",
      "\n",
      "--- Step 6: Refining Sample Results (Post-Processing) ---\n",
      "Original number of mentions: 3,345\n",
      "Refined to 572 high-confidence, unique mentions.\n",
      "------------------------------ \n",
      "\n",
      "--- Step 7: Saving SAMPLE Results ---\n",
      "Successfully saved 572 final sample risk mentions to: ../data/03_models/risk_mentions_SAMPLE_FINAL.csv\n",
      "------------------------------ \n",
      "\n",
      "--- Final Extracted Risk Factors (from Sample) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>date</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>risk_factor</th>\n",
       "      <th>confidence_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3250</th>\n",
       "      <td>169</td>\n",
       "      <td>2024-06-27</td>\n",
       "      <td>\".</td>\n",
       "      <td>without international aid</td>\n",
       "      <td>0.960398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365</th>\n",
       "      <td>37</td>\n",
       "      <td>2024-06-29</td>\n",
       "      <td>\"African narco-jihadism among Al Qaeda and Isl...</td>\n",
       "      <td>jihadist groups</td>\n",
       "      <td>0.998151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>62</td>\n",
       "      <td>2024-06-28</td>\n",
       "      <td>\"Al-Baghdadi´s sermon - an extension of the ex...</td>\n",
       "      <td>jihadist groups</td>\n",
       "      <td>0.993771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>37</td>\n",
       "      <td>2024-06-29</td>\n",
       "      <td>\"Counternetwork.</td>\n",
       "      <td>blockade</td>\n",
       "      <td>0.996570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>37</td>\n",
       "      <td>2024-06-29</td>\n",
       "      <td>\"El Sahel: epicentro yihadista en África Occid...</td>\n",
       "      <td>drought</td>\n",
       "      <td>0.983723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      article_id        date  \\\n",
       "3250         169  2024-06-27   \n",
       "1365          37  2024-06-29   \n",
       "1778          62  2024-06-28   \n",
       "1393          37  2024-06-29   \n",
       "1325          37  2024-06-29   \n",
       "\n",
       "                                          sentence_text  \\\n",
       "3250                                                 \".   \n",
       "1365  \"African narco-jihadism among Al Qaeda and Isl...   \n",
       "1778  \"Al-Baghdadi´s sermon - an extension of the ex...   \n",
       "1393                                   \"Counternetwork.   \n",
       "1325  \"El Sahel: epicentro yihadista en África Occid...   \n",
       "\n",
       "                    risk_factor  confidence_score  \n",
       "3250  without international aid          0.960398  \n",
       "1365            jihadist groups          0.998151  \n",
       "1778            jihadist groups          0.993771  \n",
       "1393                   blockade          0.996570  \n",
       "1325                    drought          0.983723  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. Load Processed Data ---\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"--- Step 1: Loading Data ---\")\n",
    "DATA_DIR = '../data'\n",
    "# Loading the complete preprocessed data, not the filtered version\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, '02_processed')\n",
    "\n",
    "df_eng = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, 'news_eng_processed.pkl'))\n",
    "df_ara = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, 'news_ara_processed.pkl'))\n",
    "print(\"Full preprocessed data loaded successfully.\")\n",
    "print(f\"  - English: {len(df_eng):,} articles\")\n",
    "print(f\"  - Arabic:  {len(df_ara):,} articles\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 2. Load Risk Factors ---\n",
    "print(\"--- Step 2: Loading Risk Factors ---\")\n",
    "risk_factors_path = os.path.join(DATA_DIR, '01_raw/risk-factors.xlsx')\n",
    "df_risk_factors_eng = pd.read_excel(risk_factors_path)\n",
    "df_risk_factors_eng.dropna(subset=['risk_factor_english'], inplace=True)\n",
    "risk_factor_labels = df_risk_factors_eng['risk_factor_english'].tolist()\n",
    "print(f\"Loaded {len(risk_factor_labels)} English risk factors.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 3. Initialize Models ---\n",
    "print(\"--- Step 3: Initializing Models ---\")\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "if device == 0:\n",
    "    print(\"GPU found. Models will run on the GPU for maximum speed.\")\n",
    "else:\n",
    "    print(\"No GPU found. Models will run on the CPU.\")\n",
    "\n",
    "MODEL_NAME = 'MoritzLaurer/deberta-v3-xsmall-zeroshot-v1.1-all-33'\n",
    "classifier = pipeline(\"zero-shot-classification\", model=MODEL_NAME, device=device)\n",
    "print(f\"Main classifier initialized: {MODEL_NAME}\")\n",
    "\n",
    "FAST_MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "fast_embedder = SentenceTransformer(FAST_MODEL_NAME, device=device)\n",
    "print(f\"Fast pre-filtering model initialized: {FAST_MODEL_NAME}\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 4. Define the Extraction Function ---\n",
    "print(\"--- Step 4: Defining the Extraction Function ---\")\n",
    "risk_factor_embeddings = fast_embedder.encode(risk_factor_labels, convert_to_tensor=True)\n",
    "print(\"Risk factor embeddings pre-computed.\")\n",
    "\n",
    "def extract_risk_factors_optimized(\n",
    "    df, classifier, labels, threshold, batch_size,\n",
    "    sentence_embedder, risk_factor_embeddings, sentence_similarity_threshold\n",
    "):\n",
    "    if 'article_id' not in df.columns:\n",
    "        df['article_id'] = df.index\n",
    "    df_sentences = df.explode('sentences').rename(columns={'sentences': 'sentence_text'})\n",
    "    df_sentences = df_sentences[['article_id', 'date', 'sentence_text']].dropna(subset=['sentence_text'])\n",
    "    all_sentences = df_sentences['sentence_text'].tolist()\n",
    "    if not all_sentences: return pd.DataFrame()\n",
    "\n",
    "    print(f\"\\nOriginal sentence count: {len(all_sentences):,}\")\n",
    "    print(f\"Pre-filtering sentences with threshold: {sentence_similarity_threshold}\")\n",
    "    sentence_embeddings = sentence_embedder.encode(all_sentences, convert_to_tensor=True, show_progress_bar=True)\n",
    "    hits = util.semantic_search(sentence_embeddings, risk_factor_embeddings, top_k=1)\n",
    "    relevant_indices = [i for i, hit_list in enumerate(hits) if hit_list and hit_list[0]['score'] >= sentence_similarity_threshold]\n",
    "    filtered_sentences_df = df_sentences.iloc[relevant_indices]\n",
    "    sentence_list = filtered_sentences_df['sentence_text'].tolist()\n",
    "    if not sentence_list: return pd.DataFrame()\n",
    "\n",
    "    print(f\"Reduced to {len(sentence_list):,} sentences after filtering.\")\n",
    "    print(f\"Running classifier with confidence threshold: {threshold}\")\n",
    "    results_list = []\n",
    "    for i, result in tqdm(enumerate(classifier(sentence_list, labels, multi_label=True, batch_size=batch_size)), total=len(sentence_list)):\n",
    "        for label, score in zip(result['labels'], result['scores']):\n",
    "            if score >= threshold:\n",
    "                original_row = filtered_sentences_df.iloc[i]\n",
    "                results_list.append({\n",
    "                    'article_id': original_row['article_id'],\n",
    "                    'date': original_row['date'],\n",
    "                    'sentence_text': result['sequence'],\n",
    "                    'risk_factor': label,\n",
    "                    'confidence_score': score\n",
    "                })\n",
    "    return pd.DataFrame(results_list)\n",
    "\n",
    "# --- 5. Set Parameters and Run on a SAMPLE ---\n",
    "print(\"\\n--- Step 5: Running on a SAMPLE ---\")\n",
    "CLASSIFIER_BATCH_SIZE = 128\n",
    "SENTENCE_SIMILARITY_THRESHOLD = 0.55\n",
    "CLASSIFIER_CONFIDENCE_THRESHOLD = 0.90\n",
    "\n",
    "# Create samples to test the pipeline\n",
    "df_eng_sample = df_eng.head(100).copy()\n",
    "df_ara_sample = df_ara.head(100).copy()\n",
    "df_sample = pd.concat([df_eng_sample, df_ara_sample], ignore_index=True)\n",
    "\n",
    "print(f\"Processing a sample of {len(df_sample):,} articles...\")\n",
    "\n",
    "all_risk_mentions = extract_risk_factors_optimized(\n",
    "    df_sample,\n",
    "    classifier,\n",
    "    risk_factor_labels,\n",
    "    threshold=CLASSIFIER_CONFIDENCE_THRESHOLD,\n",
    "    batch_size=CLASSIFIER_BATCH_SIZE,\n",
    "    sentence_embedder=fast_embedder,\n",
    "    risk_factor_embeddings=risk_factor_embeddings,\n",
    "    sentence_similarity_threshold=SENTENCE_SIMILARITY_THRESHOLD\n",
    ")\n",
    "print(\"\\nSample risk factor extraction complete.\")\n",
    "print(f\"Found {len(all_risk_mentions):,} potential risk mentions in the sample.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 6. Post-Processing: Keep Only the Top Label per Sentence ---\n",
    "print(\"--- Step 6: Refining Sample Results (Post-Processing) ---\")\n",
    "if not all_risk_mentions.empty:\n",
    "    print(f\"Original number of mentions: {len(all_risk_mentions):,}\")\n",
    "    # For each sentence, find the index of the row with the max confidence score\n",
    "    idx = all_risk_mentions.groupby('sentence_text')['confidence_score'].idxmax()\n",
    "    # Use the index to select the corresponding rows\n",
    "    all_risk_mentions_refined = all_risk_mentions.loc[idx]\n",
    "    print(f\"Refined to {len(all_risk_mentions_refined):,} high-confidence, unique mentions.\")\n",
    "else:\n",
    "    all_risk_mentions_refined = pd.DataFrame() # Create empty df if no mentions were found\n",
    "    print(\"No risk mentions found to refine.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 7. Save the SAMPLE Results ---\n",
    "print(\"--- Step 7: Saving SAMPLE Results ---\")\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, '03_models')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(OUTPUT_DIR, 'risk_mentions_SAMPLE_FINAL.csv')\n",
    "all_risk_mentions_refined.to_csv(output_path, index=False)\n",
    "print(f\"Successfully saved {len(all_risk_mentions_refined):,} final sample risk mentions to: {output_path}\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "print(\"--- Final Extracted Risk Factors (from Sample) ---\")\n",
    "if all_risk_mentions_refined.empty:\n",
    "    print(\"No risk factors found with the current settings.\")\n",
    "else:\n",
    "    display(all_risk_mentions_refined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
