{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection\n",
    "This notebook performs the core task of the project: identifying risk factors in news articles using a multilingual, zero-shot classification model. We will use the facebook/xlm-roberta-base model from the Hugging Face library to classify individual sentences against a predefined list of 167 risk factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main steps are:\n",
    "\n",
    "1.  **Load Processed Data**: Load the sentence-tokenized dataframes for both English and Arabic news articles.\n",
    "2.  **Load Risk Factors**: Load the list of 167 risk factors that will be used as candidate labels for the classification model.\n",
    "3.  **Initialize the Model**: Set up the zero-shot classification pipeline from the `transformers` library.\n",
    "4.  **Run Classification**: Process each sentence from every article, classify it against the risk factors, and store the results that meet a confidence threshold of 0.80.\n",
    "5.  **Save the Results**: Save the final DataFrame containing the identified risk factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# --- Load the processed datasets ---\n",
    "DATA_DIR = '../data'\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, '02_processed')\n",
    "\n",
    "df_eng = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, 'news_eng_processed_filtered.pkl'))\n",
    "df_ara = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, 'news_eng_processed_filtered.pkl'))\n",
    "\n",
    "print(\"Processed data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Risk Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167 risk factors loaded.\n",
      "Sample risk factors: ['massive starvation', 'rinderpest', 'scanty rainfall', 'dysfunction', 'rise']\n"
     ]
    }
   ],
   "source": [
    "# --- Load risk factors ---\n",
    "risk_factors_path = os.path.join(DATA_DIR, '01_raw/risk-factors.xlsx')\n",
    "df_risk_factors = pd.read_excel(risk_factors_path)\n",
    "\n",
    "# --- CORRECTED LINE ---\n",
    "# The column name is 'risk_factor_english', not 'Risk Factor'.\n",
    "risk_factor_labels = df_risk_factors['risk_factor_english'].tolist()\n",
    "\n",
    "print(f\"{len(risk_factor_labels)} risk factors loaded.\")\n",
    "print(\"Sample risk factors:\", risk_factor_labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found. The model will run on the GPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7e13591081427f855428094d45559e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc590a7e595d497bba6ab8e00863a2fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22461c0699c42ccbc8120d9b508a433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c7573ec6c14b778e2eb19dc45506de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01cd045dea2c4c5db1f7a15d3da9f1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6495d8bf3c4a3fbb280b66ed5d2449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db3336177584aba81fc0edf097c6bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/286 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot classification pipeline initialized with SOTA model: MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\n"
     ]
    }
   ],
   "source": [
    "# --- Check for GPU ---\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "if device == 0:\n",
    "    print(\"GPU found. The model will run on the GPU.\")\n",
    "else:\n",
    "    print(\"No GPU found. The model will run on the CPU. This may be slow.\")\n",
    "\n",
    "# --- Initialize the pipeline with the SOTA model ---\n",
    "\n",
    "# This model is a newer, more powerful replacement for XLM-RoBERTa\n",
    "# for zero-shot classification tasks.\n",
    "MODEL_NAME = 'MoritzLaurer/mDeBERTa-v3-base-mnli-xnli' \n",
    "\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=MODEL_NAME,\n",
    "    device=device # Use your GPU\n",
    ")\n",
    "\n",
    "print(f\"Zero-shot classification pipeline initialized with SOTA model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Classification\n",
    "\n",
    "This is the main processing step. To maximize efficiency, we will use a batch processing approach instead of iterating through each article one by one. This involves:\n",
    "\n",
    "Restructuring the Data: We will combine all sentences from all articles into a single, large list.\n",
    "\n",
    "Batch Inference: This entire list is fed directly to the transformers pipeline, which automatically groups the sentences into optimal batches to keep the GPU fully utilized.\n",
    "\n",
    "This method is significantly faster than a traditional loop as it minimizes CPU-GPU communication overhead. We will run the process on a small sample of 5 articles from each language to verify the pipeline and see the output structure.\n",
    "\n",
    "Note: Even with this optimization, processing the full dataset of ~172,000 articles is a computationally intensive task that will still take a considerable amount of time. Running this initial small sample is crucial for ensuring the code works correctly before launching the full analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting risk factor extraction for English articles...\n",
      "Processing 1,294 sentences in batches of 128...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63374148db1c4c9e8ab4ffe0149efca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1294 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting risk factor extraction for Arabic articles...\n",
      "Processing 938 sentences in batches of 128...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db91af663aab4bce9107ff2ae9a24277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Risk factor extraction complete.\n",
      "\n",
      "--- Final Extracted Risk Factors ---\n",
      "       article_id        date  \\\n",
      "0               0  2024-07-09   \n",
      "1               0  2024-07-09   \n",
      "2               0  2024-07-09   \n",
      "3               0  2024-07-09   \n",
      "4               0  2024-07-09   \n",
      "...           ...         ...   \n",
      "29514           9  2024-07-22   \n",
      "29515           9  2024-07-22   \n",
      "29516           9  2024-07-22   \n",
      "29517           9  2024-07-22   \n",
      "29518           9  2024-07-22   \n",
      "\n",
      "                                           sentence_text          risk_factor  \\\n",
      "0      Hussam al-Mahmoud | Yamen Moghrabi | Hassan Ib...             conflict   \n",
      "1      Hussam al-Mahmoud | Yamen Moghrabi | Hassan Ib...               mayhem   \n",
      "2      Hussam al-Mahmoud | Yamen Moghrabi | Hassan Ib...                siege   \n",
      "3      Hussam al-Mahmoud | Yamen Moghrabi | Hassan Ib...  international alarm   \n",
      "4      Hussam al-Mahmoud | Yamen Moghrabi | Hassan Ib...      major offensive   \n",
      "...                                                  ...                  ...   \n",
      "29514  نصف النهائي تقام مباراتا نصف النهائي في 28 و 2...     continued strife   \n",
      "29515  المباراة النهائية وتلعب يوم الأول من فبراير/شباط.   prolonged fighting   \n",
      "29516  المباراة النهائية وتلعب يوم الأول من فبراير/شباط.             conflict   \n",
      "29517  المباراة النهائية وتلعب يوم الأول من فبراير/شباط.     continued strife   \n",
      "29518  المباراة النهائية وتلعب يوم الأول من فبراير/شباط.      major offensive   \n",
      "\n",
      "       confidence_score  \n",
      "0              0.988027  \n",
      "1              0.974725  \n",
      "2              0.936735  \n",
      "3              0.913532  \n",
      "4              0.896342  \n",
      "...                 ...  \n",
      "29514          0.878991  \n",
      "29515          0.983769  \n",
      "29516          0.955527  \n",
      "29517          0.894807  \n",
      "29518          0.823399  \n",
      "\n",
      "[29519 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm # A library to create smart progress bars\n",
    "\n",
    "def extract_risk_factors_fast(df, classifier, labels, threshold=0.80, batch_size=32):\n",
    "    \"\"\"\n",
    "    Extracts risk factors from a DataFrame by processing all sentences in batches.\n",
    "    \"\"\"\n",
    "    # 1. Restructure the data for batch processing\n",
    "    if 'article_id' not in df.columns:\n",
    "        df['article_id'] = df.index\n",
    "        \n",
    "    df_sentences = df.explode('sentences').rename(columns={'sentences': 'sentence_text'})\n",
    "    df_sentences = df_sentences[['article_id', 'date', 'sentence_text']].dropna(subset=['sentence_text'])\n",
    "    \n",
    "    sentence_list = df_sentences['sentence_text'].tolist()\n",
    "    \n",
    "    if not sentence_list:\n",
    "        print(\"No sentences to process for this sample.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    print(f\"Processing {len(sentence_list):,} sentences in batches of {batch_size}...\")\n",
    "\n",
    "    results_list = []\n",
    "    # 2. Process all sentences in one go with a progress bar\n",
    "    for i, result in tqdm(enumerate(classifier(sentence_list, labels, multi_label=True, batch_size=batch_size)), total=len(sentence_list)):\n",
    "        # 3. Filter results and store them\n",
    "        for label, score in zip(result['labels'], result['scores']):\n",
    "            if score >= threshold:\n",
    "                original_row = df_sentences.iloc[i]\n",
    "                results_list.append({\n",
    "                    'article_id': original_row['article_id'],\n",
    "                    'date': original_row['date'],\n",
    "                    'sentence_text': result['sequence'],\n",
    "                    'risk_factor': label,\n",
    "                    'confidence_score': score\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(results_list)\n",
    "\n",
    "# --- Set the sample size to 5 articles each ---\n",
    "# Commment this out to process the full dataset !!!\n",
    "df_eng = df_eng.head(10)\n",
    "df_ara = df_ara.head(10)\n",
    "\n",
    "BATCH_SIZE = 128 # You can adjust this based on your GPU memory\n",
    "\n",
    "# --- Run the FAST extraction process ---\n",
    "print(\"Starting risk factor extraction for English articles...\")\n",
    "eng_risk_mentions = extract_risk_factors_fast(df_eng, classifier, risk_factor_labels, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"\\nStarting risk factor extraction for Arabic articles...\")\n",
    "ara_risk_mentions = extract_risk_factors_fast(df_ara, classifier, risk_factor_labels, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"\\nRisk factor extraction complete.\")\n",
    "\n",
    "# --- Combine results from both languages ---\n",
    "all_risk_mentions = pd.concat([eng_risk_mentions, ara_risk_mentions], ignore_index=True)\n",
    "\n",
    "print(\"\\n--- Final Extracted Risk Factors ---\")\n",
    "# Display the full results from this small sample\n",
    "if all_risk_mentions.empty:\n",
    "    print(\"No risk factors found in the sample articles with the current threshold.\")\n",
    "else:\n",
    "    print(all_risk_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_risk_mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save the final DataFrame ---\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, '03_models')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(OUTPUT_DIR, 'risk_mentions.csv')\n",
    "all_risk_mentions.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nSuccessfully saved {len(all_risk_mentions)} risk mentions to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
