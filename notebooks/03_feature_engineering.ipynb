{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering: Semantic Pre-selection\n",
    "\n",
    "This notebook serves as the crucial \"coarse\" filtering stage in our \"coarse-to-fine\" analysis pipeline. After the foundational text cleaning in the previous step, this notebook intelligently reduces the dataset size by using a powerful semantic search.\n",
    "\n",
    "The key innovation here is to filter our large corpus of \\~172,000 articles down to a smaller, high-relevance subset. This ensures that our most computationally expensive model in the next stage focuses only on articles that are conceptually related to food security risks, dramatically improving efficiency without sacrificing critical information.\n",
    "\n",
    "The feature engineering pipeline consists of the following steps:\n",
    "\n",
    "1.  **Load Processed Data**: Import the clean, sentence-tokenized datasets from the previous preprocessing stage.\n",
    "2.  **Semantic Pre-selection Filtering**: Use a lightweight, multilingual sentence-embedding model (`paraphrase-multilingual-MiniLM-L12-v2`) to perform a high-speed similarity search between all articles and the 167 known risk factors.\n",
    "3.  **Save Filtered Data**: Store the final, smaller, and analysis-ready datasets for the modeling stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\\. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Load the datasets ---\n",
    "DATA_DIR = '../data'\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, '02_processed')\n",
    "\n",
    "df_eng = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, 'news_eng_processed.pkl'))\n",
    "df_ara = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, 'news_ara_processed.pkl'))\n",
    "\n",
    "print(\"Processed data loaded successfully.\")\n",
    "print(f\"  - English: {len(df_eng):,} articles\")\n",
    "print(f\"  - Arabic:  {len(df_ara):,} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\\. Pre-selection: Semantic Filtering\n",
    "\n",
    "To efficiently handle the large dataset, we use a \"coarse-to-fine\" strategy. This step serves as the \"coarse\" filter, using a fast and powerful semantic search to intelligently identify a smaller, more relevant subset of articles. This avoids running the much slower, more resource-intensive classification model on the entire dataset.\n",
    "\n",
    "The process involves:\n",
    "\n",
    "  * Loading a lightweight, multilingual sentence-embedding model that converts text into numerical vectors representing its meaning.\n",
    "  * Encoding both the 167 English risk factors and all article bodies into this shared, multilingual \"meaning space.\"\n",
    "  * Performing a high-speed similarity search to find articles whose meaning is conceptually close to the risk factors.\n",
    "  * Filtering the articles based on a similarity score to create the final, smaller dataset for the modeling stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting semantic pre-selection to filter for relevant articles...\")\n",
    "\n",
    "# --- Load a lightweight, multilingual model for fast semantic search ---\n",
    "print(\"Loading semantic search model...\")\n",
    "embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# --- Load the English risk factors to search for ---\n",
    "risk_factors_path = os.path.join(DATA_DIR, '01_raw/risk-factors.xlsx')\n",
    "df_risk_factors = pd.read_excel(risk_factors_path)\n",
    "risk_factor_labels_eng = df_risk_factors['risk_factor_english'].tolist()\n",
    "\n",
    "# --- Convert the risk factors into meaning vectors (embeddings) ---\n",
    "print(\"Encoding risk factors...\")\n",
    "risk_factor_embeddings = embedder.encode(risk_factor_labels_eng, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# --- Convert all article bodies into meaning vectors ---\n",
    "# Combine English and Arabic articles into one list for batch processing\n",
    "print(\"Encoding all article bodies (this may take some time)...\")\n",
    "all_article_bodies = pd.concat([df_eng['body_cleaned'], df_ara['body_cleaned']], ignore_index=True)\n",
    "corpus_embeddings = embedder.encode(all_article_bodies.tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# --- Perform the high-speed semantic search ---\n",
    "print(\"Performing semantic search to find relevant articles...\")\n",
    "# This will find the single most similar risk factor for each article in the corpus.\n",
    "hits = util.semantic_search(risk_factor_embeddings, corpus_embeddings, top_k=1)\n",
    "\n",
    "# --- Calculate the maximum similarity score for each article ---\n",
    "# We create a tensor to hold the highest score found for each article.\n",
    "max_similarity_scores = torch.zeros(len(all_article_bodies))\n",
    "# The 'hits' result is a list for each risk factor. We iterate through it to find the max score for each article.\n",
    "for hit_list in hits:\n",
    "    for hit in hit_list:\n",
    "        corpus_id = hit['corpus_id']\n",
    "        score = hit['score']\n",
    "        # If this hit's score is higher than the current max for that article, update it.\n",
    "        if score > max_similarity_scores[corpus_id]:\n",
    "            max_similarity_scores[corpus_id] = score\n",
    "\n",
    "# --- Filter articles based on the similarity threshold ---\n",
    "# This is the key parameter to tune. A lower value keeps more articles.\n",
    "# 0.25 is a good starting point to balance speed and recall.\n",
    "SIMILARITY_THRESHOLD = 0.25\n",
    "relevant_indices = (max_similarity_scores > SIMILARITY_THRESHOLD).nonzero().squeeze().tolist()\n",
    "\n",
    "# Handle the case where only one article is found\n",
    "if isinstance(relevant_indices, int):\n",
    "    relevant_indices = [relevant_indices]\n",
    "\n",
    "print(f\"\\nFound {len(relevant_indices):,} potentially relevant articles with a threshold of {SIMILARITY_THRESHOLD}.\")\n",
    "\n",
    "# --- Create the final filtered DataFrames ---\n",
    "# Combine the original dataframes to easily select rows by their original index\n",
    "df_all = pd.concat([df_eng, df_ara], ignore_index=True)\n",
    "df_filtered = df_all.iloc[relevant_indices].copy()\n",
    "\n",
    "# Split back into English and Arabic dataframes\n",
    "df_eng_filtered = df_filtered[df_filtered['lang'] == 'eng']\n",
    "df_ara_filtered = df_filtered[df_filtered['lang'] == 'ara']\n",
    "\n",
    "print(\"\\nSemantic pre-selection filtering complete.\")\n",
    "print(f\"  - English: Kept {len(df_eng_filtered):,} of {len(df_eng):,} articles.\")\n",
    "print(f\"  - Arabic:  Kept {len(df_ara_filtered):,} of {len(df_ara):,} articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\\. Save Filtered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define output paths ---\n",
    "output_path_eng = os.path.join(PROCESSED_DATA_DIR, 'news_eng_processed_filtered.pkl')\n",
    "output_path_ara = os.path.join(PROCESSED_DATA_DIR, 'news_ara_processed_filtered.pkl')\n",
    "\n",
    "# --- Save the smaller, filtered dataframes ---\n",
    "df_eng_filtered.to_pickle(output_path_eng)\n",
    "df_ara_filtered.to_pickle(output_path_ara)\n",
    "\n",
    "print(f\"\\nProcessed and FILTERED English data saved to: {output_path_eng}\")\n",
    "print(f\"Processed and FILTERED Arabic data saved to: {output_path_ara}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
