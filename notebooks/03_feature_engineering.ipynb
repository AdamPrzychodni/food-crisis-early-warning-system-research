{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering: Semantic Pre-selection\n",
    "\n",
    "This notebook serves as the crucial \"coarse\" filtering stage in our \"coarse-to-fine\" analysis pipeline. After the foundational text cleaning in the previous step, this notebook intelligently reduces the dataset size by using a powerful semantic search.\n",
    "\n",
    "The key innovation here is to filter our large corpus of \\~172,000 articles down to a smaller, high-relevance subset. This ensures that our most computationally expensive model in the next stage focuses only on articles that are conceptually related to food security risks, dramatically improving efficiency without sacrificing critical information.\n",
    "\n",
    "The feature engineering pipeline consists of the following steps:\n",
    "\n",
    "1.  **Load Processed Data**: Import the clean, sentence-tokenized datasets from the previous preprocessing stage.\n",
    "2.  **Semantic Pre-selection Filtering**: Use a lightweight, multilingual sentence-embedding model (`paraphrase-multilingual-MiniLM-L12-v2`) to perform a high-speed similarity search between all articles and the 167 known risk factors.\n",
    "3.  **Save Filtered Data**: Store the final, smaller, and analysis-ready datasets for the modeling stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data loaded successfully.\n",
      "  - English: 86,660 articles\n",
      "  - Arabic:  85,511 articles\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Load the datasets ---\n",
    "DATA_DIR = '../data'\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, '02_processed')\n",
    "\n",
    "df_eng = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, 'news_eng_processed.pkl'))\n",
    "df_ara = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, 'news_ara_processed.pkl'))\n",
    "\n",
    "print(\"Processed data loaded successfully.\")\n",
    "print(f\"  - English: {len(df_eng):,} articles\")\n",
    "print(f\"  - Arabic:  {len(df_ara):,} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pre-selection: Semantic Filtering\n",
    "\n",
    "To efficiently handle the large dataset, we use a \"coarse-to-fine\" strategy. This step serves as the \"coarse\" filter, using a fast and powerful semantic search to intelligently identify a smaller, more relevant subset of articles. This avoids running the much slower, more resource-intensive classification model on the entire dataset.\n",
    "\n",
    "The process involves:\n",
    "\n",
    "  * Loading a lightweight, multilingual sentence-embedding model that converts text into numerical vectors representing its meaning.\n",
    "  * Encoding both the 167 English risk factors and all article bodies into this shared, multilingual \"meaning space.\"\n",
    "  * Performing a high-speed similarity search to find articles whose meaning is conceptually close to the risk factors.\n",
    "  * Filtering the articles based on a similarity score to create the final, smaller dataset for the modeling stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting semantic pre-selection to filter for relevant articles...\n",
      "Loading semantic search model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding risk factors...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54897837cc0541fa8df786d630e963b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding all article bodies (this may take some time)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38cad8352c0042c3b567bc2dea45e6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/673 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nStarting semantic pre-selection to filter for relevant articles...\")\n",
    "\n",
    "# --- Load a lightweight, multilingual model for fast semantic search ---\n",
    "print(\"Loading semantic search model...\")\n",
    "embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# --- Load the English risk factors to search for ---\n",
    "risk_factors_path = os.path.join(DATA_DIR, '01_raw/risk-factors.xlsx')\n",
    "df_risk_factors = pd.read_excel(risk_factors_path)\n",
    "risk_factor_labels_eng = df_risk_factors['risk_factor_english'].tolist()\n",
    "\n",
    "# --- Convert the risk factors into meaning vectors (embeddings) ---\n",
    "print(\"Encoding risk factors...\")\n",
    "risk_factor_embeddings = embedder.encode(\n",
    "    risk_factor_labels_eng, \n",
    "    convert_to_tensor=True, \n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "# --- Convert all article bodies into meaning vectors ---\n",
    "# Combine English and Arabic articles into one list for batch processing\n",
    "print(\"Encoding all article bodies (this may take some time)...\")\n",
    "all_article_bodies = pd.concat([df_eng['body_cleaned'], df_ara['body_cleaned']], ignore_index=True)\n",
    "\n",
    "# Set a larger batch size to better utilize the GPU.\n",
    "# You can tune this value based on your GPU's VRAM. Start with 128 or 256.\n",
    "ENCODE_BATCH_SIZE = 256 \n",
    "\n",
    "corpus_embeddings = embedder.encode(\n",
    "    all_article_bodies.tolist(), \n",
    "    batch_size=ENCODE_BATCH_SIZE, # Added batch_size for performance\n",
    "    convert_to_tensor=True, \n",
    "    show_progress_bar=True\n",
    ")\n",
    "print(f\"Encoding complete. Used a batch size of {ENCODE_BATCH_SIZE}.\")\n",
    "\n",
    "\n",
    "# --- Perform the high-speed semantic search ---\n",
    "print(\"Performing semantic search to find relevant articles...\")\n",
    "\n",
    "# Swapped embeddings to find the best risk factor FOR EACH article.\n",
    "# This ensures we get a score for every article in the corpus.\n",
    "hits = util.semantic_search(corpus_embeddings, risk_factor_embeddings, top_k=1)\n",
    "\n",
    "# --- Extract the similarity scores ---\n",
    "# The result is a list of lists, where each inner list contains the top_k hits for a corpus document.\n",
    "# Since top_k=1, we can directly extract the scores.\n",
    "max_similarity_scores = [hit[0]['score'] for hit in hits]\n",
    "\n",
    "# --- Filter articles based on the similarity threshold ---\n",
    "\n",
    "# UPDATED: Increased the threshold again to be more selective.\n",
    "# This is the key parameter to tune. Let's try a higher value\n",
    "# to further reduce the number of articles for the next stage.\n",
    "SIMILARITY_THRESHOLD = 0.40 \n",
    "\n",
    "# Create a boolean mask for filtering\n",
    "relevant_mask = np.array(max_similarity_scores) > SIMILARITY_THRESHOLD\n",
    "relevant_indices = np.where(relevant_mask)[0].tolist()\n",
    "\n",
    "print(f\"\\nFound {len(relevant_indices):,} potentially relevant articles with a threshold of {SIMILARITY_THRESHOLD}.\")\n",
    "\n",
    "# --- Create the final filtered DataFrames ---\n",
    "# Combine the original dataframes to easily select rows by their original index\n",
    "df_all = pd.concat([df_eng, df_ara], ignore_index=True)\n",
    "df_filtered = df_all.iloc[relevant_indices].copy()\n",
    "\n",
    "# Split back into English and Arabic dataframes\n",
    "df_eng_filtered = df_filtered[df_filtered['lang'] == 'eng']\n",
    "df_ara_filtered = df_filtered[df_filtered['lang'] == 'ara']\n",
    "\n",
    "print(\"\\nSemantic pre-selection filtering complete.\")\n",
    "print(f\"  - English: Kept {len(df_eng_filtered):,} of {len(df_eng):,} articles.\")\n",
    "print(f\"  - Arabic:  Kept {len(df_ara_filtered):,} of {len(df_ara):,} articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Save Filtered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed and FILTERED English data saved to: ../data/02_processed/news_eng_processed_filtered.pkl\n",
      "Processed and FILTERED Arabic data saved to: ../data/02_processed/news_ara_processed_filtered.pkl\n"
     ]
    }
   ],
   "source": [
    "# --- Define output paths ---\n",
    "output_path_eng = os.path.join(PROCESSED_DATA_DIR, 'news_eng_processed_filtered.pkl')\n",
    "output_path_ara = os.path.join(PROCESSED_DATA_DIR, 'news_ara_processed_filtered.pkl')\n",
    "\n",
    "# --- Save the smaller, filtered dataframes ---\n",
    "df_eng_filtered.to_pickle(output_path_eng)\n",
    "df_ara_filtered.to_pickle(output_path_ara)\n",
    "\n",
    "print(f\"\\nProcessed and FILTERED English data saved to: {output_path_eng}\")\n",
    "print(f\"Processed and FILTERED Arabic data saved to: {output_path_ara}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
