{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ce3648",
   "metadata": {},
   "source": [
    "# [Take-home Assessment] Food Crisis Early Warning \n",
    "\n",
    "Welcome to the assessment. You will showcase your modeling and research skills by investigating news articles (in English and Arabic) as well as a set of food insecurity risk factors. \n",
    "\n",
    "We suggest planning to spend **~6–8 hours** on this assessment. **Please submit your response by Monday, September 15th, 9:00 AM EST via email to dime_ai@worldbank.org**. Please document your code with comments and explanations of design choices. There is one question on reflecting upon your approach at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cf2966",
   "metadata": {},
   "source": [
    "**Name:** Adam Przychodni\n",
    "\n",
    "**Email:** adam.przychodni@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09329a02",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "# Part 1: Technical Assessment\n",
    "\n",
    "\n",
    "## Task:\n",
    "\n",
    "We invite you to approach the challenge of understanding (and potentially predicting) food insecurity using the provided (limited) data. Your response should demonstrate how you tackle open-ended problems in data-scarce environments.\n",
    "\n",
    "Some example questions to consider:\n",
    "- What is the added value of geospatial data?\n",
    "- How can we address the lack of ground-truth information on food insecurity levels?\n",
    "- What are the benefits and challenges of working with multilingual data?\n",
    "- ...\n",
    "\n",
    "These are just guiding examples — you are free to explore any relevant angles to this topic/data.\n",
    "\n",
    "**Note:** There is no single \"right\" approach. Instead, we want to understand how you approach and structure open-ended problems in data-scarce environments. Given the large number of applicants, we will preselect the most impressive and complete submissions. Please take effort in structuring your response, as selection will depend on its depth and originality.\n",
    "\n",
    "\n",
    "## Provided Data:\n",
    "\n",
    "1. **Risk Factors:** A file containing 167 risk factors (unigrams, bigrams, and trigrams) in the `english_keywords` column and an empty `keywords_arabic` column. A separate file with the mapping of English risk factors to pre-defined thematic cluster assignments.\n",
    "\n",
    "\n",
    "2. **News Articles:** Two files containing one month of news articles from the Mashriq region:\n",
    "   - `news-articles-eng.csv`\n",
    "   - `news-articles-ara.csv`\n",
    "   - **Note:** You may work on a sample subset during development.\n",
    "   \n",
    "   \n",
    "3. **Geographic Taxonomy:** A file containing the names of the countries, provinces, and districts for the subset of Mashriq countries that is covered by the news articles. The files are a dictionary mapping from a key to the geographic name.\n",
    "   - `id_arabic_location_name.pkl`\n",
    "   - `id_english_location_name.pkl`\n",
    "   - **Note:** Each unique country/province/district is assigned a key (e.g. `iq`,`iq_bg` and `iq_bg_1` for country Iraq, province Baghdad, and district 1 in Baghdad respectively).\n",
    "   - The key of country names is a two character abbreviation as follows.\n",
    "       - 'iq': 'Iraq'\n",
    "       - 'jo': 'Jordan'\n",
    "       - 'lb': 'Lebanon'\n",
    "       - 'ps': 'Palestine'\n",
    "       - 'sy': 'Syria'\n",
    "       \n",
    "   - The key of provinces is a two-character abbreviation of the country followed by two-character abbreviation of the province **`{country_abbreviation}_{province_abbreviation}`**, and the key of districts is **`{country_abbreviation}_{province_abbreviation}_{unique_number}`**.\n",
    "       \n",
    "\n",
    "\n",
    "## Submission Guidelines:\n",
    "\n",
    "- **Code:** Follow best coding practices and ensure clear documentation. All notebook cells should be executed with outputs saved, and the notebook should run correctly on its own. Name your file **`solution_{FIRSTNAME}_{LASTNAME}.ipynb`**. If your solution relies on additional open-access data, either include it in your submission (e.g., as part of a ZIP file) or provide clear data-loading code/instructions as part of the nottebook. \n",
    "- **Report:** Submit a separate markdown file communicating your approach to this research problem. We expect you to detail the models, methods, or (additional) data you are using.\n",
    "\n",
    "Good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f9934a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task1",
   "metadata": {},
   "source": [
    "## Your Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b241d89f",
   "metadata": {},
   "source": [
    "parts of code generated and also formatted using LLMs, Gemini 2.5 Pro and Claude Opus 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd3ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 11:55:17,725 - INFO - Starting news article processing pipeline\n",
      "2025-09-13 11:55:17,729 - INFO - Geographic filtering is ENABLED\n",
      "2025-09-13 11:55:32,591 - INFO - Loaded 86660 English and 85511 Arabic articles\n",
      "2025-09-13 11:55:32,628 - INFO - Loaded 918 unique location aliases\n",
      "2025-09-13 11:55:32,670 - INFO - GPU available: NVIDIA L4\n",
      "2025-09-13 11:55:32,671 - INFO - Loading NER model: Babelscape/wikineural-multilingual-ner\n",
      "Device set to use cuda:0\n",
      "2025-09-13 11:55:33,821 - INFO - NER pipeline initialized successfully\n",
      "2025-09-13 11:55:34,097 - INFO - Running NER on 172,171 articles...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 425\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_eng, df_ara\n\u001b[1;32m    424\u001b[0m \u001b[38;5;66;03m# Usage example with geographic filtering enabled (default)\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m df_eng_processed, df_ara_processed \u001b[38;5;241m=\u001b[39m \u001b[43mrun_news_processing_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_geo_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to False to disable filtering\u001b[39;49;00m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_text_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\n\u001b[1;32m    430\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# Example of accessing processed data\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_eng_processed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m English articles\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 406\u001b[0m, in \u001b[0;36mrun_news_processing_pipeline\u001b[0;34m(data_dir, enable_geo_filter, max_text_length, batch_size, ner_model)\u001b[0m\n\u001b[1;32m    403\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeographic filtering is DISABLED\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# Load data (with geographic filtering if enabled)\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m df_eng, df_ara \u001b[38;5;241m=\u001b[39m \u001b[43mload_news_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_geo_filter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_text_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mner_model\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# Process each dataset\u001b[39;00m\n\u001b[1;32m    411\u001b[0m df_eng \u001b[38;5;241m=\u001b[39m process_dataframe(df_eng, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 213\u001b[0m, in \u001b[0;36mload_news_data\u001b[0;34m(data_dir, enable_geo_filter, max_text_length, batch_size, ner_model)\u001b[0m\n\u001b[1;32m    211\u001b[0m     ner_pipeline \u001b[38;5;241m=\u001b[39m initialize_ner_pipeline(ner_model)\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;66;03m# This now correctly calls the global function\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m     df_combined \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_by_geography\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation_lookup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mner_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_text_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeographic filtering disabled, using all articles\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 143\u001b[0m, in \u001b[0;36mfilter_by_geography\u001b[0;34m(df, location_lookup, ner_pipeline, max_text_length, batch_size)\u001b[0m\n\u001b[1;32m    140\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning NER on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts_to_process)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m articles...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Extract entities\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m all_entities \u001b[38;5;241m=\u001b[39m \u001b[43mner_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts_to_process\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Check for relevant locations\u001b[39;00m\n\u001b[1;32m    146\u001b[0m relevance_mask \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:279\u001b[0m, in \u001b[0;36mTokenClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset_mapping:\n\u001b[1;32m    277\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m offset_mapping\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/base.py:1448\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1445\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1446\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1447\u001b[0m     )\n\u001b[0;32m-> 1448\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:126\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:271\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 271\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/base.py:1375\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1374\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m-> 1375\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_tensor_on_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFramework \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/base.py:1275\u001b[0m, in \u001b[0;36mPipeline._ensure_tensor_on_device\u001b[0;34m(self, inputs, device)\u001b[0m\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ModelOutput(\n\u001b[1;32m   1272\u001b[0m         {name: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(tensor, device) \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m   1273\u001b[0m     )\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m-> 1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {name: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(tensor, device) \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, UserDict):\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserDict({name: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(tensor, device) \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()})\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/base.py:1275\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ModelOutput(\n\u001b[1;32m   1272\u001b[0m         {name: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(tensor, device) \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m   1273\u001b[0m     )\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m-> 1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {name: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_tensor_on_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, UserDict):\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserDict({name: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(tensor, device) \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()})\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/base.py:1283\u001b[0m, in \u001b[0;36mPipeline._ensure_tensor_on_device\u001b[0;34m(self, inputs, device)\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(item, device) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m inputs)\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "News article processing pipeline for multilingual text data.\n",
    "Filters by geographic relevance, then cleans and tokenizes English and Arabic news articles.\n",
    "Functional implementation without classes.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Set, Tuple\n",
    "from transformers import pipeline, Pipeline\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Regex patterns as module constants\n",
    "HTML_PATTERN = re.compile(r'<.*?>')\n",
    "URL_PATTERN = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "WHITESPACE_PATTERN = re.compile(r'\\s+')\n",
    "SENTENCE_SPLIT_PATTERN = re.compile(r'(?<=[.!?۔])\\s+')\n",
    "LETTER_PATTERN = re.compile(r'[a-zA-Zء-ي]')\n",
    "\n",
    "\n",
    "def get_device() -> int:\n",
    "    \"\"\"\n",
    "    Determine the best available device for processing.\n",
    "    \n",
    "    Returns:\n",
    "        int: Device ID (0 for GPU, -1 for CPU).\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "        return 0\n",
    "    else:\n",
    "        logger.info(\"No GPU found, using CPU\")\n",
    "        return -1\n",
    "\n",
    "\n",
    "def load_location_lookup(data_dir: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Load location dictionaries for geographic filtering.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing raw data.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary mapping location names to IDs.\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If location files don't exist.\n",
    "    \"\"\"\n",
    "    raw_dir = Path(data_dir) / '01_raw'\n",
    "    eng_path = raw_dir / 'id_english_location_name.pkl'\n",
    "    ara_path = raw_dir / 'id_arabic_location_name.pkl'\n",
    "    \n",
    "    location_lookup = {}\n",
    "    \n",
    "    try:\n",
    "        with open(eng_path, 'rb') as f:\n",
    "            eng_locations = pickle.load(f)\n",
    "        with open(ara_path, 'rb') as f:\n",
    "            ara_locations = pickle.load(f)\n",
    "        \n",
    "        # Build lookup dictionary\n",
    "        for location_dict in [eng_locations, ara_locations]:\n",
    "            for loc_id, names in location_dict.items():\n",
    "                for name in names:\n",
    "                    location_lookup[name.lower()] = loc_id\n",
    "        \n",
    "        logger.info(f\"Loaded {len(location_lookup):,} unique location aliases\")\n",
    "        return location_lookup\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Location file not found: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def initialize_ner_pipeline(model_name: str = \"Babelscape/wikineural-multilingual-ner\", \n",
    "                           device: Optional[int] = None) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Initialize NER pipeline for geographic filtering.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the NER model to use.\n",
    "        device: Device ID (None for auto-detection).\n",
    "        \n",
    "    Returns:\n",
    "        Pipeline: Initialized NER pipeline.\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If model loading fails.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "        \n",
    "    logger.info(f\"Loading NER model: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        ner_pipeline = pipeline(\n",
    "            \"ner\",\n",
    "            model=model_name,\n",
    "            aggregation_strategy=\"simple\",\n",
    "            device=device\n",
    "        )\n",
    "        logger.info(\"NER pipeline initialized successfully\")\n",
    "        return ner_pipeline\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load NER model: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def filter_by_geography(df: pd.DataFrame, location_lookup: Dict[str, str], \n",
    "                       ner_pipeline: Pipeline, max_text_length: int = 2000,\n",
    "                       batch_size: int = 128) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter articles containing target geographic locations.\n",
    "    \n",
    "    Args:\n",
    "        df: Combined dataframe of articles.\n",
    "        location_lookup: Dictionary mapping location names to IDs.\n",
    "        ner_pipeline: Initialized NER pipeline.\n",
    "        max_text_length: Maximum text length for NER processing.\n",
    "        batch_size: Batch size for NER processing.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered dataframe with location metadata.\n",
    "    \"\"\"\n",
    "    # Prepare texts for NER\n",
    "    article_bodies = df['body'].fillna('').tolist()\n",
    "    texts_to_process = [text[:max_text_length] for text in article_bodies]\n",
    "    \n",
    "    logger.info(f\"Running NER on {len(texts_to_process):,} articles...\")\n",
    "    \n",
    "    # Extract entities\n",
    "    all_entities = ner_pipeline(texts_to_process, batch_size=batch_size)\n",
    "    \n",
    "    # Check for relevant locations\n",
    "    relevance_mask = []\n",
    "    location_matches = []\n",
    "    \n",
    "    for article_entities in tqdm(all_entities, desc=\"Checking location relevance\"):\n",
    "        found_locations = []\n",
    "        \n",
    "        for entity in article_entities:\n",
    "            if (entity.get('entity_group') == 'LOC' and \n",
    "                entity.get('word', '').lower() in location_lookup):\n",
    "                found_locations.append(entity['word'])\n",
    "        \n",
    "        relevance_mask.append(len(found_locations) > 0)\n",
    "        location_matches.append(found_locations)\n",
    "    \n",
    "    # Add metadata and filter\n",
    "    df['matched_locations'] = location_matches\n",
    "    df_filtered = df[relevance_mask].copy()\n",
    "    \n",
    "    logger.info(f\"Geographic filtering complete:\")\n",
    "    logger.info(f\"  - Original articles: {len(df):,}\")\n",
    "    logger.info(f\"  - Relevant articles: {len(df_filtered):,}\")\n",
    "    logger.info(f\"  - Retention rate: {len(df_filtered)/len(df)*100:.1f}%\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def load_news_data(data_dir: str, enable_geo_filter: bool = True,\n",
    "                  max_text_length: int = 2000, batch_size: int = 128,\n",
    "                  ner_model: str = \"Babelscape/wikineural-multilingual-ner\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load English and Arabic news datasets with optional geographic filtering.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing raw data.\n",
    "        enable_geo_filter: Whether to apply geographic filtering.\n",
    "        max_text_length: Maximum text length for NER processing.\n",
    "        batch_size: Batch size for NER processing.\n",
    "        ner_model: Model name for NER pipeline.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: Tuple of (english_df, arabic_df).\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If dataset files don't exist.\n",
    "    \"\"\"\n",
    "    raw_dir = Path(data_dir) / '01_raw'\n",
    "    eng_path = raw_dir / 'news-articles-eng.csv'\n",
    "    ara_path = raw_dir / 'news-articles-ara.csv'\n",
    "    \n",
    "    try:\n",
    "        df_eng = pd.read_csv(eng_path)\n",
    "        df_ara = pd.read_csv(ara_path)\n",
    "        \n",
    "        # Add language column for tracking\n",
    "        df_eng['language'] = 'english'\n",
    "        df_ara['language'] = 'arabic'\n",
    "        \n",
    "        logger.info(f\"Loaded {len(df_eng)} English and {len(df_ara)} Arabic articles\")\n",
    "        \n",
    "        # Combine for geographic filtering\n",
    "        df_combined = pd.concat([df_eng, df_ara], ignore_index=True)\n",
    "        \n",
    "        # Apply geographic filtering if enabled\n",
    "        if enable_geo_filter:\n",
    "            location_lookup = load_location_lookup(data_dir)\n",
    "            ner_pipeline = initialize_ner_pipeline(ner_model)\n",
    "            # This now correctly calls the global function\n",
    "            df_combined = filter_by_geography(\n",
    "                df_combined, location_lookup, ner_pipeline, \n",
    "                max_text_length, batch_size\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\"Geographic filtering disabled, using all articles\")\n",
    "            df_combined['matched_locations'] = [[] for _ in range(len(df_combined))]\n",
    "        \n",
    "        # Split back into language-specific dataframes\n",
    "        df_eng_filtered = df_combined[df_combined['language'] == 'english'].copy()\n",
    "        df_ara_filtered = df_combined[df_combined['language'] == 'arabic'].copy()\n",
    "        \n",
    "        # Remove the language column as it's no longer needed\n",
    "        df_eng_filtered = df_eng_filtered.drop('language', axis=1)\n",
    "        df_ara_filtered = df_ara_filtered.drop('language', axis=1)\n",
    "        \n",
    "        return df_eng_filtered, df_ara_filtered\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Dataset file not found: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def clean_text(text: Optional[str]) -> str:\n",
    "    \"\"\"\n",
    "    Clean raw text by removing HTML tags, URLs, and normalizing whitespace.\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text string to clean.\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text string.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = HTML_PATTERN.sub('', text)\n",
    "    # Remove URLs\n",
    "    text = URL_PATTERN.sub('', text)\n",
    "    # Normalize whitespace\n",
    "    text = WHITESPACE_PATTERN.sub(' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize_sentences(text: Optional[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences using regex and filter non-textual results.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to tokenize.\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of sentence strings.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return []\n",
    "    \n",
    "    # Split on sentence-ending punctuation\n",
    "    sentences = SENTENCE_SPLIT_PATTERN.split(text)\n",
    "    \n",
    "    # Filter empty strings and non-textual content\n",
    "    valid_sentences = [\n",
    "        s for s in sentences \n",
    "        if s and LETTER_PATTERN.search(s)\n",
    "    ]\n",
    "    \n",
    "    return valid_sentences\n",
    "\n",
    "\n",
    "def process_dataframe(df: pd.DataFrame, language: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply cleaning and tokenization to a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'body' column containing article text.\n",
    "        language: Language identifier for logging.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with cleaned text and sentences.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Processing {language} articles...\")\n",
    "    \n",
    "    # Clean text\n",
    "    df['body_cleaned'] = df['body'].apply(clean_text)\n",
    "    logger.info(f\"Cleaned {len(df)} {language} articles\")\n",
    "    \n",
    "    # Tokenize into sentences\n",
    "    df['sentences'] = df['body_cleaned'].apply(tokenize_sentences)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    sentence_counts = df['sentences'].apply(len)\n",
    "    logger.info(f\"Tokenized {language} articles: \"\n",
    "               f\"avg {sentence_counts.mean():.1f} sentences per article\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def save_processed_data(df_eng: pd.DataFrame, df_ara: pd.DataFrame, data_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Save processed dataframes to pickle files.\n",
    "    \n",
    "    Args:\n",
    "        df_eng: Processed English dataframe.\n",
    "        df_ara: Processed Arabic dataframe.\n",
    "        data_dir: Root directory for saving data.\n",
    "        \n",
    "    Returns:\n",
    "        -> None\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If saving fails.\n",
    "    \"\"\"\n",
    "    processed_dir = Path(data_dir) / '02_processed'\n",
    "    processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    eng_path = processed_dir / 'news_eng_processed.pkl'\n",
    "    ara_path = processed_dir / 'news_ara_processed.pkl'\n",
    "    \n",
    "    try:\n",
    "        df_eng.to_pickle(eng_path)\n",
    "        df_ara.to_pickle(ara_path)\n",
    "        logger.info(f\"Saved English data to: {eng_path}\")\n",
    "        logger.info(f\"Saved Arabic data to: {ara_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving processed data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def display_sample(df: pd.DataFrame, num_sentences: int = 3) -> None:\n",
    "    \"\"\"\n",
    "    Display sample processed sentences for verification.\n",
    "    \n",
    "    Args:\n",
    "        df: Processed dataframe.\n",
    "        num_sentences: Number of sentences to display.\n",
    "        \n",
    "    Returns:\n",
    "        -> None\n",
    "    \"\"\"\n",
    "    if df.empty or 'sentences' not in df.columns:\n",
    "        logger.warning(\"No sentences to display\")\n",
    "        return\n",
    "    \n",
    "    first_article = df.iloc[0]\n",
    "    sentences = first_article['sentences']\n",
    "    \n",
    "    print(f\"\\n--- Sample Tokenization Results ---\")\n",
    "    print(f\"Article split into {len(sentences)} sentences\")\n",
    "    print(f\"First {min(num_sentences, len(sentences))} sentences:\")\n",
    "    \n",
    "    for i, sentence in enumerate(sentences[:num_sentences], 1):\n",
    "        print(f\"  {i}. {sentence}\")\n",
    "    \n",
    "    # Show matched locations if available\n",
    "    if 'matched_locations' in df.columns and first_article['matched_locations']:\n",
    "        print(f\"\\nMatched locations: {', '.join(first_article['matched_locations'])}\")\n",
    "\n",
    "\n",
    "def run_news_processing_pipeline(data_dir: str = '../data', \n",
    "                               enable_geo_filter: bool = True,\n",
    "                               max_text_length: int = 2000,\n",
    "                               batch_size: int = 128,\n",
    "                               ner_model: str = \"Babelscape/wikineural-multilingual-ner\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Execute the complete news processing pipeline.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing raw and processed data folders.\n",
    "        enable_geo_filter: Whether to apply geographic filtering.\n",
    "        max_text_length: Maximum text length for NER processing.\n",
    "        batch_size: Batch size for NER processing.\n",
    "        ner_model: Model name for NER pipeline.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: Tuple of processed (english_df, arabic_df).\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If required files don't exist.\n",
    "        Exception: If processing fails.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting news article processing pipeline\")\n",
    "    \n",
    "    if enable_geo_filter:\n",
    "        logger.info(\"Geographic filtering is ENABLED\")\n",
    "    else:\n",
    "        logger.info(\"Geographic filtering is DISABLED\")\n",
    "    \n",
    "    # Load data (with geographic filtering if enabled)\n",
    "    df_eng, df_ara = load_news_data(\n",
    "        data_dir, enable_geo_filter, max_text_length, batch_size, ner_model\n",
    "    )\n",
    "    \n",
    "    # Process each dataset\n",
    "    df_eng = process_dataframe(df_eng, \"English\")\n",
    "    df_ara = process_dataframe(df_ara, \"Arabic\")\n",
    "    \n",
    "    # Display sample for verification\n",
    "    display_sample(df_eng)\n",
    "    \n",
    "    # Save processed data\n",
    "    save_processed_data(df_eng, df_ara, data_dir)\n",
    "    \n",
    "    logger.info(\"Pipeline completed successfully\")\n",
    "    return df_eng, df_ara\n",
    "\n",
    "\n",
    "# Usage example with geographic filtering enabled (default)\n",
    "df_eng_processed, df_ara_processed = run_news_processing_pipeline(\n",
    "    data_dir='../data',\n",
    "    enable_geo_filter=True,  # Set to False to disable filtering\n",
    "    max_text_length=2000,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "# Example of accessing processed data\n",
    "print(f\"\\nProcessed {len(df_eng_processed)} English articles\")\n",
    "print(f\"Processed {len(df_ara_processed)} Arabic articles\")\n",
    "\n",
    "# Check if location data is available\n",
    "if 'matched_locations' in df_eng_processed.columns:\n",
    "    total_locations = sum(len(locs) for locs in df_eng_processed['matched_locations'])\n",
    "    print(f\"Total location matches in English articles: {total_locations}\")\n",
    "\n",
    "\n",
    "# Alternative usage examples:\n",
    "\n",
    "# Process without geographic filtering\n",
    "df_eng_all, df_ara_all = run_news_processing_pipeline(\n",
    "    data_dir='../data',\n",
    "    enable_geo_filter=False\n",
    ")\n",
    "\n",
    "# Process with custom parameters\n",
    "df_eng_custom, df_ara_custom = run_news_processing_pipeline(\n",
    "    data_dir='../data',\n",
    "    enable_geo_filter=True,\n",
    "    max_text_length=2000,\n",
    "    batch_size=2048,\n",
    "    ner_model=\"Babelscape/wikineural-multilingual-ner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b647556e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 11:58:23,351 - INFO - Starting risk factor extraction pipeline\n",
      "2025-09-13 11:58:23,352 - INFO - Running in SAMPLE mode (10 articles)\n",
      "2025-09-13 11:58:23,354 - ERROR - Filtered data not found: [Errno 2] No such file or directory: '../data/02_processed/news_geographically_filtered.pkl'\n",
      "2025-09-13 11:58:23,355 - ERROR - Please run geographic filtering first\n",
      "2025-09-13 11:58:23,356 - ERROR - Pipeline failed: [Errno 2] No such file or directory: '../data/02_processed/news_geographically_filtered.pkl'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/02_processed/news_geographically_filtered.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 524\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# Usage example - Sample mode (for testing)\u001b[39;00m\n\u001b[0;32m--> 524\u001b[0m df_risk_mentions_sample \u001b[38;5;241m=\u001b[39m \u001b[43mrun_risk_extraction_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43msentence_similarity_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.55\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier_confidence_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.90\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\n\u001b[1;32m    531\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# Display sample results if available\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df_risk_mentions_sample\u001b[38;5;241m.\u001b[39mempty:\n",
      "Cell \u001b[0;32mIn[1], line 471\u001b[0m, in \u001b[0;36mrun_risk_extraction_pipeline\u001b[0;34m(data_dir, classifier_model, embedder_model, classifier_batch_size, sentence_similarity_threshold, classifier_confidence_threshold, use_sample, sample_size)\u001b[0m\n\u001b[1;32m    467\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning in FULL mode (all articles)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m--> 471\u001b[0m     df_articles \u001b[38;5;241m=\u001b[39m \u001b[43mload_filtered_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     total_articles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df_articles)\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# Load risk factors\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 59\u001b[0m, in \u001b[0;36mload_filtered_data\u001b[0;34m(data_dir, use_sample, sample_size)\u001b[0m\n\u001b[1;32m     56\u001b[0m filtered_path \u001b[38;5;241m=\u001b[39m processed_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews_geographically_filtered.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m geographically filtered articles\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_sample:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pandas/io/pickle.py:189\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pandas/io/common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/02_processed/news_geographically_filtered.pkl'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Risk factor extraction pipeline for geographically filtered news articles.\n",
    "Uses zero-shot classification to identify risk factors in article sentences.\n",
    "Functional implementation without classes.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from transformers import pipeline, Pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_device() -> int:\n",
    "    \"\"\"\n",
    "    Determine the best available device for processing.\n",
    "    \n",
    "    Returns:\n",
    "        Device ID (0 for GPU, -1 for CPU)\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device_name = torch.cuda.get_device_name(0)\n",
    "        logger.info(f\"GPU available: {device_name}\")\n",
    "        return 0\n",
    "    else:\n",
    "        logger.info(\"No GPU found, using CPU\")\n",
    "        return -1\n",
    "\n",
    "\n",
    "def load_filtered_data(data_dir: str, use_sample: bool = True, sample_size: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load geographically filtered article data.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing processed data\n",
    "        use_sample: Whether to use a sample of articles for testing\n",
    "        sample_size: Number of articles to sample if use_sample is True\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame of filtered articles\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If filtered data doesn't exist\n",
    "    \"\"\"\n",
    "    processed_dir = Path(data_dir) / '02_processed'\n",
    "    filtered_path = processed_dir / 'news_geographically_filtered.pkl'\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_pickle(filtered_path)\n",
    "        logger.info(f\"Loaded {len(df):,} geographically filtered articles\")\n",
    "        \n",
    "        if use_sample:\n",
    "            df = df.head(sample_size).copy()\n",
    "            logger.info(f\"Using sample of {len(df)} articles for testing\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Filtered data not found: {e}\")\n",
    "        logger.error(\"Please run geographic filtering first\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_risk_factors(data_dir: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load risk factor labels from Excel file.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing raw data\n",
    "        \n",
    "    Returns:\n",
    "        List of risk factor labels\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If risk factors file doesn't exist\n",
    "    \"\"\"\n",
    "    raw_dir = Path(data_dir) / '01_raw'\n",
    "    risk_path = raw_dir / 'risk-factors.xlsx'\n",
    "    \n",
    "    try:\n",
    "        df_risk = pd.read_excel(risk_path)\n",
    "        df_risk.dropna(subset=['risk_factor_english'], inplace=True)\n",
    "        \n",
    "        risk_factor_labels = df_risk['risk_factor_english'].tolist()\n",
    "        logger.info(f\"Loaded {len(risk_factor_labels)} risk factors\")\n",
    "        \n",
    "        return risk_factor_labels\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Risk factors file not found: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def initialize_classifier(model_name: str = 'MoritzLaurer/deberta-v3-xsmall-zeroshot-v1.1-all-33',\n",
    "                         device: Optional[int] = None) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Initialize zero-shot classification pipeline.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the classification model to use\n",
    "        device: Device ID (None for auto-detection)\n",
    "        \n",
    "    Returns:\n",
    "        Initialized classification pipeline\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If model loading fails\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "        \n",
    "    try:\n",
    "        classifier = pipeline(\n",
    "            \"zero-shot-classification\",\n",
    "            model=model_name,\n",
    "            device=device\n",
    "        )\n",
    "        logger.info(f\"Classifier initialized: {model_name}\")\n",
    "        return classifier\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize classifier: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def initialize_embedder(model_name: str = 'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "                       device: Optional[int] = None) -> SentenceTransformer:\n",
    "    \"\"\"\n",
    "    Initialize sentence embedding model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the embedding model to use\n",
    "        device: Device ID (None for auto-detection)\n",
    "        \n",
    "    Returns:\n",
    "        Initialized sentence transformer\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If model loading fails\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "        \n",
    "    try:\n",
    "        embedder = SentenceTransformer(\n",
    "            model_name,\n",
    "            device='cuda' if device == 0 else 'cpu'\n",
    "        )\n",
    "        logger.info(f\"Embedder initialized: {model_name}\")\n",
    "        return embedder\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize embedder: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def prepare_sentences(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare sentences from articles for processing.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with articles containing 'sentences' column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with individual sentences\n",
    "    \"\"\"\n",
    "    # Add article ID if not present\n",
    "    if 'article_id' not in df.columns:\n",
    "        df['article_id'] = df.index\n",
    "    \n",
    "    # Explode sentences into individual rows\n",
    "    df_sentences = df.explode('sentences').rename(columns={'sentences': 'sentence_text'})\n",
    "    df_sentences = df_sentences[['article_id', 'date', 'sentence_text']].dropna(subset=['sentence_text'])\n",
    "    \n",
    "    logger.info(f\"Prepared {len(df_sentences):,} sentences from articles\")\n",
    "    \n",
    "    return df_sentences\n",
    "\n",
    "\n",
    "def filter_relevant_sentences(df_sentences: pd.DataFrame, \n",
    "                             embedder: SentenceTransformer,\n",
    "                             risk_factor_labels: List[str],\n",
    "                             similarity_threshold: float = 0.55) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pre-filter sentences using semantic similarity to risk factors.\n",
    "    \n",
    "    Args:\n",
    "        df_sentences: DataFrame with sentence data\n",
    "        embedder: Initialized sentence transformer\n",
    "        risk_factor_labels: List of risk factor labels\n",
    "        similarity_threshold: Minimum similarity score for filtering\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with filtered relevant sentences\n",
    "    \"\"\"\n",
    "    if df_sentences.empty:\n",
    "        return df_sentences\n",
    "    \n",
    "    sentences = df_sentences['sentence_text'].tolist()\n",
    "    \n",
    "    logger.info(f\"Pre-filtering {len(sentences):,} sentences...\")\n",
    "    logger.info(f\"Similarity threshold: {similarity_threshold}\")\n",
    "    \n",
    "    # Pre-compute risk factor embeddings\n",
    "    risk_factor_embeddings = embedder.encode(\n",
    "        risk_factor_labels,\n",
    "        convert_to_tensor=True\n",
    "    )\n",
    "    \n",
    "    # Encode sentences\n",
    "    sentence_embeddings = embedder.encode(\n",
    "        sentences,\n",
    "        convert_to_tensor=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Find semantically similar sentences to risk factors\n",
    "    hits = util.semantic_search(\n",
    "        sentence_embeddings,\n",
    "        risk_factor_embeddings,\n",
    "        top_k=1\n",
    "    )\n",
    "    \n",
    "    # Filter by similarity threshold\n",
    "    relevant_indices = [\n",
    "        i for i, hit_list in enumerate(hits)\n",
    "        if hit_list and hit_list[0]['score'] >= similarity_threshold\n",
    "    ]\n",
    "    \n",
    "    df_filtered = df_sentences.iloc[relevant_indices].copy()\n",
    "    \n",
    "    logger.info(f\"Reduced to {len(df_filtered):,} relevant sentences \"\n",
    "               f\"({len(df_filtered)/len(sentences)*100:.1f}% retention)\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def classify_sentences(df_sentences: pd.DataFrame,\n",
    "                      classifier: Pipeline,\n",
    "                      risk_factor_labels: List[str],\n",
    "                      confidence_threshold: float = 0.90,\n",
    "                      batch_size: int = 128) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Classify filtered sentences for risk factors.\n",
    "    \n",
    "    Args:\n",
    "        df_sentences: DataFrame with filtered sentences\n",
    "        classifier: Initialized classification pipeline\n",
    "        risk_factor_labels: List of risk factor labels\n",
    "        confidence_threshold: Minimum confidence score for classification\n",
    "        batch_size: Batch size for classification\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with risk factor classifications\n",
    "    \"\"\"\n",
    "    if df_sentences.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    sentences = df_sentences['sentence_text'].tolist()\n",
    "    \n",
    "    logger.info(f\"Classifying {len(sentences):,} sentences...\")\n",
    "    logger.info(f\"Confidence threshold: {confidence_threshold}\")\n",
    "    \n",
    "    results_list = []\n",
    "    \n",
    "    # Run classification in batches\n",
    "    for i, result in tqdm(\n",
    "        enumerate(classifier(\n",
    "            sentences,\n",
    "            risk_factor_labels,\n",
    "            multi_label=True,\n",
    "            batch_size=batch_size\n",
    "        )),\n",
    "        total=len(sentences),\n",
    "        desc=\"Classifying sentences\"\n",
    "    ):\n",
    "        # Check each label's confidence\n",
    "        for label, score in zip(result['labels'], result['scores']):\n",
    "            if score >= confidence_threshold:\n",
    "                original_row = df_sentences.iloc[i]\n",
    "                results_list.append({\n",
    "                    'article_id': original_row['article_id'],\n",
    "                    'date': original_row['date'],\n",
    "                    'sentence_text': result['sequence'],\n",
    "                    'risk_factor': label,\n",
    "                    'confidence_score': score\n",
    "                })\n",
    "    \n",
    "    df_results = pd.DataFrame(results_list)\n",
    "    \n",
    "    logger.info(f\"Found {len(df_results):,} risk factor mentions\")\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "\n",
    "def refine_results(df_mentions: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Post-process results to keep only highest confidence mention per sentence.\n",
    "    \n",
    "    Args:\n",
    "        df_mentions: DataFrame with all risk mentions\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with refined risk mentions\n",
    "    \"\"\"\n",
    "    if df_mentions.empty:\n",
    "        return df_mentions\n",
    "    \n",
    "    logger.info(\"Refining results to highest confidence per sentence...\")\n",
    "    \n",
    "    # Keep only the highest confidence label for each sentence\n",
    "    idx = df_mentions.groupby('sentence_text')['confidence_score'].idxmax()\n",
    "    df_refined = df_mentions.loc[idx].copy()\n",
    "    \n",
    "    # Sort by confidence score\n",
    "    df_refined = df_refined.sort_values('confidence_score', ascending=False)\n",
    "    \n",
    "    logger.info(f\"Refined from {len(df_mentions):,} to {len(df_refined):,} unique mentions\")\n",
    "    \n",
    "    return df_refined\n",
    "\n",
    "\n",
    "def save_results(df_results: pd.DataFrame, data_dir: str, use_sample: bool = True) -> Path:\n",
    "    \"\"\"\n",
    "    Save extraction results to CSV.\n",
    "    \n",
    "    Args:\n",
    "        df_results: DataFrame with risk factor mentions\n",
    "        data_dir: Root directory for saving data\n",
    "        use_sample: Whether this is sample data (affects filename)\n",
    "        \n",
    "    Returns:\n",
    "        Path to saved file\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If saving fails\n",
    "    \"\"\"\n",
    "    models_dir = Path(data_dir) / '03_models'\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if use_sample:\n",
    "        filename = 'risk_mentions_SAMPLE.csv'\n",
    "    else:\n",
    "        filename = 'risk_mentions_FULL.csv'\n",
    "    \n",
    "    output_path = models_dir / filename\n",
    "    \n",
    "    try:\n",
    "        df_results.to_csv(output_path, index=False)\n",
    "        logger.info(f\"Saved {len(df_results):,} risk mentions to {output_path}\")\n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save results: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def calculate_statistics(df_results: pd.DataFrame, \n",
    "                        total_articles: int,\n",
    "                        total_sentences: int,\n",
    "                        filtered_sentences: int,\n",
    "                        risk_mentions: int) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate statistics about the extraction.\n",
    "    \n",
    "    Args:\n",
    "        df_results: DataFrame with extraction results\n",
    "        total_articles: Total number of articles processed\n",
    "        total_sentences: Total number of sentences extracted\n",
    "        filtered_sentences: Number of sentences after filtering\n",
    "        risk_mentions: Number of risk mentions found\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of statistics\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'total_articles': total_articles,\n",
    "        'total_sentences': total_sentences,\n",
    "        'filtered_sentences': filtered_sentences,\n",
    "        'risk_mentions': risk_mentions,\n",
    "        'unique_mentions': len(df_results)\n",
    "    }\n",
    "    \n",
    "    if not df_results.empty:\n",
    "        stats.update({\n",
    "            'unique_risk_factors': df_results['risk_factor'].nunique(),\n",
    "            'avg_confidence': df_results['confidence_score'].mean(),\n",
    "            'min_confidence': df_results['confidence_score'].min(),\n",
    "            'max_confidence': df_results['confidence_score'].max(),\n",
    "            'top_risk_factors': df_results['risk_factor'].value_counts().head(5).to_dict()\n",
    "        })\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "def display_statistics(stats: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Display extraction statistics.\n",
    "    \n",
    "    Args:\n",
    "        stats: Dictionary containing statistics\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EXTRACTION STATISTICS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total articles processed: {stats['total_articles']:,}\")\n",
    "    print(f\"Total sentences extracted: {stats['total_sentences']:,}\")\n",
    "    print(f\"Sentences after filtering: {stats['filtered_sentences']:,}\")\n",
    "    print(f\"Risk mentions found: {stats['risk_mentions']:,}\")\n",
    "    print(f\"Unique mentions (refined): {stats['unique_mentions']:,}\")\n",
    "    \n",
    "    if 'unique_risk_factors' in stats:\n",
    "        print(f\"\\nUnique risk factors: {stats['unique_risk_factors']}\")\n",
    "        print(f\"Average confidence: {stats['avg_confidence']:.3f}\")\n",
    "        print(f\"Confidence range: {stats['min_confidence']:.3f} - {stats['max_confidence']:.3f}\")\n",
    "        \n",
    "        if stats.get('top_risk_factors'):\n",
    "            print(\"\\nTop 5 risk factors:\")\n",
    "            for risk, count in stats['top_risk_factors'].items():\n",
    "                print(f\"  - {risk}: {count} mentions\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "def run_risk_extraction_pipeline(data_dir: str = '../data',\n",
    "                               classifier_model: str = 'MoritzLaurer/deberta-v3-xsmall-zeroshot-v1.1-all-33',\n",
    "                               embedder_model: str = 'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "                               classifier_batch_size: int = 128,\n",
    "                               sentence_similarity_threshold: float = 0.55,\n",
    "                               classifier_confidence_threshold: float = 0.90,\n",
    "                               use_sample: bool = True,\n",
    "                               sample_size: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Execute the complete risk factor extraction pipeline.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing data folders\n",
    "        classifier_model: Name of the classification model to use\n",
    "        embedder_model: Name of the embedding model to use\n",
    "        classifier_batch_size: Batch size for classification\n",
    "        sentence_similarity_threshold: Minimum similarity score for sentence filtering\n",
    "        classifier_confidence_threshold: Minimum confidence score for classification\n",
    "        use_sample: Whether to use a sample of articles for testing\n",
    "        sample_size: Number of articles to sample if use_sample is True\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with refined risk factor mentions\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If required files don't exist\n",
    "        Exception: If processing fails\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting risk factor extraction pipeline\")\n",
    "    \n",
    "    if use_sample:\n",
    "        logger.info(f\"Running in SAMPLE mode ({sample_size} articles)\")\n",
    "    else:\n",
    "        logger.info(\"Running in FULL mode (all articles)\")\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        df_articles = load_filtered_data(data_dir, use_sample, sample_size)\n",
    "        total_articles = len(df_articles)\n",
    "        \n",
    "        # Load risk factors\n",
    "        risk_factor_labels = load_risk_factors(data_dir)\n",
    "        \n",
    "        # Initialize models\n",
    "        classifier = initialize_classifier(classifier_model)\n",
    "        embedder = initialize_embedder(embedder_model)\n",
    "        \n",
    "        # Prepare sentences\n",
    "        df_sentences = prepare_sentences(df_articles)\n",
    "        total_sentences = len(df_sentences)\n",
    "        \n",
    "        if df_sentences.empty:\n",
    "            logger.warning(\"No sentences to process\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Filter relevant sentences\n",
    "        df_filtered = filter_relevant_sentences(\n",
    "            df_sentences, embedder, risk_factor_labels, sentence_similarity_threshold\n",
    "        )\n",
    "        filtered_sentences = len(df_filtered)\n",
    "        \n",
    "        # Classify sentences\n",
    "        df_mentions = classify_sentences(\n",
    "            df_filtered, classifier, risk_factor_labels, \n",
    "            classifier_confidence_threshold, classifier_batch_size\n",
    "        )\n",
    "        risk_mentions = len(df_mentions)\n",
    "        \n",
    "        # Refine results\n",
    "        df_refined = refine_results(df_mentions)\n",
    "        \n",
    "        # Save results\n",
    "        save_results(df_refined, data_dir, use_sample)\n",
    "        \n",
    "        # Display statistics\n",
    "        stats = calculate_statistics(\n",
    "            df_refined, total_articles, total_sentences, \n",
    "            filtered_sentences, risk_mentions\n",
    "        )\n",
    "        display_statistics(stats)\n",
    "        \n",
    "        logger.info(\"Pipeline completed successfully\")\n",
    "        return df_refined\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline failed: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Usage example - Sample mode (for testing)\n",
    "df_risk_mentions_sample = run_risk_extraction_pipeline(\n",
    "    data_dir='../data',\n",
    "    use_sample=True,\n",
    "    sample_size=10,\n",
    "    sentence_similarity_threshold=0.55,\n",
    "    classifier_confidence_threshold=0.90,\n",
    "    classifier_batch_size=128\n",
    ")\n",
    "\n",
    "# Display sample results if available\n",
    "if not df_risk_mentions_sample.empty:\n",
    "    print(\"\\n--- Sample Risk Mentions ---\")\n",
    "    print(df_risk_mentions_sample[['risk_factor', 'confidence_score', 'sentence_text']].head())\n",
    "\n",
    "\n",
    "# Alternative usage examples:\n",
    "\n",
    "# Full processing mode\n",
    "# df_risk_mentions_full = run_risk_extraction_pipeline(\n",
    "#     data_dir='../data',\n",
    "#     use_sample=False,\n",
    "#     sentence_similarity_threshold=0.55,\n",
    "#     classifier_confidence_threshold=0.90\n",
    "# )\n",
    "\n",
    "# Custom model configuration\n",
    "# df_risk_mentions_custom = run_risk_extraction_pipeline(\n",
    "#     data_dir='../data',\n",
    "#     classifier_model='MoritzLaurer/deberta-v3-xsmall-zeroshot-v1.1-all-33',\n",
    "#     embedder_model='paraphrase-multilingual-MiniLM-L12-v2',\n",
    "#     use_sample=True,\n",
    "#     sample_size=5,\n",
    "#     sentence_similarity_threshold=0.60,\n",
    "#     classifier_confidence_threshold=0.85\n",
    "# )\n",
    "\n",
    "# Processing with different thresholds for experimentation\n",
    "# df_risk_mentions_strict = run_risk_extraction_pipeline(\n",
    "#     data_dir='../data',\n",
    "#     use_sample=True,\n",
    "#     sample_size=20,\n",
    "#     sentence_similarity_threshold=0.70,  # Higher similarity threshold\n",
    "#     classifier_confidence_threshold=0.95  # Higher confidence threshold\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5e0c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Risk mention geotagging pipeline for multilingual news articles.\n",
    "Extracts and resolves geographic locations from risk-related sentences using NER,\n",
    "applying hierarchical logic to maximize precision.\n",
    "Functional implementation without classes.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple, Optional\n",
    "from transformers import pipeline, Pipeline\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_device() -> int:\n",
    "    \"\"\"\n",
    "    Determine the best available device for processing.\n",
    "    \n",
    "    Returns:\n",
    "        Device ID (0 for GPU, -1 for CPU)\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "        return 0\n",
    "    else:\n",
    "        logger.info(\"No GPU found, using CPU\")\n",
    "        return -1\n",
    "\n",
    "\n",
    "def load_risk_mentions(data_dir: str, sample_size: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load risk mention sentences from processed data.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing data folders\n",
    "        sample_size: Number of rows to process (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing risk mentions\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If risk mentions file doesn't exist\n",
    "    \"\"\"\n",
    "    models_dir = Path(data_dir) / '03_models'\n",
    "    risk_path = models_dir / 'risk_mentions_SAMPLE_FINAL.csv'\n",
    "    \n",
    "    try:\n",
    "        df_risk = pd.read_csv(risk_path)\n",
    "        \n",
    "        # Apply sampling if specified\n",
    "        if sample_size:\n",
    "            df_risk = df_risk.head(sample_size).copy()\n",
    "            logger.info(f\"Sampling {sample_size} rows for processing\")\n",
    "        \n",
    "        logger.info(f\"Loaded {len(df_risk):,} risk mention sentences\")\n",
    "        return df_risk\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Risk mentions file not found: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_articles(data_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and combine English and Arabic article datasets.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing data folders\n",
    "        \n",
    "    Returns:\n",
    "        Combined DataFrame with article content\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If article files don't exist\n",
    "    \"\"\"\n",
    "    processed_dir = Path(data_dir) / '02_processed'\n",
    "    eng_path = processed_dir / 'news_eng_processed.pkl'\n",
    "    ara_path = processed_dir / 'news_ara_processed.pkl'\n",
    "    \n",
    "    try:\n",
    "        # Load datasets\n",
    "        df_eng = pd.read_pickle(eng_path)\n",
    "        df_eng['language'] = 'english'\n",
    "        \n",
    "        df_ara = pd.read_pickle(ara_path)\n",
    "        df_ara['language'] = 'arabic'\n",
    "        \n",
    "        # Ensure article IDs exist\n",
    "        if 'article_id' not in df_eng.columns:\n",
    "            df_eng['article_id'] = df_eng.index\n",
    "        if 'article_id' not in df_ara.columns:\n",
    "            df_ara['article_id'] = df_ara.index\n",
    "        \n",
    "        # Combine datasets\n",
    "        df_articles = pd.concat([df_eng, df_ara], ignore_index=True)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(df_eng):,} English and {len(df_ara):,} Arabic articles\")\n",
    "        return df_articles\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Article file not found: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def build_location_resolvers(data_dir: str) -> Tuple[Dict[str, str], Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Build lookup dictionaries for location name resolution.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing raw data\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (location_lookup, id_to_english_name) dictionaries\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If location files don't exist\n",
    "    \"\"\"\n",
    "    raw_dir = Path(data_dir) / '01_raw'\n",
    "    eng_path = raw_dir / 'id_english_location_name.pkl'\n",
    "    ara_path = raw_dir / 'id_arabic_location_name.pkl'\n",
    "    \n",
    "    try:\n",
    "        # Load location dictionaries\n",
    "        with open(eng_path, 'rb') as f:\n",
    "            eng_locations = pickle.load(f)\n",
    "        with open(ara_path, 'rb') as f:\n",
    "            ara_locations = pickle.load(f)\n",
    "        \n",
    "        # Build name-to-ID lookup\n",
    "        location_lookup = {}\n",
    "        for location_dict in [eng_locations, ara_locations]:\n",
    "            for loc_id, names in location_dict.items():\n",
    "                for name in names:\n",
    "                    location_lookup[name.lower()] = loc_id\n",
    "        \n",
    "        # Build ID-to-English name lookup\n",
    "        id_to_english_name = {\n",
    "            loc_id: names[0] \n",
    "            for loc_id, names in eng_locations.items()\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Built location resolvers with {len(location_lookup):,} aliases\")\n",
    "        \n",
    "        return location_lookup, id_to_english_name\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Location dictionary not found: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def initialize_ner_pipeline(model_name: str = \"Babelscape/wikineural-multilingual-ner\", \n",
    "                           device: Optional[int] = None) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Initialize the NER pipeline for entity extraction.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the NER model to use\n",
    "        device: Device ID (None for auto-detection)\n",
    "        \n",
    "    Returns:\n",
    "        Initialized NER pipeline\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If model loading fails\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "        \n",
    "    logger.info(f\"Loading NER model: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        ner_pipeline = pipeline(\n",
    "            \"ner\",\n",
    "            model=model_name,\n",
    "            aggregation_strategy=\"simple\",\n",
    "            device=device\n",
    "        )\n",
    "        logger.info(\"NER pipeline initialized successfully\")\n",
    "        return ner_pipeline\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load NER model: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def resolve_locations_from_entities(entities: List[Dict], \n",
    "                                   location_lookup: Dict[str, str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract and resolve location IDs from NER entities.\n",
    "    \n",
    "    Args:\n",
    "        entities: List of entity dictionaries from NER pipeline\n",
    "        location_lookup: Dictionary mapping location names to IDs\n",
    "        \n",
    "    Returns:\n",
    "        List of resolved location IDs\n",
    "    \"\"\"\n",
    "    found_ids = set()\n",
    "    \n",
    "    for entity in entities:\n",
    "        if entity.get('entity_group') == 'LOC':\n",
    "            loc_name_lower = entity.get('word', '').lower()\n",
    "            if loc_name_lower in location_lookup:\n",
    "                found_ids.add(location_lookup[loc_name_lower])\n",
    "    \n",
    "    return list(found_ids)\n",
    "\n",
    "\n",
    "def extract_article_locations(df_articles: pd.DataFrame,\n",
    "                             ner_pipeline: Pipeline,\n",
    "                             location_lookup: Dict[str, str],\n",
    "                             batch_size: int = 128) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract locations from article bodies using batch NER processing.\n",
    "    \n",
    "    Args:\n",
    "        df_articles: DataFrame containing article bodies\n",
    "        ner_pipeline: Initialized NER pipeline\n",
    "        location_lookup: Dictionary mapping location names to IDs\n",
    "        batch_size: Batch size for NER processing\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with article_locations column added\n",
    "    \"\"\"\n",
    "    logger.info(\"Extracting article-level locations...\")\n",
    "    \n",
    "    # Prepare texts for processing\n",
    "    article_bodies = df_articles['body'].fillna('').tolist()\n",
    "    \n",
    "    # Batch process with NER\n",
    "    article_entities = ner_pipeline(\n",
    "        article_bodies, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Resolve locations for each article\n",
    "    article_locations = [\n",
    "        resolve_locations_from_entities(entities, location_lookup) \n",
    "        for entities in tqdm(article_entities, desc=\"Resolving article locations\")\n",
    "    ]\n",
    "    \n",
    "    df_articles = df_articles.copy()\n",
    "    df_articles['article_locations'] = article_locations\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_locations = sum(len(locs) for locs in article_locations)\n",
    "    articles_with_locations = sum(1 for locs in article_locations if locs)\n",
    "    \n",
    "    logger.info(f\"Found {total_locations:,} total locations in \"\n",
    "               f\"{articles_with_locations:,} articles\")\n",
    "    \n",
    "    return df_articles\n",
    "\n",
    "\n",
    "def extract_sentence_locations(df_sentences: pd.DataFrame,\n",
    "                              ner_pipeline: Pipeline,\n",
    "                              location_lookup: Dict[str, str],\n",
    "                              batch_size: int = 128) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract locations from individual sentences using batch NER processing.\n",
    "    \n",
    "    Args:\n",
    "        df_sentences: DataFrame containing sentence texts\n",
    "        ner_pipeline: Initialized NER pipeline\n",
    "        location_lookup: Dictionary mapping location names to IDs\n",
    "        batch_size: Batch size for NER processing\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with sentence_locations column added\n",
    "    \"\"\"\n",
    "    logger.info(\"Extracting sentence-level locations...\")\n",
    "    \n",
    "    # Prepare texts for processing\n",
    "    sentence_texts = df_sentences['sentence_text'].fillna('').tolist()\n",
    "    \n",
    "    # Batch process with NER\n",
    "    sentence_entities = ner_pipeline(\n",
    "        sentence_texts,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Resolve locations for each sentence\n",
    "    sentence_locations = [\n",
    "        resolve_locations_from_entities(entities, location_lookup)\n",
    "        for entities in tqdm(sentence_entities, desc=\"Resolving sentence locations\")\n",
    "    ]\n",
    "    \n",
    "    df_sentences = df_sentences.copy()\n",
    "    df_sentences['sentence_locations'] = sentence_locations\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_locations = sum(len(locs) for locs in sentence_locations)\n",
    "    sentences_with_locations = sum(1 for locs in sentence_locations if locs)\n",
    "    \n",
    "    logger.info(f\"Found {total_locations:,} total locations in \"\n",
    "               f\"{sentences_with_locations:,} sentences\")\n",
    "    \n",
    "    return df_sentences\n",
    "\n",
    "\n",
    "def apply_hierarchical_logic(row: pd.Series) -> List[str]:\n",
    "    \"\"\"\n",
    "    Apply hierarchical location selection logic.\n",
    "    \n",
    "    Priority order:\n",
    "    1. Specific sentence locations (ID length > 2)\n",
    "    2. Any sentence locations\n",
    "    3. Specific article locations (ID length > 2)\n",
    "    4. Any article locations\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row with location columns\n",
    "        \n",
    "    Returns:\n",
    "        List of selected location IDs\n",
    "    \"\"\"\n",
    "    # Check for specific sentence locations\n",
    "    sentence_specific = {\n",
    "        loc for loc in row['sentence_locations'] \n",
    "        if len(loc) > 2\n",
    "    }\n",
    "    if sentence_specific:\n",
    "        return list(sentence_specific)\n",
    "    \n",
    "    # Use any sentence locations\n",
    "    if row['sentence_locations']:\n",
    "        return row['sentence_locations']\n",
    "    \n",
    "    # Check for specific article locations\n",
    "    article_specific = {\n",
    "        loc for loc in row['article_locations'] \n",
    "        if len(loc) > 2\n",
    "    }\n",
    "    if article_specific:\n",
    "        return list(article_specific)\n",
    "    \n",
    "    # Fall back to any article locations\n",
    "    return row['article_locations']\n",
    "\n",
    "\n",
    "def merge_and_finalize(df_risk: pd.DataFrame,\n",
    "                      df_articles: pd.DataFrame,\n",
    "                      id_to_english_name: Dict[str, str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge risk mentions with locations and apply hierarchical logic.\n",
    "    \n",
    "    Args:\n",
    "        df_risk: DataFrame with risk mentions and sentence locations\n",
    "        df_articles: DataFrame with article locations\n",
    "        id_to_english_name: Dictionary mapping location IDs to English names\n",
    "        \n",
    "    Returns:\n",
    "        Final DataFrame with one row per risk-location pair\n",
    "    \"\"\"\n",
    "    logger.info(\"Applying hierarchical logic and finalizing data...\")\n",
    "    \n",
    "    # Merge sentence and article data\n",
    "    df_merged = pd.merge(\n",
    "        df_risk,\n",
    "        df_articles[['article_id', 'language', 'article_locations']],\n",
    "        on='article_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Apply hierarchical location selection\n",
    "    df_merged['final_locations'] = df_merged.apply(\n",
    "        apply_hierarchical_logic, \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Explode to one row per location\n",
    "    df_exploded = df_merged.explode('final_locations').rename(\n",
    "        columns={'final_locations': 'location_id'}\n",
    "    )\n",
    "    \n",
    "    # Remove rows without locations\n",
    "    df_exploded = df_exploded.dropna(subset=['location_id'])\n",
    "    \n",
    "    # Add English location names\n",
    "    df_exploded['location_name_english'] = df_exploded['location_id'].map(\n",
    "        id_to_english_name\n",
    "    )\n",
    "    \n",
    "    # Select and order final columns\n",
    "    final_columns = [\n",
    "        'article_id', 'date', 'language', 'sentence_text', \n",
    "        'risk_factor', 'confidence_score', 'location_id', \n",
    "        'location_name_english'\n",
    "    ]\n",
    "    df_final = df_exploded[final_columns].copy()\n",
    "    \n",
    "    logger.info(f\"Created {len(df_final):,} risk-location pairs\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "\n",
    "def save_geotagging_results(df_final: pd.DataFrame, data_dir: str) -> Path:\n",
    "    \"\"\"\n",
    "    Save geotagged results to CSV file.\n",
    "    \n",
    "    Args:\n",
    "        df_final: Final processed DataFrame\n",
    "        data_dir: Root directory for saving data\n",
    "        \n",
    "    Returns:\n",
    "        Path to saved file\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If saving fails\n",
    "    \"\"\"\n",
    "    models_dir = Path(data_dir) / '03_models'\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_path = models_dir / 'risk_mentions_geotagged_FINAL.csv'\n",
    "    \n",
    "    try:\n",
    "        df_final.to_csv(output_path, index=False)\n",
    "        logger.info(f\"Saved {len(df_final):,} geotagged risk mentions to: {output_path}\")\n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving results: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def display_geotagging_sample(df: pd.DataFrame, n: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Display sample results for verification.\n",
    "    \n",
    "    Args:\n",
    "        df: Final DataFrame\n",
    "        n: Number of samples to display\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        logger.warning(\"No data to display\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n--- Sample Geotagged Risk Mentions ---\")\n",
    "    print(f\"Showing {min(n, len(df))} of {len(df):,} total records:\")\n",
    "    print()\n",
    "    \n",
    "    sample = df.head(n)\n",
    "    for idx, row in sample.iterrows():\n",
    "        print(f\"Risk: {row['risk_factor']} (confidence: {row['confidence_score']:.2f})\")\n",
    "        print(f\"Location: {row['location_name_english']} ({row['location_id']})\")\n",
    "        print(f\"Sentence: {row['sentence_text'][:100]}...\")\n",
    "        print(f\"Language: {row['language']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "\n",
    "def run_geotagging_pipeline(data_dir: str = '../data',\n",
    "                           sample_size: Optional[int] = None,\n",
    "                           batch_size: int = 128,\n",
    "                           ner_model: str = \"Babelscape/wikineural-multilingual-ner\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Execute the complete geotagging pipeline.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing data folders\n",
    "        sample_size: Number of rows to process (None for all)\n",
    "        batch_size: Batch size for NER processing\n",
    "        ner_model: Model name for NER pipeline\n",
    "        \n",
    "    Returns:\n",
    "        Final geotagged DataFrame\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If required files don't exist\n",
    "        Exception: If processing fails\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting risk mention geotagging pipeline\")\n",
    "    \n",
    "    if sample_size:\n",
    "        logger.info(f\"Running on SAMPLE of {sample_size} rows\")\n",
    "    \n",
    "    # Step 1: Load all necessary data\n",
    "    logger.info(\"Step 1: Loading data\")\n",
    "    df_risk = load_risk_mentions(data_dir, sample_size)\n",
    "    df_articles = load_articles(data_dir)\n",
    "    \n",
    "    # Step 2: Build location resolvers\n",
    "    logger.info(\"Step 2: Building location resolvers\")\n",
    "    location_lookup, id_to_english_name = build_location_resolvers(data_dir)\n",
    "    \n",
    "    # Step 3: Initialize NER pipeline\n",
    "    logger.info(\"Step 3: Initializing NER pipeline\")\n",
    "    ner_pipeline = initialize_ner_pipeline(ner_model)\n",
    "    \n",
    "    # Step 4: Extract locations (hybrid geotagging)\n",
    "    logger.info(\"Step 4: Performing hybrid geotagging\")\n",
    "    \n",
    "    # Filter articles to those with risk mentions\n",
    "    risk_article_ids = df_risk['article_id'].unique()\n",
    "    df_articles_filtered = df_articles[\n",
    "        df_articles['article_id'].isin(risk_article_ids)\n",
    "    ][['article_id', 'body', 'language']].copy()\n",
    "    \n",
    "    logger.info(f\"Processing {len(df_articles_filtered):,} articles with risk mentions\")\n",
    "    \n",
    "    # Extract article-level locations\n",
    "    df_articles_filtered = extract_article_locations(\n",
    "        df_articles_filtered, ner_pipeline, location_lookup, batch_size\n",
    "    )\n",
    "    \n",
    "    # Extract sentence-level locations\n",
    "    df_risk = extract_sentence_locations(\n",
    "        df_risk, ner_pipeline, location_lookup, batch_size\n",
    "    )\n",
    "    \n",
    "    # Step 5: Merge and apply hierarchical logic\n",
    "    logger.info(\"Step 5: Merging and finalizing data\")\n",
    "    df_final = merge_and_finalize(df_risk, df_articles_filtered, id_to_english_name)\n",
    "    \n",
    "    # Display sample for verification\n",
    "    display_geotagging_sample(df_final)\n",
    "    \n",
    "    # Step 6: Save results\n",
    "    logger.info(\"Step 6: Saving results\")\n",
    "    save_geotagging_results(df_final, data_dir)\n",
    "    \n",
    "    logger.info(\"Pipeline completed successfully\")\n",
    "    return df_final\n",
    "\n",
    "\n",
    "# Usage example with sampling for quick testing\n",
    "df_geotagged = run_geotagging_pipeline(\n",
    "    data_dir='../data',\n",
    "    sample_size=100,  # Set to None to process all data\n",
    "    batch_size=128,\n",
    "    ner_model=\"Babelscape/wikineural-multilingual-ner\"\n",
    ")\n",
    "\n",
    "# Example of accessing the results\n",
    "print(f\"\\nProcessed {len(df_geotagged):,} risk-location pairs\")\n",
    "\n",
    "# Analyze distribution\n",
    "if not df_geotagged.empty:\n",
    "    risk_distribution = df_geotagged['risk_factor'].value_counts()\n",
    "    print(f\"\\nRisk factor distribution:\")\n",
    "    for risk, count in risk_distribution.head().items():\n",
    "        print(f\"  {risk}: {count:,}\")\n",
    "\n",
    "    location_distribution = df_geotagged['location_name_english'].value_counts()\n",
    "    print(f\"\\nTop locations mentioned:\")\n",
    "    for location, count in location_distribution.head().items():\n",
    "        print(f\"  {location}: {count:,}\")\n",
    "\n",
    "\n",
    "# Alternative usage examples:\n",
    "\n",
    "# Full processing mode\n",
    "# df_geotagged_full = run_geotagging_pipeline(\n",
    "#     data_dir='../data',\n",
    "#     sample_size=None,  # Process all data\n",
    "#     batch_size=256,\n",
    "#     ner_model=\"Babelscape/wikineural-multilingual-ner\"\n",
    "# )\n",
    "\n",
    "# Custom configuration with different batch size and model\n",
    "# df_geotagged_custom = run_geotagging_pipeline(\n",
    "#     data_dir='../data',\n",
    "#     sample_size=50,\n",
    "#     batch_size=64,\n",
    "#     ner_model=\"dbmdz/bert-large-cased-finetuned-conll03-english\"  # Alternative NER model\n",
    "# )\n",
    "\n",
    "# Process specific number of records for testing\n",
    "# df_geotagged_test = run_geotagging_pipeline(\n",
    "#     data_dir='../data',\n",
    "#     sample_size=20,\n",
    "#     batch_size=32\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495c075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1. Load Necessary Data (Corrected) ---\n",
    "print(\"--- Step 1: Loading Processed Data ---\")\n",
    "DATA_DIR = '../data'\n",
    "MODELS_DIR = os.path.join(DATA_DIR, '03_models')\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, '01_raw')\n",
    "\n",
    "# CORRECTED: Load the file you just created from notebook 05\n",
    "df_final_exploded = pd.read_csv(os.path.join(MODELS_DIR, 'risk_mentions_geotagged_FINAL.csv'))\n",
    "\n",
    "# CORRECTED: Load the risk factor cluster file with the correct name\n",
    "df_clusters = pd.read_excel(os.path.join(RAW_DATA_DIR, 'risk-factors-categories.xlsx'))\n",
    "\n",
    "# This file is needed for normalization\n",
    "df_geo_articles = pd.read_pickle(os.path.join(DATA_DIR, '02_processed', 'news_geographically_filtered.pkl'))\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "# --- 2. Prepare the Data (Corrected) ---\n",
    "print(\"--- Step 2: Preparing Data for Aggregation ---\")\n",
    "\n",
    "# --- HELPFUL DEBUGGING STEP ---\n",
    "# Print the column names to see what they are actually called\n",
    "print(\"Columns in df_clusters (from risk-factors-categories.xlsx):\")\n",
    "print(df_clusters.columns)\n",
    "# -----------------------------\n",
    "\n",
    "# Ensure the 'date' column is in datetime format\n",
    "df_final_exploded['date'] = pd.to_datetime(df_final_exploded['date'])\n",
    "\n",
    "# Merge the risk mentions with their thematic clusters\n",
    "# CORRECTED: Changed 'right_on' to the correct column name. It's likely 'risk_factor'.\n",
    "df_merged = pd.merge(df_final_exploded, df_clusters, on='risk_factor') # Using 'on' is cleaner when column names match\n",
    "\n",
    "print(\"\\nMerged risk mentions with thematic clusters.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 3. Aggregate Risk Mentions (Corrected) ---\n",
    "print(\"--- Step 3: Aggregating Daily Risk Counts ---\")\n",
    "\n",
    "# CORRECTED: Changed 'theme' to 'cluster' to match the actual column name\n",
    "df_daily_counts = df_merged.groupby([pd.Grouper(key='date', freq='D'), 'location_id', 'location_name_english', 'cluster']).size().reset_index(name='risk_mention_count')\n",
    "\n",
    "print(\"Calculated raw daily counts of risk mentions per location and theme.\")\n",
    "display(df_daily_counts.head())\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 4. Normalization (Crucial Step) ---\n",
    "# To avoid bias from varying news volume, we normalize by the number of articles published per day.\n",
    "# NOTE: This is a simplified normalization. A more advanced approach would be to get article counts *per location*,\n",
    "# which would require re-running the geotagger on ALL articles, not just those with risks.\n",
    "# For this assessment, normalizing by total daily articles is a reasonable simplification.\n",
    "\n",
    "print(\"--- Step 4: Normalizing Risk Counts ---\")\n",
    "df_geo_articles['date'] = pd.to_datetime(df_geo_articles['date'])\n",
    "daily_article_volume = df_geo_articles.groupby(pd.Grouper(key='date', freq='D')).size().reset_index(name='total_articles_published')\n",
    "\n",
    "# Merge the risk counts with the total article volume for normalization\n",
    "df_normalized = pd.merge(df_daily_counts, daily_article_volume, on='date')\n",
    "df_normalized['normalized_risk'] = df_normalized['risk_mention_count'] / df_normalized['total_articles_published']\n",
    "\n",
    "print(\"Normalized risk scores by total daily article volume.\")\n",
    "display(df_normalized.head())\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 5. Calculate Thematic and Composite Risk Indices ---\n",
    "print(\"--- Step 5: Constructing Risk Indices ---\")\n",
    "# The 'normalized_risk' is already our Thematic Risk Index for each theme.\n",
    "# Now, we calculate the Composite Risk Index (CRI) by averaging themes per day/location.\n",
    "\n",
    "df_cri = df_normalized.groupby(['date', 'location_id', 'location_name_english'])['normalized_risk'].mean().reset_index(name='composite_risk_index')\n",
    "\n",
    "print(\"Calculated Composite Risk Index (CRI).\")\n",
    "display(df_cri.head())\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 6. Save the Final Time-Series Data ---\n",
    "print(\"--- Step 6: Saving Final Index Data ---\")\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, '04_feature')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Save the thematic and composite indices\n",
    "df_normalized.to_csv(os.path.join(OUTPUT_DIR, 'thematic_risk_indices.csv'), index=False)\n",
    "df_cri.to_csv(os.path.join(OUTPUT_DIR, 'composite_risk_index.csv'), index=False)\n",
    "\n",
    "print(f\"Final time-series data saved to: {OUTPUT_DIR}\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 7. Example Visualization ---\n",
    "print(\"--- Step 7: Example Visualization ---\")\n",
    "# Let's visualize the CRI for a specific location, e.g., Baghdad\n",
    "location_to_plot = 'Baghdad'\n",
    "df_plot = df_cri[df_cri['location_name_english'] == location_to_plot]\n",
    "\n",
    "if not df_plot.empty:\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(15, 7))\n",
    "    \n",
    "    ax.plot(df_plot['date'], df_plot['composite_risk_index'], marker='o', linestyle='-', label='Composite Risk Index')\n",
    "    ax.set_title(f\"Daily Composite Risk Index for {location_to_plot}\", fontsize=16)\n",
    "    ax.set_ylabel(\"Risk Index (Normalized Score)\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"No data found for location: {location_to_plot}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f304389f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualitative",
   "metadata": {},
   "source": [
    "# Part 2: Reflection\n",
    "\n",
    "Please outline (1) some of the limitations of your approach and (2) how you would tackle these if you had more time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49236dab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d211d3b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
