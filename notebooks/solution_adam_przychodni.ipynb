{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ce3648",
   "metadata": {},
   "source": [
    "# [Take-home Assessment] Food Crisis Early Warning \n",
    "\n",
    "Welcome to the assessment. You will showcase your modeling and research skills by investigating news articles (in English and Arabic) as well as a set of food insecurity risk factors. \n",
    "\n",
    "We suggest planning to spend **~6–8 hours** on this assessment. **Please submit your response by Monday, September 15th, 9:00 AM EST via email to dime_ai@worldbank.org**. Please document your code with comments and explanations of design choices. There is one question on reflecting upon your approach at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cf2966",
   "metadata": {},
   "source": [
    "**Name:** Adam Przychodni\n",
    "\n",
    "**Email:** adam.przychodni@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09329a02",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "# Part 1: Technical Assessment\n",
    "\n",
    "\n",
    "## Task:\n",
    "\n",
    "We invite you to approach the challenge of understanding (and potentially predicting) food insecurity using the provided (limited) data. Your response should demonstrate how you tackle open-ended problems in data-scarce environments.\n",
    "\n",
    "Some example questions to consider:\n",
    "- What is the added value of geospatial data?\n",
    "- How can we address the lack of ground-truth information on food insecurity levels?\n",
    "- What are the benefits and challenges of working with multilingual data?\n",
    "- ...\n",
    "\n",
    "These are just guiding examples — you are free to explore any relevant angles to this topic/data.\n",
    "\n",
    "**Note:** There is no single \"right\" approach. Instead, we want to understand how you approach and structure open-ended problems in data-scarce environments. Given the large number of applicants, we will preselect the most impressive and complete submissions. Please take effort in structuring your response, as selection will depend on its depth and originality.\n",
    "\n",
    "\n",
    "## Provided Data:\n",
    "\n",
    "1. **Risk Factors:** A file containing 167 risk factors (unigrams, bigrams, and trigrams) in the `english_keywords` column and an empty `keywords_arabic` column. A separate file with the mapping of English risk factors to pre-defined thematic cluster assignments.\n",
    "\n",
    "\n",
    "2. **News Articles:** Two files containing one month of news articles from the Mashriq region:\n",
    "   - `news-articles-eng.csv`\n",
    "   - `news-articles-ara.csv`\n",
    "   - **Note:** You may work on a sample subset during development.\n",
    "   \n",
    "   \n",
    "3. **Geographic Taxonomy:** A file containing the names of the countries, provinces, and districts for the subset of Mashriq countries that is covered by the news articles. The files are a dictionary mapping from a key to the geographic name.\n",
    "   - `id_arabic_location_name.pkl`\n",
    "   - `id_english_location_name.pkl`\n",
    "   - **Note:** Each unique country/province/district is assigned a key (e.g. `iq`,`iq_bg` and `iq_bg_1` for country Iraq, province Baghdad, and district 1 in Baghdad respectively).\n",
    "   - The key of country names is a two character abbreviation as follows.\n",
    "       - 'iq': 'Iraq'\n",
    "       - 'jo': 'Jordan'\n",
    "       - 'lb': 'Lebanon'\n",
    "       - 'ps': 'Palestine'\n",
    "       - 'sy': 'Syria'\n",
    "       \n",
    "   - The key of provinces is a two-character abbreviation of the country followed by two-character abbreviation of the province **`{country_abbreviation}_{province_abbreviation}`**, and the key of districts is **`{country_abbreviation}_{province_abbreviation}_{unique_number}`**.\n",
    "       \n",
    "\n",
    "\n",
    "## Submission Guidelines:\n",
    "\n",
    "- **Code:** Follow best coding practices and ensure clear documentation. All notebook cells should be executed with outputs saved, and the notebook should run correctly on its own. Name your file **`solution_{FIRSTNAME}_{LASTNAME}.ipynb`**. If your solution relies on additional open-access data, either include it in your submission (e.g., as part of a ZIP file) or provide clear data-loading code/instructions as part of the nottebook. \n",
    "- **Report:** Submit a separate markdown file communicating your approach to this research problem. We expect you to detail the models, methods, or (additional) data you are using.\n",
    "\n",
    "Good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f9934a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task1",
   "metadata": {},
   "source": [
    "## Your Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bfd3ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Cleaning news articles...\n",
      "Cleaning complete.\n",
      "Tokenizing English articles into sentences...\n",
      "Tokenizing Arabic articles into sentences...\n",
      "\n",
      "Sentence tokenization complete.\n",
      "\n",
      "--- Example of Regex Sentence Tokenization ---\n",
      "Article body has been split into 124 sentences.\n",
      "First 3 sentences:\n",
      "- Hussam al-Mahmoud | Yamen Moghrabi | Hassan Ibrahim On October 7, 2023, the world and the Middle East awoke to the drums of war beating in the Gaza Strip.\n",
      "- Over time, it turned into a reality that American efforts, Qatari and Egyptian mediation, condemnations, statements, summits, and conferences could not stop.\n",
      "- While Israel continues its war in the besieged Gaza Strip, attention is turning towards the potential outbreak of another war.\n",
      "\n",
      "Processed and FILTERED English data saved to: ../data/02_processed/news_eng_processed.pkl\n",
      "Processed and FILTERED Arabic data saved to: ../data/02_processed/news_ara_processed.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# --- Load the datasets ---\n",
    "DATA_DIR = '../data'\n",
    "df_eng = pd.read_csv(os.path.join(DATA_DIR, '01_raw/news-articles-eng.csv'))\n",
    "df_ara = pd.read_csv(os.path.join(DATA_DIR, '01_raw/news-articles-ara.csv'))\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans raw text by removing HTML tags, URLs, and normalizing whitespace.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "print(\"Cleaning news articles...\")\n",
    "df_eng['body_cleaned'] = df_eng['body'].apply(clean_text)\n",
    "df_ara['body_cleaned'] = df_ara['body'].apply(clean_text)\n",
    "print(\"Cleaning complete.\")\n",
    "\n",
    "def regex_sent_tokenize(text):\n",
    "    \"\"\"\n",
    "    Splits text into sentences using a regular expression.\n",
    "    This is a dependency-free alternative to nltk.sent_tokenize.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return []\n",
    "    # Split on sentence-ending punctuation followed by a space or end of string\n",
    "    # The regex uses a \"positive lookbehind\" to keep the punctuation with the sentence.\n",
    "    sentences = re.split(r'(?<=[.!?۔])\\s+', text)\n",
    "    # Filter out any empty strings that might result from the split\n",
    "    return [s for s in sentences if s]\n",
    "\n",
    "# --- Apply the new, reliable tokenizer ---\n",
    "print(\"Tokenizing English articles into sentences...\")\n",
    "df_eng['sentences'] = df_eng['body_cleaned'].apply(regex_sent_tokenize)\n",
    "\n",
    "print(\"Tokenizing Arabic articles into sentences...\")\n",
    "df_ara['sentences'] = df_ara['body_cleaned'].apply(regex_sent_tokenize)\n",
    "\n",
    "print(\"\\nSentence tokenization complete.\")\n",
    "\n",
    "# --- Display a sample ---\n",
    "print(\"\\n--- Example of Regex Sentence Tokenization ---\")\n",
    "print(f\"Article body has been split into {len(df_eng['sentences'].iloc[0])} sentences.\")\n",
    "print(\"First 3 sentences:\")\n",
    "for sentence in df_eng['sentences'].iloc[0][:3]:\n",
    "    print(f\"- {sentence}\")\n",
    "\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, '02_processed')\n",
    "output_path_eng = os.path.join(PROCESSED_DATA_DIR, 'news_eng_processed.pkl')\n",
    "output_path_ara = os.path.join(PROCESSED_DATA_DIR, 'news_ara_processed.pkl')\n",
    "\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Save the smaller, filtered dataframes\n",
    "df_eng.to_pickle(output_path_eng)\n",
    "df_ara.to_pickle(output_path_ara)\n",
    "\n",
    "print(f\"\\nProcessed and FILTERED English data saved to: {output_path_eng}\")\n",
    "print(f\"Processed and FILTERED Arabic data saved to: {output_path_ara}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21cec6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Initializing and Checking for Existing File ---\n",
      "Output file already exists: ../data/02_processed/news_geographically_filtered.pkl\n",
      "Skipping the filtering process.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- 1. Define Paths and Check if Output Already Exists ---\n",
    "print(\"--- Step 1: Initializing and Checking for Existing File ---\")\n",
    "DATA_DIR = '../data'\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, '02_processed')\n",
    "output_path = os.path.join(PROCESSED_DATA_DIR, 'news_geographically_filtered.pkl')\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    print(f\"Output file already exists: {output_path}\")\n",
    "    print(\"Skipping the filtering process.\")\n",
    "else:\n",
    "    print(\"Output file not found. Starting the filtering process...\")\n",
    "    # --- 2. Load All Necessary Data ---\n",
    "    print(\"--- Step 2: Loading All Necessary Data ---\")\n",
    "    df_eng = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, 'news_eng_processed.pkl'))\n",
    "    df_ara = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, 'news_ara_processed.pkl'))\n",
    "    df_articles = pd.concat([df_eng, df_ara], ignore_index=True)\n",
    "\n",
    "    with open('../data/01_raw/id_english_location_name.pkl', 'rb') as f:\n",
    "        eng_locations = pickle.load(f)\n",
    "    with open('../data/01_raw/id_arabic_location_name.pkl', 'rb') as f:\n",
    "        ara_locations = pickle.load(f)\n",
    "    print(f\"Loaded {len(df_articles):,} total articles to filter.\")\n",
    "    print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "    # --- 3. Create a Fast Location Lookup ---\n",
    "    print(\"--- Step 3: Building Location Resolver ---\")\n",
    "    def create_lookup(location_dict):\n",
    "        lookup = {}\n",
    "        for loc_id, names in location_dict.items():\n",
    "            for name in names:\n",
    "                lookup[name.lower()] = loc_id\n",
    "        return lookup\n",
    "\n",
    "    location_lookup = create_lookup(eng_locations)\n",
    "    location_lookup.update(create_lookup(ara_locations))\n",
    "    print(f\"Created a lookup with {len(location_lookup):,} unique location aliases.\")\n",
    "    print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "    # --- 4. Initialize NER Pipeline ---\n",
    "    print(\"--- Step 4: Loading NER Model ---\")\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    if device == 0: print(\"GPU found. Filtering will be fast.\")\n",
    "    else: print(\"No GPU found.\")\n",
    "\n",
    "    ner_pipeline = pipeline(\"ner\", model=\"Babelscape/wikineural-multilingual-ner\", aggregation_strategy=\"simple\", device=device)\n",
    "    print(\"NER pipeline loaded.\")\n",
    "    print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "    # --- 5. Define and Run the Filtering Process ---\n",
    "    print(\"--- Step 5: Filtering All Articles ---\")\n",
    "    def contains_target_location(text, pipeline, lookup):\n",
    "        \"\"\"\n",
    "        Checks if a body of text contains at least one of the target locations.\n",
    "        \"\"\"\n",
    "        sample_text = text[:2000]\n",
    "        entities = pipeline(sample_text)\n",
    "        for entity in entities:\n",
    "            if entity['entity_group'] == 'LOC':\n",
    "                if entity['word'].lower() in lookup:\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    tqdm.pandas(desc=\"Filtering Articles\")\n",
    "    relevance_mask = df_articles['body'].progress_apply(\n",
    "        lambda text: contains_target_location(text, ner_pipeline, location_lookup)\n",
    "    )\n",
    "    df_filtered = df_articles[relevance_mask]\n",
    "\n",
    "    print(\"\\nFiltering complete.\")\n",
    "    print(f\"  - Original number of articles: {len(df_articles):,}\")\n",
    "    print(f\"  - Geographically relevant articles: {len(df_filtered):,}\")\n",
    "    print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "    # --- 6. Save the Filtered Data ---\n",
    "    print(\"--- Step 6: Saving the Filtered Dataset ---\")\n",
    "    df_filtered.to_pickle(output_path)\n",
    "    print(f\"Successfully saved the filtered data to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ec11e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading Geographically Filtered Data ---\n",
      "Geographically filtered data loaded successfully.\n",
      "  - Total relevant articles: 96,516 articles\n",
      "------------------------------ \n",
      "\n",
      "--- Step 2: Loading Risk Factors ---\n",
      "Loaded 167 English risk factors.\n",
      "------------------------------ \n",
      "\n",
      "--- Step 3: Initializing Models ---\n",
      "GPU found. Models will run on the GPU for maximum speed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main classifier initialized: MoritzLaurer/deberta-v3-xsmall-zeroshot-v1.1-all-33\n",
      "Fast pre-filtering model initialized: paraphrase-multilingual-MiniLM-L12-v2\n",
      "------------------------------ \n",
      "\n",
      "--- Step 4: Defining the Extraction Function ---\n",
      "Risk factor embeddings pre-computed.\n",
      "\n",
      "--- Step 5: Running on a SAMPLE of 10 Articles ---\n",
      "Processing a sample of 96,516 articles...\n",
      "\n",
      "Original sentence count: 1,475,716\n",
      "Pre-filtering sentences with threshold: 0.55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae1438d8e5a46789dcda775155796ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/46117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced to 93,939 sentences after filtering.\n",
      "Running classifier with confidence threshold: 0.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3cc37501584603b93094c156c013fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93939 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample risk factor extraction complete.\n",
      "Found 263,041 potential risk mentions in the sample.\n",
      "------------------------------ \n",
      "\n",
      "--- Step 6: Refining Sample Results (Post-Processing) ---\n",
      "Original number of mentions: 263,041\n",
      "Refined to 34,275 high-confidence, unique mentions.\n",
      "------------------------------ \n",
      "\n",
      "--- Step 7: Saving SAMPLE Results ---\n",
      "Successfully saved 34,275 sample risk mentions to: ../data/03_models/risk_mentions_SAMPLE_FINAL.csv\n",
      "------------------------------ \n",
      "\n",
      "--- Final Extracted Risk Factors (from Sample) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>date</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>risk_factor</th>\n",
       "      <th>confidence_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>194993</th>\n",
       "      <td>93829</td>\n",
       "      <td>2024-07-16</td>\n",
       "      <td>!</td>\n",
       "      <td>without international aid</td>\n",
       "      <td>0.954393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251782</th>\n",
       "      <td>158563</td>\n",
       "      <td>2024-07-22</td>\n",
       "      <td>!!</td>\n",
       "      <td>without international aid</td>\n",
       "      <td>0.958109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214472</th>\n",
       "      <td>110774</td>\n",
       "      <td>2024-06-28</td>\n",
       "      <td>!!!</td>\n",
       "      <td>without international aid</td>\n",
       "      <td>0.954647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230238</th>\n",
       "      <td>130700</td>\n",
       "      <td>2024-07-05</td>\n",
       "      <td>!!!!!</td>\n",
       "      <td>without international aid</td>\n",
       "      <td>0.964153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107929</th>\n",
       "      <td>26712</td>\n",
       "      <td>2024-06-27</td>\n",
       "      <td>\"</td>\n",
       "      <td>without international aid</td>\n",
       "      <td>0.916260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        article_id        date sentence_text                risk_factor  \\\n",
       "194993       93829  2024-07-16             !  without international aid   \n",
       "251782      158563  2024-07-22            !!  without international aid   \n",
       "214472      110774  2024-06-28           !!!  without international aid   \n",
       "230238      130700  2024-07-05         !!!!!  without international aid   \n",
       "107929       26712  2024-06-27             \"  without international aid   \n",
       "\n",
       "        confidence_score  \n",
       "194993          0.954393  \n",
       "251782          0.958109  \n",
       "214472          0.954647  \n",
       "230238          0.964153  \n",
       "107929          0.916260  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- 1. Load GEOGRAPHICALLY FILTERED Data ---\n",
    "print(\"--- Step 1: Loading Geographically Filtered Data ---\")\n",
    "DATA_DIR = '../data'\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, '02_processed')\n",
    "\n",
    "filtered_data_path = os.path.join(PROCESSED_DATA_DIR, 'news_geographically_filtered.pkl')\n",
    "df_filtered = pd.read_pickle(filtered_data_path)\n",
    "\n",
    "print(\"Geographically filtered data loaded successfully.\")\n",
    "print(f\"  - Total relevant articles: {len(df_filtered):,} articles\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 2. Load Risk Factors ---\n",
    "print(\"--- Step 2: Loading Risk Factors ---\")\n",
    "risk_factors_path = os.path.join(DATA_DIR, '01_raw/risk-factors.xlsx')\n",
    "df_risk_factors_eng = pd.read_excel(risk_factors_path)\n",
    "df_risk_factors_eng.dropna(subset=['risk_factor_english'], inplace=True)\n",
    "risk_factor_labels = df_risk_factors_eng['risk_factor_english'].tolist()\n",
    "print(f\"Loaded {len(risk_factor_labels)} English risk factors.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 3. Initialize Models ---\n",
    "print(\"--- Step 3: Initializing Models ---\")\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "if device == 0:\n",
    "    print(\"GPU found. Models will run on the GPU for maximum speed.\")\n",
    "else:\n",
    "    print(\"No GPU found. Models will run on the CPU.\")\n",
    "\n",
    "MODEL_NAME = 'MoritzLaurer/deberta-v3-xsmall-zeroshot-v1.1-all-33'\n",
    "classifier = pipeline(\"zero-shot-classification\", model=MODEL_NAME, device=device)\n",
    "print(f\"Main classifier initialized: {MODEL_NAME}\")\n",
    "\n",
    "FAST_MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "fast_embedder = SentenceTransformer(FAST_MODEL_NAME, device=device)\n",
    "print(f\"Fast pre-filtering model initialized: {FAST_MODEL_NAME}\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 4. Define the Extraction Function ---\n",
    "# (No changes needed in this section)\n",
    "print(\"--- Step 4: Defining the Extraction Function ---\")\n",
    "risk_factor_embeddings = fast_embedder.encode(risk_factor_labels, convert_to_tensor=True)\n",
    "print(\"Risk factor embeddings pre-computed.\")\n",
    "\n",
    "def extract_risk_factors_optimized(\n",
    "    df, classifier, labels, threshold, batch_size,\n",
    "    sentence_embedder, risk_factor_embeddings, sentence_similarity_threshold\n",
    "):\n",
    "    if 'article_id' not in df.columns:\n",
    "        df['article_id'] = df.index\n",
    "    df_sentences = df.explode('sentences').rename(columns={'sentences': 'sentence_text'})\n",
    "    df_sentences = df_sentences[['article_id', 'date', 'sentence_text']].dropna(subset=['sentence_text'])\n",
    "    all_sentences = df_sentences['sentence_text'].tolist()\n",
    "    if not all_sentences: return pd.DataFrame()\n",
    "\n",
    "    print(f\"\\nOriginal sentence count: {len(all_sentences):,}\")\n",
    "    print(f\"Pre-filtering sentences with threshold: {sentence_similarity_threshold}\")\n",
    "    sentence_embeddings = sentence_embedder.encode(all_sentences, convert_to_tensor=True, show_progress_bar=True)\n",
    "    hits = util.semantic_search(sentence_embeddings, risk_factor_embeddings, top_k=1)\n",
    "    relevant_indices = [i for i, hit_list in enumerate(hits) if hit_list and hit_list[0]['score'] >= sentence_similarity_threshold]\n",
    "    filtered_sentences_df = df_sentences.iloc[relevant_indices]\n",
    "    sentence_list = filtered_sentences_df['sentence_text'].tolist()\n",
    "    if not sentence_list: return pd.DataFrame()\n",
    "\n",
    "    print(f\"Reduced to {len(sentence_list):,} sentences after filtering.\")\n",
    "    print(f\"Running classifier with confidence threshold: {threshold}\")\n",
    "    results_list = []\n",
    "    for i, result in tqdm(enumerate(classifier(sentence_list, labels, multi_label=True, batch_size=batch_size)), total=len(sentence_list)):\n",
    "        for label, score in zip(result['labels'], result['scores']):\n",
    "            if score >= threshold:\n",
    "                original_row = filtered_sentences_df.iloc[i]\n",
    "                results_list.append({\n",
    "                    'article_id': original_row['article_id'],\n",
    "                    'date': original_row['date'],\n",
    "                    'sentence_text': result['sequence'],\n",
    "                    'risk_factor': label,\n",
    "                    'confidence_score': score\n",
    "                })\n",
    "    return pd.DataFrame(results_list)\n",
    "\n",
    "# --- 5. Set Parameters and Run on a SAMPLE (CHANGED) ---\n",
    "print(\"\\n--- Step 5: Running on a SAMPLE of 10 Articles ---\")\n",
    "CLASSIFIER_BATCH_SIZE = 128\n",
    "SENTENCE_SIMILARITY_THRESHOLD = 0.55\n",
    "CLASSIFIER_CONFIDENCE_THRESHOLD = 0.90\n",
    "\n",
    "# Create a small sample to test the pipeline\n",
    "# df_filtered = df_filtered.head(10).copy()\n",
    "\n",
    "print(f\"Processing a sample of {len(df_filtered):,} articles...\")\n",
    "\n",
    "all_risk_mentions = extract_risk_factors_optimized(\n",
    "    df_filtered,  \n",
    "    classifier,\n",
    "    risk_factor_labels,\n",
    "    threshold=CLASSIFIER_CONFIDENCE_THRESHOLD,\n",
    "    batch_size=CLASSIFIER_BATCH_SIZE,\n",
    "    sentence_embedder=fast_embedder,\n",
    "    risk_factor_embeddings=risk_factor_embeddings,\n",
    "    sentence_similarity_threshold=SENTENCE_SIMILARITY_THRESHOLD\n",
    ")\n",
    "print(\"\\nSample risk factor extraction complete.\")\n",
    "print(f\"Found {len(all_risk_mentions):,} potential risk mentions in the sample.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 6. Post-Processing: Keep Only the Top Label per Sentence ---\n",
    "print(\"--- Step 6: Refining Sample Results (Post-Processing) ---\")\n",
    "if not all_risk_mentions.empty:\n",
    "    print(f\"Original number of mentions: {len(all_risk_mentions):,}\")\n",
    "    idx = all_risk_mentions.groupby('sentence_text')['confidence_score'].idxmax()\n",
    "    all_risk_mentions_refined = all_risk_mentions.loc[idx]\n",
    "    print(f\"Refined to {len(all_risk_mentions_refined):,} high-confidence, unique mentions.\")\n",
    "else:\n",
    "    all_risk_mentions_refined = pd.DataFrame()\n",
    "    print(\"No risk mentions found to refine.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 7. Save the SAMPLE Results (CHANGED) ---\n",
    "print(\"--- Step 7: Saving SAMPLE Results ---\")\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, '03_models')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Save the sample results to a separate file\n",
    "output_path = os.path.join(OUTPUT_DIR, 'risk_mentions_SAMPLE_FINAL.csv')\n",
    "all_risk_mentions_refined.to_csv(output_path, index=False)\n",
    "print(f\"Successfully saved {len(all_risk_mentions_refined):,} sample risk mentions to: {output_path}\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "print(\"--- Final Extracted Risk Factors (from Sample) ---\")\n",
    "if all_risk_mentions_refined.empty:\n",
    "    print(\"No risk factors found with the current settings.\")\n",
    "else:\n",
    "    display(all_risk_mentions_refined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ccf34b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Step 1: Loading All Necessary Data\n",
      "All data loaded successfully.\n",
      "------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Loading All Necessary Data\n",
    "print(\"# Step 1: Loading All Necessary Data\")\n",
    "DATA_DIR = '../data'\n",
    "MODELS_DIR = os.path.join(DATA_DIR, '03_models')\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, '02_processed')\n",
    "\n",
    "df_risk_sentences = pd.read_csv(os.path.join(MODELS_DIR, 'risk_mentions_SAMPLE_FINAL.csv'))\n",
    "df_eng = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, 'news_eng_processed.pkl'))\n",
    "df_eng['language'] = 'english'\n",
    "df_ara = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, 'news_ara_processed.pkl'))\n",
    "df_ara['language'] = 'arabic'\n",
    "if 'article_id' not in df_eng: df_eng['article_id'] = df_eng.index\n",
    "if 'article_id' not in df_ara: df_ara['article_id'] = df_ara.index\n",
    "df_articles = pd.concat([df_eng, df_ara])\n",
    "with open('../data/01_raw/id_english_location_name.pkl', 'rb') as f: eng_locations = pickle.load(f)\n",
    "with open('../data/01_raw/id_arabic_location_name.pkl', 'rb') as f: ara_locations = pickle.load(f)\n",
    "print(\"All data loaded successfully.\")\n",
    "print(\"-\" * 30, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbee8e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>date</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>risk_factor</th>\n",
       "      <th>confidence_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93829</td>\n",
       "      <td>2024-07-16</td>\n",
       "      <td>!</td>\n",
       "      <td>without international aid</td>\n",
       "      <td>0.954393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>158563</td>\n",
       "      <td>2024-07-22</td>\n",
       "      <td>!!</td>\n",
       "      <td>without international aid</td>\n",
       "      <td>0.958109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110774</td>\n",
       "      <td>2024-06-28</td>\n",
       "      <td>!!!</td>\n",
       "      <td>without international aid</td>\n",
       "      <td>0.954647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>130700</td>\n",
       "      <td>2024-07-05</td>\n",
       "      <td>!!!!!</td>\n",
       "      <td>without international aid</td>\n",
       "      <td>0.964153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26712</td>\n",
       "      <td>2024-06-27</td>\n",
       "      <td>\"</td>\n",
       "      <td>without international aid</td>\n",
       "      <td>0.916260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34270</th>\n",
       "      <td>87057</td>\n",
       "      <td>2024-07-23</td>\n",
       "      <td> معهد آخر يؤشر بدء قرع طبول هجمات الفصائل وال...</td>\n",
       "      <td>wreaked havoc</td>\n",
       "      <td>0.936865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34271</th>\n",
       "      <td>93762</td>\n",
       "      <td>2024-06-26</td>\n",
       "      <td>ﺣﺎﻟﯿﺎً، ﯾﺸﻜّﻞ ﺗﻮﻗّﻒ اﻟﺘﺴﻠﯿﻒ ﺑﺴﺒﺐ أزﻣﺔ اﻟﻤﺼﺎرف ...</td>\n",
       "      <td>destructive pattern</td>\n",
       "      <td>0.900661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34272</th>\n",
       "      <td>93762</td>\n",
       "      <td>2024-06-26</td>\n",
       "      <td>ﻓﻘﺪ أدّى اﻧﺨﻔﺎض اﻟﻘﺪرة اﻟﺸﺮاﺋﯿﺔ ﻟﻠﻤﻮاطﻨﯿﻦ، وﺗﺮ...</td>\n",
       "      <td>human rights abuses</td>\n",
       "      <td>0.960100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34273</th>\n",
       "      <td>97204</td>\n",
       "      <td>2024-06-25</td>\n",
       "      <td>ﻓﻲ اﻟﻮاﻗﻊ، ﯾﻮاﺟﮫ اﻻﻗﺘﺼﺎد اﻟﻠﺒﻨﺎﻧﻲ ﻣﻨﺬ أﻛﺜﺮ ﻣﻦ ...</td>\n",
       "      <td>authoritarian</td>\n",
       "      <td>0.914291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34274</th>\n",
       "      <td>50708</td>\n",
       "      <td>2024-06-28</td>\n",
       "      <td>🚸 RSVP is required with a $5 donation.</td>\n",
       "      <td>call for donations</td>\n",
       "      <td>0.997985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34275 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       article_id        date  \\\n",
       "0           93829  2024-07-16   \n",
       "1          158563  2024-07-22   \n",
       "2          110774  2024-06-28   \n",
       "3          130700  2024-07-05   \n",
       "4           26712  2024-06-27   \n",
       "...           ...         ...   \n",
       "34270       87057  2024-07-23   \n",
       "34271       93762  2024-06-26   \n",
       "34272       93762  2024-06-26   \n",
       "34273       97204  2024-06-25   \n",
       "34274       50708  2024-06-28   \n",
       "\n",
       "                                           sentence_text  \\\n",
       "0                                                      !   \n",
       "1                                                     !!   \n",
       "2                                                    !!!   \n",
       "3                                                  !!!!!   \n",
       "4                                                      \"   \n",
       "...                                                  ...   \n",
       "34270   معهد آخر يؤشر بدء قرع طبول هجمات الفصائل وال...   \n",
       "34271  ﺣﺎﻟﯿﺎً، ﯾﺸﻜّﻞ ﺗﻮﻗّﻒ اﻟﺘﺴﻠﯿﻒ ﺑﺴﺒﺐ أزﻣﺔ اﻟﻤﺼﺎرف ...   \n",
       "34272  ﻓﻘﺪ أدّى اﻧﺨﻔﺎض اﻟﻘﺪرة اﻟﺸﺮاﺋﯿﺔ ﻟﻠﻤﻮاطﻨﯿﻦ، وﺗﺮ...   \n",
       "34273  ﻓﻲ اﻟﻮاﻗﻊ، ﯾﻮاﺟﮫ اﻻﻗﺘﺼﺎد اﻟﻠﺒﻨﺎﻧﻲ ﻣﻨﺬ أﻛﺜﺮ ﻣﻦ ...   \n",
       "34274             🚸 RSVP is required with a $5 donation.   \n",
       "\n",
       "                     risk_factor  confidence_score  \n",
       "0      without international aid          0.954393  \n",
       "1      without international aid          0.958109  \n",
       "2      without international aid          0.954647  \n",
       "3      without international aid          0.964153  \n",
       "4      without international aid          0.916260  \n",
       "...                          ...               ...  \n",
       "34270              wreaked havoc          0.936865  \n",
       "34271        destructive pattern          0.900661  \n",
       "34272        human rights abuses          0.960100  \n",
       "34273              authoritarian          0.914291  \n",
       "34274         call for donations          0.997985  \n",
       "\n",
       "[34275 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_risk_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b418f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Building Location Resolvers\n",
    "print(\"# Step 2: Building Location Resolvers\")\n",
    "def create_name_to_id_lookup(location_dict):\n",
    "    lookup = {}\n",
    "    for loc_id, names in location_dict.items():\n",
    "        for name in names: lookup[name.lower()] = loc_id\n",
    "    return lookup\n",
    "location_lookup = create_name_to_id_lookup(eng_locations)\n",
    "location_lookup.update(create_name_to_id_lookup(ara_locations))\n",
    "id_to_english_name_lookup = {loc_id: names[0] for loc_id, names in eng_locations.items()}\n",
    "print(\"Location resolvers created.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# Step 3: Initialize Hugging Face NER Pipeline\n",
    "print(\"# Step 3: Loading Hugging Face Model for NER\")\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "if device == 0: print(\"GPU found.\")\n",
    "else: print(\"No GPU found.\")\n",
    "ner_pipeline = pipeline(\"ner\", model=\"Babelscape/wikineural-multilingual-ner\", aggregation_strategy=\"simple\", device=device)\n",
    "print(\"NER pipeline loaded.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# Step 4: Hybrid Geotagging (Article and Sentence Level)\n",
    "print(\"# Step 4: Hybrid Geotagging (Article & Sentence Levels)\")\n",
    "articles_with_risks_ids = df_risk_sentences['article_id'].unique()\n",
    "df_articles_with_risks = df_articles[df_articles['article_id'].isin(articles_with_risks_ids)][['article_id', 'body', 'language']].copy()\n",
    "print(f\"Found {len(df_articles_with_risks)} unique articles for context.\")\n",
    "\n",
    "def resolve_locations(text, pipeline, lookup):\n",
    "    \"\"\"Shared function to extract and resolve locations from any text.\"\"\"\n",
    "    entities = pipeline(text)\n",
    "    found_ids = set()\n",
    "    for entity in entities:\n",
    "        if entity['entity_group'] == 'LOC':\n",
    "            loc_name_lower = entity['word'].lower()\n",
    "            if loc_name_lower in lookup: found_ids.add(lookup[loc_name_lower])\n",
    "    return list(found_ids)\n",
    "\n",
    "# 4a: Get ARTICLE-level locations (the broad context)\n",
    "df_articles_with_risks['article_locations'] = df_articles_with_risks['body'].apply(\n",
    "    lambda text: resolve_locations(text, ner_pipeline, location_lookup)\n",
    ")\n",
    "print(\"Extracted article-level locations.\")\n",
    "\n",
    "# 4b: Get SENTENCE-level locations (the specific context)\n",
    "df_risk_sentences['sentence_locations'] = df_risk_sentences['sentence_text'].apply(\n",
    "    lambda text: resolve_locations(text, ner_pipeline, location_lookup)\n",
    ")\n",
    "print(\"Extracted sentence-level locations.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# Step 5: Merge and Apply Hierarchical Logic\n",
    "print(\"# Step 5: Applying Hierarchical Logic and Finalizing Data\")\n",
    "# Merge article context (language and locations) into the sentence dataframe\n",
    "df_merged = pd.merge(\n",
    "    df_risk_sentences,\n",
    "    df_articles_with_risks[['article_id', 'language', 'article_locations']],\n",
    "    on='article_id'\n",
    ")\n",
    "\n",
    "# HIERARCHICAL LOGIC: Choose the most specific location available\n",
    "def choose_locations(row):\n",
    "    # Prioritize specific (longer ID) sentence-level locations\n",
    "    sentence_specific = {loc for loc in row['sentence_locations'] if len(loc) > 2}\n",
    "    if sentence_specific:\n",
    "        return list(sentence_specific)\n",
    "    \n",
    "    # Fallback to any sentence-level locations\n",
    "    if row['sentence_locations']:\n",
    "        return row['sentence_locations']\n",
    "        \n",
    "    # Fallback to specific article-level locations\n",
    "    article_specific = {loc for loc in row['article_locations'] if len(loc) > 2}\n",
    "    if article_specific:\n",
    "        return list(article_specific)\n",
    "        \n",
    "    # Finally, use any article-level location as the last resort\n",
    "    return row['article_locations']\n",
    "\n",
    "df_merged['final_locations'] = df_merged.apply(choose_locations, axis=1)\n",
    "\n",
    "# Explode, clean, and add the English name\n",
    "df_final_exploded = df_merged.explode('final_locations').rename(columns={'final_locations': 'location_id'})\n",
    "df_final_exploded = df_final_exploded.dropna(subset=['location_id'])\n",
    "df_final_exploded['location_name_english'] = df_final_exploded['location_id'].map(id_to_english_name_lookup)\n",
    "\n",
    "# Reorder columns for final output\n",
    "final_columns = [\n",
    "    'article_id', 'date', 'language', 'sentence_text', 'risk_factor',\n",
    "    'confidence_score', 'location_id', 'location_name_english'\n",
    "]\n",
    "df_final_exploded = df_final_exploded[final_columns]\n",
    "\n",
    "print(f\"Created {len(df_final_exploded):,} final, high-precision risk-location pairs.\")\n",
    "print(\"Sample of final data:\")\n",
    "display(df_final_exploded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b647556e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f304389f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualitative",
   "metadata": {},
   "source": [
    "# Part 2: Reflection\n",
    "\n",
    "Please outline (1) some of the limitations of your approach and (2) how you would tackle these if you had more time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49236dab",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
