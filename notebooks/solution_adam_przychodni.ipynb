{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ce3648",
   "metadata": {},
   "source": [
    "# [Take-home Assessment] Food Crisis Early Warning \n",
    "\n",
    "Welcome to the assessment. You will showcase your modeling and research skills by investigating news articles (in English and Arabic) as well as a set of food insecurity risk factors. \n",
    "\n",
    "We suggest planning to spend **~6–8 hours** on this assessment. **Please submit your response by Monday, September 15th, 9:00 AM EST via email to dime_ai@worldbank.org**. Please document your code with comments and explanations of design choices. There is one question on reflecting upon your approach at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cf2966",
   "metadata": {},
   "source": [
    "**Name:** Adam Przychodni\n",
    "\n",
    "**Email:** adam.przychodni@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09329a02",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "# Part 1: Technical Assessment\n",
    "\n",
    "\n",
    "## Task:\n",
    "\n",
    "We invite you to approach the challenge of understanding (and potentially predicting) food insecurity using the provided (limited) data. Your response should demonstrate how you tackle open-ended problems in data-scarce environments.\n",
    "\n",
    "Some example questions to consider:\n",
    "- What is the added value of geospatial data?\n",
    "- How can we address the lack of ground-truth information on food insecurity levels?\n",
    "- What are the benefits and challenges of working with multilingual data?\n",
    "- ...\n",
    "\n",
    "These are just guiding examples — you are free to explore any relevant angles to this topic/data.\n",
    "\n",
    "**Note:** There is no single \"right\" approach. Instead, we want to understand how you approach and structure open-ended problems in data-scarce environments. Given the large number of applicants, we will preselect the most impressive and complete submissions. Please take effort in structuring your response, as selection will depend on its depth and originality.\n",
    "\n",
    "\n",
    "## Provided Data:\n",
    "\n",
    "1. **Risk Factors:** A file containing 167 risk factors (unigrams, bigrams, and trigrams) in the `english_keywords` column and an empty `keywords_arabic` column. A separate file with the mapping of English risk factors to pre-defined thematic cluster assignments.\n",
    "\n",
    "\n",
    "2. **News Articles:** Two files containing one month of news articles from the Mashriq region:\n",
    "   - `news-articles-eng.csv`\n",
    "   - `news-articles-ara.csv`\n",
    "   - **Note:** You may work on a sample subset during development.\n",
    "   \n",
    "   \n",
    "3. **Geographic Taxonomy:** A file containing the names of the countries, provinces, and districts for the subset of Mashriq countries that is covered by the news articles. The files are a dictionary mapping from a key to the geographic name.\n",
    "   - `id_arabic_location_name.pkl`\n",
    "   - `id_english_location_name.pkl`\n",
    "   - **Note:** Each unique country/province/district is assigned a key (e.g. `iq`,`iq_bg` and `iq_bg_1` for country Iraq, province Baghdad, and district 1 in Baghdad respectively).\n",
    "   - The key of country names is a two character abbreviation as follows.\n",
    "       - 'iq': 'Iraq'\n",
    "       - 'jo': 'Jordan'\n",
    "       - 'lb': 'Lebanon'\n",
    "       - 'ps': 'Palestine'\n",
    "       - 'sy': 'Syria'\n",
    "       \n",
    "   - The key of provinces is a two-character abbreviation of the country followed by two-character abbreviation of the province **`{country_abbreviation}_{province_abbreviation}`**, and the key of districts is **`{country_abbreviation}_{province_abbreviation}_{unique_number}`**.\n",
    "       \n",
    "\n",
    "\n",
    "## Submission Guidelines:\n",
    "\n",
    "- **Code:** Follow best coding practices and ensure clear documentation. All notebook cells should be executed with outputs saved, and the notebook should run correctly on its own. Name your file **`solution_{FIRSTNAME}_{LASTNAME}.ipynb`**. If your solution relies on additional open-access data, either include it in your submission (e.g., as part of a ZIP file) or provide clear data-loading code/instructions as part of the nottebook. \n",
    "- **Report:** Submit a separate markdown file communicating your approach to this research problem. We expect you to detail the models, methods, or (additional) data you are using.\n",
    "\n",
    "Good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f9934a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task1",
   "metadata": {},
   "source": [
    "## Your Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3029b2",
   "metadata": {},
   "source": [
    "tutaj mialem rowniez pomysl zeby przetestowac LLM model ktory by prxefiltorowal po regionach i lokalizacjach zeby miec pewnosc ze wyciagamy dane tylko dla interesujacych nas obszarow chcialem przestestowac NER natomiast byl on bardzo computionally kosztowny i nie zdazylem poprawnie zweryfikowac. \n",
    "\n",
    "parts of code generated and also formatted using LLMs, Gemini 2.5 Pro and Claude Opus 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd3ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 15:21:58,853 - INFO - ============================================================\n",
      "2025-09-13 15:21:58,854 - INFO - Starting news article processing pipeline\n",
      "2025-09-13 15:21:58,854 - INFO - ============================================================\n",
      "2025-09-13 15:21:58,855 - INFO - Geographic filtering: ENABLED\n",
      "2025-09-13 15:21:58,856 - INFO -   - Pre-filtering: ENABLED\n",
      "2025-09-13 15:21:58,856 - INFO -   - NER filtering: DISABLED\n",
      "2025-09-13 15:21:58,857 - INFO -   - Using all keywords: True\n",
      "2025-09-13 15:21:58,857 - INFO -   - Keywords sample size: 50\n",
      "2025-09-13 15:21:58,858 - INFO - Loading news articles from CSV files...\n",
      "2025-09-13 15:22:14,999 - INFO - Loaded 86,660 English and 85,511 Arabic articles\n",
      "2025-09-13 15:22:15,044 - INFO - Loaded 918 unique location aliases\n",
      "2025-09-13 15:22:15,046 - INFO - Using all 918 location keywords for pre-filtering\n",
      "2025-09-13 15:22:15,067 - INFO - Starting pre-filtering...\n",
      "2025-09-13 15:32:21,843 - INFO - Pre-filtering complete: 162117/172171 articles contain location keywords (94.2%)\n",
      "2025-09-13 15:32:21,862 - INFO - NER filtering disabled.\n",
      "2025-09-13 15:32:22,311 - INFO - Final counts: 84,970 English, 77,147 Arabic articles\n",
      "2025-09-13 15:32:22,355 - INFO - Processing English articles...\n",
      "2025-09-13 15:32:47,469 - INFO - Cleaned 84,970 English articles\n",
      "2025-09-13 15:32:59,383 - INFO - Tokenized English articles: avg 35.2 sentences per article\n",
      "2025-09-13 15:32:59,386 - INFO - Processing Arabic articles...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample Tokenization Results ---\n",
      "Article split into 124 sentences\n",
      "First 3 sentences:\n",
      "  1. Hussam al-Mahmoud | Yamen Moghrabi | Hassan Ibrahim On October 7, 2023, the world and the Middle East awoke to the drums of war beating in the Gaza Strip.\n",
      "  2. Over time, it turned into a reality that American efforts, Qatari and Egyptian mediation, condemnations, statements, summits, and conferences could not stop.\n",
      "  3. While Israel continues its war in the besieged Gaza Strip, attention is turning towards the potential outbreak of another war.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 15:33:12,567 - INFO - Cleaned 77,147 Arabic articles\n",
      "2025-09-13 15:33:17,889 - INFO - Tokenized Arabic articles: avg 11.8 sentences per article\n",
      "2025-09-13 15:33:28,650 - INFO - Saved English data to: ../data/02_processed/news_eng_processed.pkl\n",
      "2025-09-13 15:33:28,651 - INFO - Saved Arabic data to: ../data/02_processed/news_ara_processed.pkl\n",
      "2025-09-13 15:33:28,652 - INFO - ============================================================\n",
      "2025-09-13 15:33:28,653 - INFO - Pipeline completed successfully!\n",
      "2025-09-13 15:33:28,653 - INFO - ============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Strategy 4 - Pre-filtering Only:\n",
      "English articles: 84,970\n",
      "Arabic articles: 77,147\n",
      "\n",
      "Memory cleaned up!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "News article processing pipeline for multilingual text data.\n",
    "Balanced version with improved pre-filtering strategy.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Set, Tuple\n",
    "from transformers import pipeline, Pipeline\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Regex patterns as module constants\n",
    "HTML_PATTERN = re.compile(r'<.*?>')\n",
    "URL_PATTERN = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "WHITESPACE_PATTERN = re.compile(r'\\s+')\n",
    "SENTENCE_SPLIT_PATTERN = re.compile(r'(?<=[.!?۔])\\s+')\n",
    "LETTER_PATTERN = re.compile(r'[a-zA-Zء-ي]')\n",
    "\n",
    "\n",
    "def get_device() -> int:\n",
    "    \"\"\"\n",
    "    Determine the best available device for processing.\n",
    "    \n",
    "    Returns:\n",
    "        int: Device ID (0 for GPU, -1 for CPU).\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "        return 0\n",
    "    else:\n",
    "        logger.info(\"No GPU found, using CPU\")\n",
    "        return -1\n",
    "\n",
    "\n",
    "def load_location_lookup(data_dir: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Load location dictionaries for geographic filtering.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing raw data.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary mapping location names to IDs.\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If location files don't exist.\n",
    "    \"\"\"\n",
    "    raw_dir = Path(data_dir) / '01_raw'\n",
    "    eng_path = raw_dir / 'id_english_location_name.pkl'\n",
    "    ara_path = raw_dir / 'id_arabic_location_name.pkl'\n",
    "    \n",
    "    location_lookup = {}\n",
    "    \n",
    "    try:\n",
    "        with open(eng_path, 'rb') as f:\n",
    "            eng_locations = pickle.load(f)\n",
    "        with open(ara_path, 'rb') as f:\n",
    "            ara_locations = pickle.load(f)\n",
    "        \n",
    "        # Build lookup dictionary\n",
    "        for location_dict in [eng_locations, ara_locations]:\n",
    "            for loc_id, names in location_dict.items():\n",
    "                for name in names:\n",
    "                    location_lookup[name.lower()] = loc_id\n",
    "        \n",
    "        logger.info(f\"Loaded {len(location_lookup):,} unique location aliases\")\n",
    "        return location_lookup\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Location file not found: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def prefilter_by_keywords(df: pd.DataFrame, location_lookup: Dict[str, str], \n",
    "                         use_all_keywords: bool = False, sample_size: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Balanced keyword-based pre-filtering before expensive NER.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with articles.\n",
    "        location_lookup: Dictionary of location names.\n",
    "        use_all_keywords: If True, use all keywords. If False, use sample_size.\n",
    "        sample_size: Number of most common locations to use if not using all.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Pre-filtered dataframe.\n",
    "    \"\"\"\n",
    "    location_keywords = list(location_lookup.keys())\n",
    "    \n",
    "    if not use_all_keywords and len(location_keywords) > sample_size:\n",
    "        # Prioritize important locations (countries and major cities)\n",
    "        # Sort by length - shorter names are often more important\n",
    "        location_keywords = sorted(location_keywords, key=len)[:sample_size]\n",
    "        logger.info(f\"Using top {sample_size} location keywords for pre-filtering\")\n",
    "    else:\n",
    "        logger.info(f\"Using all {len(location_keywords)} location keywords for pre-filtering\")\n",
    "    \n",
    "    # Create more flexible regex pattern\n",
    "    # Use word boundaries for whole word matching\n",
    "    escaped_keywords = [re.escape(loc) for loc in location_keywords]\n",
    "    # Create pattern in chunks to avoid regex size limits\n",
    "    chunk_size = 100\n",
    "    patterns = []\n",
    "    \n",
    "    for i in range(0, len(escaped_keywords), chunk_size):\n",
    "        chunk = escaped_keywords[i:i+chunk_size]\n",
    "        pattern = r'\\b(' + '|'.join(chunk) + r')\\b'\n",
    "        patterns.append(re.compile(pattern, re.IGNORECASE))\n",
    "    \n",
    "    def contains_location(text):\n",
    "        if not isinstance(text, str):\n",
    "            return False\n",
    "        # Check against all pattern chunks\n",
    "        for pattern in patterns:\n",
    "            if pattern.search(text):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    # Apply pre-filter\n",
    "    logger.info(\"Starting pre-filtering...\")\n",
    "    mask = df['body'].apply(contains_location)\n",
    "    df_prefiltered = df[mask].copy()\n",
    "    \n",
    "    logger.info(f\"Pre-filtering complete: {len(df_prefiltered)}/{len(df)} articles \"\n",
    "               f\"contain location keywords ({len(df_prefiltered)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # If pre-filtering is too aggressive, warn\n",
    "    if len(df_prefiltered) < len(df) * 0.01:  # Less than 1%\n",
    "        logger.warning(\"Pre-filtering may be too aggressive! Consider adjusting parameters.\")\n",
    "    \n",
    "    return df_prefiltered\n",
    "\n",
    "\n",
    "def initialize_ner_pipeline(model_name: str = \"Babelscape/wikineural-multilingual-ner\", \n",
    "                           device: Optional[int] = None) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Initialize NER pipeline for geographic filtering.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the NER model to use.\n",
    "        device: Device ID (None for auto-detection).\n",
    "        \n",
    "    Returns:\n",
    "        Pipeline: Initialized NER pipeline.\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If model loading fails.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "        \n",
    "    logger.info(f\"Loading NER model: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        ner_pipeline = pipeline(\n",
    "            \"ner\",\n",
    "            model=model_name,\n",
    "            aggregation_strategy=\"simple\",\n",
    "            device=device\n",
    "        )\n",
    "        logger.info(\"NER pipeline initialized successfully\")\n",
    "        return ner_pipeline\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load NER model: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def filter_by_geography(df: pd.DataFrame, location_lookup: Dict[str, str], \n",
    "                       ner_pipeline: Pipeline, max_text_length: int = 1000,\n",
    "                       batch_size: int = 256, chunk_size: int = 5000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter articles containing target geographic locations.\n",
    "    \n",
    "    Args:\n",
    "        df: Combined dataframe of articles.\n",
    "        location_lookup: Dictionary mapping location names to IDs.\n",
    "        ner_pipeline: Initialized NER pipeline.\n",
    "        max_text_length: Maximum text length for NER processing.\n",
    "        batch_size: Batch size for NER processing.\n",
    "        chunk_size: Size of chunks for progress tracking.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered dataframe with location metadata.\n",
    "    \"\"\"\n",
    "    # Convert lookup keys to a set for O(1) lookups\n",
    "    location_set = set(location_lookup.keys())\n",
    "    \n",
    "    # Prepare texts for NER\n",
    "    article_bodies = df['body'].fillna('').tolist()\n",
    "    texts_to_process = [text[:max_text_length] for text in article_bodies]\n",
    "    \n",
    "    total_articles = len(texts_to_process)\n",
    "    logger.info(f\"Running NER on {total_articles:,} articles...\")\n",
    "    logger.info(f\"Batch size: {batch_size}, Chunk size: {chunk_size}\")\n",
    "    logger.info(f\"Max text length: {max_text_length} chars\")\n",
    "    \n",
    "    # Process in chunks to show progress and manage memory\n",
    "    all_relevance = []\n",
    "    all_locations = []\n",
    "    \n",
    "    with tqdm(total=total_articles, desc=\"Processing articles\") as pbar:\n",
    "        for i in range(0, total_articles, chunk_size):\n",
    "            chunk_end = min(i + chunk_size, total_articles)\n",
    "            chunk_texts = texts_to_process[i:chunk_end]\n",
    "            \n",
    "            # Extract entities for this chunk\n",
    "            try:\n",
    "                chunk_entities = ner_pipeline(chunk_texts, batch_size=batch_size)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error processing chunk {i//chunk_size + 1}: {e}\")\n",
    "                # If batch fails, try smaller batches\n",
    "                logger.info(\"Retrying with smaller batch size...\")\n",
    "                chunk_entities = ner_pipeline(chunk_texts, batch_size=batch_size//2)\n",
    "            \n",
    "            # Process results\n",
    "            for article_entities in chunk_entities:\n",
    "                found_locations = []\n",
    "                \n",
    "                # Use set operations for faster checking\n",
    "                for entity in article_entities:\n",
    "                    if entity.get('entity_group') == 'LOC':\n",
    "                        word_lower = entity.get('word', '').lower()\n",
    "                        if word_lower in location_set:\n",
    "                            found_locations.append(entity['word'])\n",
    "                \n",
    "                all_relevance.append(len(found_locations) > 0)\n",
    "                all_locations.append(found_locations)\n",
    "            \n",
    "            # Update progress\n",
    "            pbar.update(chunk_end - i)\n",
    "            \n",
    "            # Clear GPU cache periodically to prevent OOM\n",
    "            if i % (chunk_size * 5) == 0 and i > 0:\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                logger.info(f\"Cleared memory at article {i}\")\n",
    "    \n",
    "    # Add metadata and filter\n",
    "    df['matched_locations'] = all_locations\n",
    "    df_filtered = df[all_relevance].copy()\n",
    "    \n",
    "    # Log statistics\n",
    "    logger.info(f\"Geographic filtering complete:\")\n",
    "    logger.info(f\"  - Original articles: {len(df):,}\")\n",
    "    logger.info(f\"  - Relevant articles: {len(df_filtered):,}\")\n",
    "    logger.info(f\"  - Retention rate: {len(df_filtered)/len(df)*100:.1f}%\")\n",
    "    \n",
    "    # Calculate location frequency\n",
    "    if len(df_filtered) > 0:\n",
    "        all_matched_locs = [loc for locs in df_filtered['matched_locations'] for loc in locs]\n",
    "        if all_matched_locs:\n",
    "            from collections import Counter\n",
    "            loc_freq = Counter(all_matched_locs)\n",
    "            logger.info(f\"  - Top 5 locations: {loc_freq.most_common(5)}\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def load_news_data(data_dir: str, enable_geo_filter: bool = True,\n",
    "                  max_text_length: int = 1000, batch_size: int = 256,\n",
    "                  ner_model: str = \"Babelscape/wikineural-multilingual-ner\",\n",
    "                  use_prefilter: bool = True, run_ner_filter: bool = True,\n",
    "                  use_all_keywords: bool = False,\n",
    "                  prefilter_sample: int = 100) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load English and Arabic news datasets with optional geographic filtering.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing raw data.\n",
    "        enable_geo_filter: Whether to apply geographic filtering.\n",
    "        max_text_length: Maximum text length for NER processing.\n",
    "        batch_size: Batch size for NER processing.\n",
    "        ner_model: Model name for NER pipeline.\n",
    "        use_prefilter: Whether to use keyword pre-filtering.\n",
    "        run_ner_filter: Whether to run the NER model after pre-filtering.\n",
    "        use_all_keywords: Whether to use all location keywords.\n",
    "        prefilter_sample: Number of keywords to use if not using all.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: Tuple of (english_df, arabic_df).\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If dataset files don't exist.\n",
    "    \"\"\"\n",
    "    raw_dir = Path(data_dir) / '01_raw'\n",
    "    eng_path = raw_dir / 'news-articles-eng.csv'\n",
    "    ara_path = raw_dir / 'news-articles-ara.csv'\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"Loading news articles from CSV files...\")\n",
    "        df_eng = pd.read_csv(eng_path)\n",
    "        df_ara = pd.read_csv(ara_path)\n",
    "        \n",
    "        # Add language column for tracking\n",
    "        df_eng['language'] = 'english'\n",
    "        df_ara['language'] = 'arabic'\n",
    "        \n",
    "        logger.info(f\"Loaded {len(df_eng):,} English and {len(df_ara):,} Arabic articles\")\n",
    "        \n",
    "        # Combine for geographic filtering\n",
    "        df_combined = pd.concat([df_eng, df_ara], ignore_index=True)\n",
    "        \n",
    "        # Apply geographic filtering if enabled\n",
    "        if enable_geo_filter:\n",
    "            location_lookup = load_location_lookup(data_dir)\n",
    "            \n",
    "            # Apply keyword pre-filtering if enabled\n",
    "            if use_prefilter:\n",
    "                df_combined = prefilter_by_keywords(\n",
    "                    df_combined, location_lookup, use_all_keywords, prefilter_sample\n",
    "                )\n",
    "                if df_combined.empty:\n",
    "                    logger.warning(\"Pre-filtering removed all articles! No data to process further.\")\n",
    "                    return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "            # Apply NER-based filtering if enabled\n",
    "            if run_ner_filter:\n",
    "                if not df_combined.empty:\n",
    "                    ner_pipeline = initialize_ner_pipeline(ner_model)\n",
    "                    df_combined = filter_by_geography(\n",
    "                        df_combined, location_lookup, ner_pipeline, \n",
    "                        max_text_length, batch_size, chunk_size=5000\n",
    "                    )\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                else:\n",
    "                    logger.warning(\"Skipping NER as no articles remain after pre-filtering.\")\n",
    "            else:\n",
    "                logger.info(\"NER filtering disabled.\")\n",
    "                # Ensure 'matched_locations' column exists for consistency\n",
    "                df_combined['matched_locations'] = [[] for _ in range(len(df_combined))]\n",
    "            \n",
    "            # Handle edge case where both filters are off but geo_filter is on\n",
    "            if not use_prefilter and not run_ner_filter:\n",
    "                logger.warning(\"Geo-filtering enabled, but both pre-filter and NER are off. All articles will be processed.\")\n",
    "\n",
    "        else:\n",
    "            logger.info(\"Geographic filtering disabled, using all articles\")\n",
    "            df_combined['matched_locations'] = [[] for _ in range(len(df_combined))]\n",
    "        \n",
    "        # Split back into language-specific dataframes\n",
    "        df_eng_filtered = df_combined[df_combined['language'] == 'english'].copy()\n",
    "        df_ara_filtered = df_combined[df_combined['language'] == 'arabic'].copy()\n",
    "        \n",
    "        # Remove the language column\n",
    "        df_eng_filtered = df_eng_filtered.drop('language', axis=1)\n",
    "        df_ara_filtered = df_ara_filtered.drop('language', axis=1)\n",
    "        \n",
    "        logger.info(f\"Final counts: {len(df_eng_filtered):,} English, \"\n",
    "                   f\"{len(df_ara_filtered):,} Arabic articles\")\n",
    "        \n",
    "        return df_eng_filtered, df_ara_filtered\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Dataset file not found: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def clean_text(text: Optional[str]) -> str:\n",
    "    \"\"\"\n",
    "    Clean raw text by removing HTML tags, URLs, and normalizing whitespace.\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text string to clean.\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text string.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = HTML_PATTERN.sub('', text)\n",
    "    # Remove URLs\n",
    "    text = URL_PATTERN.sub('', text)\n",
    "    # Normalize whitespace\n",
    "    text = WHITESPACE_PATTERN.sub(' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize_sentences(text: Optional[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences using regex and filter non-textual results.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to tokenize.\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of sentence strings.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return []\n",
    "    \n",
    "    # Split on sentence-ending punctuation\n",
    "    sentences = SENTENCE_SPLIT_PATTERN.split(text)\n",
    "    \n",
    "    # Filter empty strings and non-textual content\n",
    "    valid_sentences = [\n",
    "        s for s in sentences \n",
    "        if s and LETTER_PATTERN.search(s)\n",
    "    ]\n",
    "    \n",
    "    return valid_sentences\n",
    "\n",
    "\n",
    "def process_dataframe(df: pd.DataFrame, language: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply cleaning and tokenization to a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'body' column containing article text.\n",
    "        language: Language identifier for logging.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with cleaned text and sentences.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Processing {language} articles...\")\n",
    "    \n",
    "    # Clean text\n",
    "    df['body_cleaned'] = df['body'].apply(clean_text)\n",
    "    logger.info(f\"Cleaned {len(df):,} {language} articles\")\n",
    "    \n",
    "    # Tokenize into sentences\n",
    "    df['sentences'] = df['body_cleaned'].apply(tokenize_sentences)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    sentence_counts = df['sentences'].apply(len)\n",
    "    if len(sentence_counts) > 0:\n",
    "        logger.info(f\"Tokenized {language} articles: \"\n",
    "                   f\"avg {sentence_counts.mean():.1f} sentences per article\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def save_processed_data(df_eng: pd.DataFrame, df_ara: pd.DataFrame, data_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Save processed dataframes to pickle files.\n",
    "    \n",
    "    Args:\n",
    "        df_eng: Processed English dataframe.\n",
    "        df_ara: Processed Arabic dataframe.\n",
    "        data_dir: Root directory for saving data.\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If saving fails.\n",
    "    \"\"\"\n",
    "    processed_dir = Path(data_dir) / '02_processed'\n",
    "    processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    eng_path = processed_dir / 'news_eng_processed.pkl'\n",
    "    ara_path = processed_dir / 'news_ara_processed.pkl'\n",
    "    \n",
    "    try:\n",
    "        df_eng.to_pickle(eng_path)\n",
    "        df_ara.to_pickle(ara_path)\n",
    "        logger.info(f\"Saved English data to: {eng_path}\")\n",
    "        logger.info(f\"Saved Arabic data to: {ara_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving processed data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def display_sample(df: pd.DataFrame, num_sentences: int = 3) -> None:\n",
    "    \"\"\"\n",
    "    Display sample processed sentences for verification.\n",
    "    \n",
    "    Args:\n",
    "        df: Processed dataframe.\n",
    "        num_sentences: Number of sentences to display.\n",
    "    \"\"\"\n",
    "    if df.empty or 'sentences' not in df.columns:\n",
    "        logger.warning(\"No sentences to display\")\n",
    "        return\n",
    "    \n",
    "    first_article = df.iloc[0]\n",
    "    sentences = first_article['sentences']\n",
    "    \n",
    "    print(f\"\\n--- Sample Tokenization Results ---\")\n",
    "    print(f\"Article split into {len(sentences)} sentences\")\n",
    "    print(f\"First {min(num_sentences, len(sentences))} sentences:\")\n",
    "    \n",
    "    for i, sentence in enumerate(sentences[:num_sentences], 1):\n",
    "        print(f\"  {i}. {sentence}\")\n",
    "    \n",
    "    # Show matched locations if available\n",
    "    if 'matched_locations' in df.columns and first_article['matched_locations']:\n",
    "        print(f\"\\nMatched locations: {', '.join(first_article['matched_locations'])}\")\n",
    "\n",
    "\n",
    "def run_news_processing_pipeline(data_dir: str = '../data', \n",
    "                                enable_geo_filter: bool = True,\n",
    "                                use_prefilter: bool = True,\n",
    "                                run_ner_filter: bool = True,\n",
    "                                max_text_length: int = 1000,\n",
    "                                batch_size: int = 256,\n",
    "                                ner_model: str = \"Babelscape/wikineural-multilingual-ner\",\n",
    "                                use_all_keywords: bool = False,\n",
    "                                prefilter_sample: int = 50) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Execute the complete news processing pipeline with balanced filtering.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing raw and processed data folders.\n",
    "        enable_geo_filter: Whether to apply geographic filtering.\n",
    "        use_prefilter: Whether to use keyword pre-filtering.\n",
    "        run_ner_filter: Whether to run the NER model for filtering.\n",
    "        max_text_length: Maximum text length for NER processing.\n",
    "        batch_size: Batch size for NER processing.\n",
    "        ner_model: Model name for NER pipeline.\n",
    "        use_all_keywords: Whether to use all location keywords.\n",
    "        prefilter_sample: Number of keywords to use if not using all.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: Tuple of processed (english_df, arabic_df).\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If required files don't exist.\n",
    "        Exception: If processing fails.\n",
    "    \"\"\"\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"Starting news article processing pipeline\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    \n",
    "    if enable_geo_filter:\n",
    "        logger.info(\"Geographic filtering: ENABLED\")\n",
    "        logger.info(f\"  - Pre-filtering: {'ENABLED' if use_prefilter else 'DISABLED'}\")\n",
    "        logger.info(f\"  - NER filtering: {'ENABLED' if run_ner_filter else 'DISABLED'}\")\n",
    "        if use_prefilter:\n",
    "            logger.info(f\"  - Using all keywords: {use_all_keywords}\")\n",
    "            logger.info(f\"  - Keywords sample size: {prefilter_sample}\")\n",
    "        if run_ner_filter:\n",
    "            logger.info(f\"  - Max text length: {max_text_length} chars\")\n",
    "            logger.info(f\"  - Batch size: {batch_size}\")\n",
    "    else:\n",
    "        logger.info(\"Geographic filtering: DISABLED\")\n",
    "    \n",
    "    # Load data (with geographic filtering if enabled)\n",
    "    df_eng, df_ara = load_news_data(\n",
    "        data_dir=data_dir, \n",
    "        enable_geo_filter=enable_geo_filter, \n",
    "        max_text_length=max_text_length, \n",
    "        batch_size=batch_size, \n",
    "        ner_model=ner_model, \n",
    "        use_prefilter=use_prefilter, \n",
    "        run_ner_filter=run_ner_filter,\n",
    "        use_all_keywords=use_all_keywords, \n",
    "        prefilter_sample=prefilter_sample\n",
    "    )\n",
    "    \n",
    "    # Check if we have data to process\n",
    "    if df_eng.empty and df_ara.empty:\n",
    "        logger.warning(\"No articles to process after filtering!\")\n",
    "        return df_eng, df_ara\n",
    "    \n",
    "    # Process each dataset\n",
    "    if not df_eng.empty:\n",
    "        df_eng = process_dataframe(df_eng, \"English\")\n",
    "        display_sample(df_eng)\n",
    "    \n",
    "    if not df_ara.empty:\n",
    "        df_ara = process_dataframe(df_ara, \"Arabic\")\n",
    "    \n",
    "    # Save processed data\n",
    "    save_processed_data(df_eng, df_ara, data_dir)\n",
    "    \n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"Pipeline completed successfully!\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    \n",
    "    return df_eng, df_ara\n",
    "\n",
    "# ============================================================\n",
    "# STRATEGY 1: NO FILTERING (Process all articles)\n",
    "# ============================================================\n",
    "# Use this to get ALL articles for comprehensive analysis\n",
    "# df_eng_all, df_ara_all = run_news_processing_pipeline(\n",
    "#     data_dir='../data',\n",
    "#     enable_geo_filter=False  # Disable geographic filtering entirely\n",
    "# )\n",
    "\n",
    "# print(f\"\\nStrategy 1 - No Filtering:\")\n",
    "# print(f\"English articles: {len(df_eng_all):,}\")\n",
    "# print(f\"Arabic articles: {len(df_ara_all):,}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STRATEGY 2: BALANCED FILTERING (Recommended: Pre-filter + NER)\n",
    "# ============================================================\n",
    "# Use focused keywords for pre-filtering, then apply NER\n",
    "# df_eng_balanced, df_ara_balanced = run_news_processing_pipeline(\n",
    "#     data_dir='../data',\n",
    "#     enable_geo_filter=True,\n",
    "#     use_prefilter=True,\n",
    "#     run_ner_filter=True,      # Ensure NER is also on\n",
    "#     use_all_keywords=True,  \n",
    "#     # prefilter_sample=50,    # Focus on 50 most important locations\n",
    "#     max_text_length=1500,     # Look at more text\n",
    "#     batch_size=512            # Balanced batch size\n",
    "# )\n",
    "\n",
    "# print(f\"\\nStrategy 2 - Balanced Filtering (Prefilter + NER):\")\n",
    "# print(f\"English articles: {len(df_eng_balanced):,}\")\n",
    "# print(f\"Arabic articles: {len(df_ara_balanced):,}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STRATEGY 3: NER-ONLY FILTERING (More inclusive, slower)\n",
    "# ============================================================\n",
    "# Skip pre-filtering but still use NER\n",
    "# df_eng_relaxed, df_ara_relaxed = run_news_processing_pipeline(\n",
    "#     data_dir='../data',\n",
    "#     enable_geo_filter=True,\n",
    "#     use_prefilter=False,      # Skip pre-filtering\n",
    "#     run_ner_filter=True,      # Run NER\n",
    "#     max_text_length=1000,     # Reduced for speed\n",
    "#     batch_size=512            # Larger batches for efficiency\n",
    "# )\n",
    "\n",
    "# print(f\"\\nStrategy 3 - NER-Only Filtering:\")\n",
    "# print(f\"English articles: {len(df_eng_relaxed):,}\")\n",
    "# print(f\"Arabic articles: {len(df_ara_relaxed):,}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STRATEGY 4: PRE-FILTERING ONLY (Fastest geographic filtering)\n",
    "# ============================================================\n",
    "# Use this for a quick, less precise filtering based on keywords.\n",
    "df_eng_pre, df_ara_pre = run_news_processing_pipeline(\n",
    "    data_dir='../data',\n",
    "    enable_geo_filter=True,\n",
    "    use_prefilter=True,\n",
    "    run_ner_filter=False,     # The key change: disable NER\n",
    "    use_all_keywords=True,    # Use all available location keywords\n",
    ")\n",
    "\n",
    "print(f\"\\nStrategy 4 - Pre-filtering Only:\")\n",
    "print(f\"English articles: {len(df_eng_pre):,}\")\n",
    "print(f\"Arabic articles: {len(df_ara_pre):,}\")\n",
    "\n",
    "# ============================================================\n",
    "# MEMORY CLEANUP\n",
    "# ============================================================\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"\\nMemory cleaned up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc460808",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d4ad82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-14 07:51:09,112 - INFO - ============================================================\n",
      "2025-09-14 07:51:09,112 - INFO - Starting risk factor extraction pipeline\n",
      "2025-09-14 07:51:09,113 - INFO - ============================================================\n",
      "2025-09-14 07:51:09,114 - INFO - Running in FULL mode (all articles)\n",
      "2025-09-14 07:51:11,693 - INFO - Loaded 84,970 English articles.\n",
      "2025-09-14 07:51:15,085 - INFO - Loaded 77,147 Arabic articles.\n",
      "2025-09-14 07:51:15,140 - INFO - Combined data: 162,117 total articles.\n",
      "2025-09-14 07:51:15,596 - INFO - Loaded 167 risk factors\n",
      "2025-09-14 07:51:15,643 - INFO - GPU available: NVIDIA L4\n",
      "Device set to use cuda:0\n",
      "2025-09-14 07:51:17,202 - INFO - Classifier initialized: MoritzLaurer/deberta-v3-xsmall-zeroshot-v1.1-all-33\n",
      "2025-09-14 07:51:17,204 - INFO - GPU available: NVIDIA L4\n",
      "2025-09-14 07:51:17,207 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n",
      "2025-09-14 07:51:19,966 - INFO - Embedder initialized: paraphrase-multilingual-MiniLM-L12-v2\n",
      "2025-09-14 07:51:27,552 - INFO - Prepared 3,897,406 sentences from articles\n",
      "2025-09-14 07:51:27,650 - INFO - Pre-filtering 3,897,406 sentences...\n",
      "2025-09-14 07:51:27,650 - INFO - Similarity threshold: 0.7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e9335222914e0ea932a50e4153cc01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574efe87de8e4d4fbe16127b36867f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/121794 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-14 08:22:48,600 - INFO - Reduced to 12,782 relevant sentences (0.3% retention)\n",
      "2025-09-14 08:22:49,037 - INFO - Classifying 12,782 sentences...\n",
      "2025-09-14 08:22:49,037 - INFO - Confidence threshold: 0.95\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d8028327194b00b054b17e01d0e480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifying sentences:   0%|          | 0/12782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-14 08:44:41,619 - INFO - Found 19,060 risk factor mentions\n",
      "2025-09-14 08:44:41,623 - INFO - Refining results to highest confidence per sentence...\n",
      "2025-09-14 08:44:41,751 - INFO - Refined from 19,060 to 2,238 unique mentions\n",
      "2025-09-14 08:44:41,772 - INFO - Saved 2,238 risk mentions to ../data/03_models/risk_mentions_FULL.csv\n",
      "2025-09-14 08:44:41,776 - INFO - ============================================================\n",
      "2025-09-14 08:44:41,777 - INFO - Pipeline completed successfully!\n",
      "2025-09-14 08:44:41,777 - INFO - ============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EXTRACTION STATISTICS\n",
      "==================================================\n",
      "Total articles processed: 162,117\n",
      "Total sentences extracted: 3,897,406\n",
      "Sentences after filtering: 12,782\n",
      "Risk mentions found: 19,060\n",
      "Unique mentions (refined): 2,238\n",
      "\n",
      "Unique risk factors: 141\n",
      "Average confidence: 0.988\n",
      "Confidence range: 0.950 - 0.999\n",
      "\n",
      "Top 5 risk factors:\n",
      "  - conflict: 288 mentions\n",
      "  - repression: 165 mentions\n",
      "  - weather extremes: 85 mentions\n",
      "  - without international aid: 76 mentions\n",
      "  - humanitarian situation: 74 mentions\n",
      "==================================================\n",
      "\n",
      "--- Full Run: Top Risk Mentions ---\n",
      "              risk_factor  confidence_score  \\\n",
      "5562              pirates          0.999301   \n",
      "5560              pirates          0.999278   \n",
      "9307              pirates          0.999264   \n",
      "9301              pirates          0.999263   \n",
      "10237  call for donations          0.999262   \n",
      "\n",
      "                                      sentence_text  \n",
      "5562        Pirates: Camden Janik, C, Illinois 355.  \n",
      "5560   Pirates: Matt Ager, P, UC Santa Barbara 175.  \n",
      "9307                             Pirates 9, Reds 4.  \n",
      "9301                             Pirates 8, Reds 3.  \n",
      "10237                          Please donate today.  \n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Risk factor extraction pipeline for geographically filtered news articles.\n",
    "Uses zero-shot classification to identify risk factors in article sentences.\n",
    "Functional implementation without classes.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from transformers import pipeline, Pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_device() -> int:\n",
    "    \"\"\"\n",
    "    Determine the best available device for processing.\n",
    "    \n",
    "    Returns:\n",
    "        int: Device ID (0 for GPU, -1 for CPU).\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device_name = torch.cuda.get_device_name(0)\n",
    "        logger.info(f\"GPU available: {device_name}\")\n",
    "        return 0\n",
    "    else:\n",
    "        logger.info(\"No GPU found, using CPU\")\n",
    "        return -1\n",
    "\n",
    "\n",
    "def load_filtered_data(data_dir: str, use_sample: bool = True, sample_size: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load geographically filtered English and Arabic article data and combine them.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing processed data.\n",
    "        use_sample: Whether to use a sample of articles for testing.\n",
    "        sample_size: Number of articles to sample if use_sample is True.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A combined DataFrame of filtered articles.\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If no processed data files are found.\n",
    "    \"\"\"\n",
    "    processed_dir = Path(data_dir) / '02_processed'\n",
    "    eng_path = processed_dir / 'news_eng_processed.pkl'\n",
    "    ara_path = processed_dir / 'news_ara_processed.pkl'\n",
    "    \n",
    "    df_list = []\n",
    "    \n",
    "    # Load English data if it exists and is not empty\n",
    "    try:\n",
    "        df_eng = pd.read_pickle(eng_path)\n",
    "        if not df_eng.empty:\n",
    "            df_list.append(df_eng)\n",
    "            logger.info(f\"Loaded {len(df_eng):,} English articles.\")\n",
    "    except FileNotFoundError:\n",
    "        logger.warning(f\"English data file not found at: {eng_path}\")\n",
    "\n",
    "    # Load Arabic data if it exists and is not empty\n",
    "    try:\n",
    "        df_ara = pd.read_pickle(ara_path)\n",
    "        if not df_ara.empty:\n",
    "            df_list.append(df_ara)\n",
    "            logger.info(f\"Loaded {len(df_ara):,} Arabic articles.\")\n",
    "    except FileNotFoundError:\n",
    "        logger.warning(f\"Arabic data file not found at: {ara_path}\")\n",
    "\n",
    "    if not df_list:\n",
    "        error_msg = \"No processed data found. Please run the news processing pipeline first.\"\n",
    "        logger.error(error_msg)\n",
    "        raise FileNotFoundError(error_msg)\n",
    "        \n",
    "    # Combine the dataframes\n",
    "    df_combined = pd.concat(df_list, ignore_index=True)\n",
    "    logger.info(f\"Combined data: {len(df_combined):,} total articles.\")\n",
    "\n",
    "    if use_sample and not df_combined.empty:\n",
    "        # Ensure sample size is not larger than the dataframe\n",
    "        sample_size = min(sample_size, len(df_combined))\n",
    "        df_combined = df_combined.sample(n=sample_size, random_state=42).copy()\n",
    "        logger.info(f\"Using a random sample of {len(df_combined)} articles for processing.\")\n",
    "    \n",
    "    return df_combined\n",
    "\n",
    "\n",
    "def load_risk_factors(data_dir: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load risk factor labels from Excel file.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing raw data.\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of risk factor labels.\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If risk factors file doesn't exist.\n",
    "    \"\"\"\n",
    "    raw_dir = Path(data_dir) / '01_raw'\n",
    "    risk_path = raw_dir / 'risk-factors.xlsx'\n",
    "    \n",
    "    try:\n",
    "        df_risk = pd.read_excel(risk_path)\n",
    "        df_risk.dropna(subset=['risk_factor_english'], inplace=True)\n",
    "        \n",
    "        risk_factor_labels = df_risk['risk_factor_english'].tolist()\n",
    "        logger.info(f\"Loaded {len(risk_factor_labels)} risk factors\")\n",
    "        \n",
    "        return risk_factor_labels\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Risk factors file not found: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def initialize_classifier(model_name: str = 'MoritzLaurer/deberta-v3-xsmall-zeroshot-v1.1-all-33',\n",
    "                         device: Optional[int] = None) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Initialize zero-shot classification pipeline.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the classification model to use.\n",
    "        device: Device ID (None for auto-detection).\n",
    "        \n",
    "    Returns:\n",
    "        Pipeline: Initialized classification pipeline.\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If model loading fails.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "        \n",
    "    try:\n",
    "        classifier = pipeline(\n",
    "            \"zero-shot-classification\",\n",
    "            model=model_name,\n",
    "            device=device\n",
    "        )\n",
    "        logger.info(f\"Classifier initialized: {model_name}\")\n",
    "        return classifier\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize classifier: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def initialize_embedder(model_name: str = 'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "                       device: Optional[int] = None) -> SentenceTransformer:\n",
    "    \"\"\"\n",
    "    Initialize sentence embedding model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the embedding model to use.\n",
    "        device: Device ID (None for auto-detection).\n",
    "        \n",
    "    Returns:\n",
    "        SentenceTransformer: Initialized sentence transformer.\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If model loading fails.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "        \n",
    "    try:\n",
    "        embedder = SentenceTransformer(\n",
    "            model_name,\n",
    "            device='cuda' if device == 0 else 'cpu'\n",
    "        )\n",
    "        logger.info(f\"Embedder initialized: {model_name}\")\n",
    "        return embedder\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize embedder: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def prepare_sentences(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare sentences from articles for processing.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with articles containing 'sentences' column.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with individual sentences.\n",
    "    \"\"\"\n",
    "    # Add article ID if not present\n",
    "    if 'article_id' not in df.columns:\n",
    "        df['article_id'] = df.index\n",
    "    \n",
    "    # Explode sentences into individual rows\n",
    "    df_sentences = df.explode('sentences').rename(columns={'sentences': 'sentence_text'})\n",
    "    df_sentences = df_sentences[['article_id', 'date', 'sentence_text']].dropna(subset=['sentence_text'])\n",
    "    \n",
    "    logger.info(f\"Prepared {len(df_sentences):,} sentences from articles\")\n",
    "    \n",
    "    return df_sentences\n",
    "\n",
    "\n",
    "def filter_relevant_sentences(df_sentences: pd.DataFrame, \n",
    "                             embedder: SentenceTransformer,\n",
    "                             risk_factor_labels: List[str],\n",
    "                             similarity_threshold: float = 0.55) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pre-filter sentences using semantic similarity to risk factors.\n",
    "    \n",
    "    Args:\n",
    "        df_sentences: DataFrame with sentence data.\n",
    "        embedder: Initialized sentence transformer.\n",
    "        risk_factor_labels: List of risk factor labels.\n",
    "        similarity_threshold: Minimum similarity score for filtering.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with filtered relevant sentences.\n",
    "    \"\"\"\n",
    "    if df_sentences.empty:\n",
    "        return df_sentences\n",
    "    \n",
    "    sentences = df_sentences['sentence_text'].tolist()\n",
    "    \n",
    "    logger.info(f\"Pre-filtering {len(sentences):,} sentences...\")\n",
    "    logger.info(f\"Similarity threshold: {similarity_threshold}\")\n",
    "    \n",
    "    # Pre-compute risk factor embeddings\n",
    "    risk_factor_embeddings = embedder.encode(\n",
    "        risk_factor_labels,\n",
    "        convert_to_tensor=True\n",
    "    )\n",
    "    \n",
    "    # Encode sentences\n",
    "    sentence_embeddings = embedder.encode(\n",
    "        sentences,\n",
    "        convert_to_tensor=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Find semantically similar sentences to risk factors\n",
    "    hits = util.semantic_search(\n",
    "        sentence_embeddings,\n",
    "        risk_factor_embeddings,\n",
    "        top_k=1\n",
    "    )\n",
    "    \n",
    "    # Filter by similarity threshold\n",
    "    relevant_indices = [\n",
    "        i for i, hit_list in enumerate(hits)\n",
    "        if hit_list and hit_list[0]['score'] >= similarity_threshold\n",
    "    ]\n",
    "    \n",
    "    df_filtered = df_sentences.iloc[relevant_indices].copy()\n",
    "    \n",
    "    logger.info(f\"Reduced to {len(df_filtered):,} relevant sentences \"\n",
    "               f\"({len(df_filtered)/len(sentences)*100:.1f}% retention)\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def classify_sentences(df_sentences: pd.DataFrame,\n",
    "                      classifier: Pipeline,\n",
    "                      risk_factor_labels: List[str],\n",
    "                      confidence_threshold: float = 0.90,\n",
    "                      batch_size: int = 128) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Classify filtered sentences for risk factors.\n",
    "    \n",
    "    Args:\n",
    "        df_sentences: DataFrame with filtered sentences.\n",
    "        classifier: Initialized classification pipeline.\n",
    "        risk_factor_labels: List of risk factor labels.\n",
    "        confidence_threshold: Minimum confidence score for classification.\n",
    "        batch_size: Batch size for classification.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with risk factor classifications.\n",
    "    \"\"\"\n",
    "    if df_sentences.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    sentences = df_sentences['sentence_text'].tolist()\n",
    "    \n",
    "    logger.info(f\"Classifying {len(sentences):,} sentences...\")\n",
    "    logger.info(f\"Confidence threshold: {confidence_threshold}\")\n",
    "    \n",
    "    results_list = []\n",
    "    \n",
    "    # Run classification in batches\n",
    "    for i, result in tqdm(\n",
    "        enumerate(classifier(\n",
    "            sentences,\n",
    "            risk_factor_labels,\n",
    "            multi_label=True,\n",
    "            batch_size=batch_size\n",
    "        )),\n",
    "        total=len(sentences),\n",
    "        desc=\"Classifying sentences\"\n",
    "    ):\n",
    "        # Check each label's confidence\n",
    "        for label, score in zip(result['labels'], result['scores']):\n",
    "            if score >= confidence_threshold:\n",
    "                original_row = df_sentences.iloc[i]\n",
    "                results_list.append({\n",
    "                    'article_id': original_row['article_id'],\n",
    "                    'date': original_row['date'],\n",
    "                    'sentence_text': result['sequence'],\n",
    "                    'risk_factor': label,\n",
    "                    'confidence_score': score\n",
    "                })\n",
    "    \n",
    "    df_results = pd.DataFrame(results_list)\n",
    "    \n",
    "    logger.info(f\"Found {len(df_results):,} risk factor mentions\")\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "\n",
    "def refine_results(df_mentions: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Post-process results to keep only highest confidence mention per sentence.\n",
    "    \n",
    "    Args:\n",
    "        df_mentions: DataFrame with all risk mentions.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with refined risk mentions.\n",
    "    \"\"\"\n",
    "    if df_mentions.empty:\n",
    "        return df_mentions\n",
    "    \n",
    "    logger.info(\"Refining results to highest confidence per sentence...\")\n",
    "    \n",
    "    # Keep only the highest confidence label for each sentence\n",
    "    idx = df_mentions.groupby('sentence_text')['confidence_score'].idxmax()\n",
    "    df_refined = df_mentions.loc[idx].copy()\n",
    "    \n",
    "    # Sort by confidence score\n",
    "    df_refined = df_refined.sort_values('confidence_score', ascending=False)\n",
    "    \n",
    "    logger.info(f\"Refined from {len(df_mentions):,} to {len(df_refined):,} unique mentions\")\n",
    "    \n",
    "    return df_refined\n",
    "\n",
    "\n",
    "def save_results(df_results: pd.DataFrame, data_dir: str, use_sample: bool = True) -> Path:\n",
    "    \"\"\"\n",
    "    Save extraction results to CSV.\n",
    "    \n",
    "    Args:\n",
    "        df_results: DataFrame with risk factor mentions.\n",
    "        data_dir: Root directory for saving data.\n",
    "        use_sample: Whether this is sample data (affects filename).\n",
    "        \n",
    "    Returns:\n",
    "        Path: Path to saved file.\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If saving fails.\n",
    "    \"\"\"\n",
    "    models_dir = Path(data_dir) / '03_models'\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    filename = 'risk_mentions_SAMPLE.csv' if use_sample else 'risk_mentions_FULL.csv'\n",
    "    \n",
    "    output_path = models_dir / filename\n",
    "    \n",
    "    try:\n",
    "        df_results.to_csv(output_path, index=False)\n",
    "        logger.info(f\"Saved {len(df_results):,} risk mentions to {output_path}\")\n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save results: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def calculate_statistics(df_results: pd.DataFrame, \n",
    "                        total_articles: int,\n",
    "                        total_sentences: int,\n",
    "                        filtered_sentences: int,\n",
    "                        risk_mentions: int) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate statistics about the extraction.\n",
    "    \n",
    "    Args:\n",
    "        df_results: DataFrame with extraction results.\n",
    "        total_articles: Total number of articles processed.\n",
    "        total_sentences: Total number of sentences extracted.\n",
    "        filtered_sentences: Number of sentences after filtering.\n",
    "        risk_mentions: Number of risk mentions found.\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Dictionary of statistics.\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'total_articles': total_articles,\n",
    "        'total_sentences': total_sentences,\n",
    "        'filtered_sentences': filtered_sentences,\n",
    "        'risk_mentions': risk_mentions,\n",
    "        'unique_mentions': len(df_results)\n",
    "    }\n",
    "    \n",
    "    if not df_results.empty:\n",
    "        stats.update({\n",
    "            'unique_risk_factors': df_results['risk_factor'].nunique(),\n",
    "            'avg_confidence': df_results['confidence_score'].mean(),\n",
    "            'min_confidence': df_results['confidence_score'].min(),\n",
    "            'max_confidence': df_results['confidence_score'].max(),\n",
    "            'top_risk_factors': df_results['risk_factor'].value_counts().head(5).to_dict()\n",
    "        })\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "def display_statistics(stats: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Display extraction statistics.\n",
    "    \n",
    "    Args:\n",
    "        stats: Dictionary containing statistics.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EXTRACTION STATISTICS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total articles processed: {stats['total_articles']:,}\")\n",
    "    print(f\"Total sentences extracted: {stats['total_sentences']:,}\")\n",
    "    print(f\"Sentences after filtering: {stats['filtered_sentences']:,}\")\n",
    "    print(f\"Risk mentions found: {stats['risk_mentions']:,}\")\n",
    "    print(f\"Unique mentions (refined): {stats['unique_mentions']:,}\")\n",
    "    \n",
    "    if 'unique_risk_factors' in stats:\n",
    "        print(f\"\\nUnique risk factors: {stats['unique_risk_factors']}\")\n",
    "        print(f\"Average confidence: {stats['avg_confidence']:.3f}\")\n",
    "        print(f\"Confidence range: {stats['min_confidence']:.3f} - {stats['max_confidence']:.3f}\")\n",
    "        \n",
    "        if stats.get('top_risk_factors'):\n",
    "            print(\"\\nTop 5 risk factors:\")\n",
    "            for risk, count in stats['top_risk_factors'].items():\n",
    "                print(f\"  - {risk}: {count} mentions\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "def run_risk_extraction_pipeline(data_dir: str = '../data',\n",
    "                               classifier_model: str = 'MoritzLaurer/deberta-v3-xsmall-zeroshot-v1.1-all-33',\n",
    "                               embedder_model: str = 'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "                               classifier_batch_size: int = 128,\n",
    "                               sentence_similarity_threshold: float = 0.55,\n",
    "                               classifier_confidence_threshold: float = 0.90,\n",
    "                               use_sample: bool = True,\n",
    "                               sample_size: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Execute the complete risk factor extraction pipeline.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing data folders.\n",
    "        classifier_model: Name of the classification model to use.\n",
    "        embedder_model: Name of the embedding model to use.\n",
    "        classifier_batch_size: Batch size for classification.\n",
    "        sentence_similarity_threshold: Minimum similarity score for sentence filtering.\n",
    "        classifier_confidence_threshold: Minimum confidence score for classification.\n",
    "        use_sample: Whether to use a sample of articles for testing.\n",
    "        sample_size: Number of articles to sample if use_sample is True.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with refined risk factor mentions.\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If required files don't exist.\n",
    "        Exception: If processing fails.\n",
    "    \"\"\"\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"Starting risk factor extraction pipeline\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    \n",
    "    if use_sample:\n",
    "        logger.info(f\"Running in SAMPLE mode ({sample_size} articles)\")\n",
    "    else:\n",
    "        logger.info(\"Running in FULL mode (all articles)\")\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        df_articles = load_filtered_data(data_dir, use_sample, sample_size)\n",
    "        if df_articles.empty:\n",
    "            logger.warning(\"No articles to process after loading.\")\n",
    "            return pd.DataFrame()\n",
    "        total_articles = len(df_articles)\n",
    "        \n",
    "        # Load risk factors\n",
    "        risk_factor_labels = load_risk_factors(data_dir)\n",
    "        \n",
    "        # Initialize models\n",
    "        classifier = initialize_classifier(classifier_model)\n",
    "        embedder = initialize_embedder(embedder_model)\n",
    "        \n",
    "        # Prepare sentences\n",
    "        df_sentences = prepare_sentences(df_articles)\n",
    "        if df_sentences.empty:\n",
    "            logger.warning(\"No sentences to process after preparation.\")\n",
    "            return pd.DataFrame()\n",
    "        total_sentences = len(df_sentences)\n",
    "        \n",
    "        # Filter relevant sentences\n",
    "        df_filtered = filter_relevant_sentences(\n",
    "            df_sentences, embedder, risk_factor_labels, sentence_similarity_threshold\n",
    "        )\n",
    "        filtered_sentences = len(df_filtered)\n",
    "        \n",
    "        # Classify sentences\n",
    "        df_mentions = classify_sentences(\n",
    "            df_filtered, classifier, risk_factor_labels, \n",
    "            classifier_confidence_threshold, classifier_batch_size\n",
    "        )\n",
    "        risk_mentions = len(df_mentions)\n",
    "        \n",
    "        # Refine results\n",
    "        df_refined = refine_results(df_mentions)\n",
    "        \n",
    "        # Save results\n",
    "        save_results(df_refined, data_dir, use_sample)\n",
    "        \n",
    "        # Display statistics\n",
    "        stats = calculate_statistics(\n",
    "            df_refined, total_articles, total_sentences, \n",
    "            filtered_sentences, risk_mentions\n",
    "        )\n",
    "        display_statistics(stats)\n",
    "        \n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"Pipeline completed successfully!\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        return df_refined\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline failed: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# --- SCRIPT EXECUTION ---\n",
    "\n",
    "# Usage example - Sample mode (for testing)\n",
    "# This will now correctly load your Arabic data and process a small sample.\n",
    "# df_risk_mentions_sample = run_risk_extraction_pipeline(\n",
    "#     data_dir='../data',\n",
    "#     use_sample=True,\n",
    "#     sample_size=20, # Increased sample size a bit\n",
    "#     sentence_similarity_threshold=0.55,\n",
    "#     classifier_confidence_threshold=0.90,\n",
    "#     classifier_batch_size=128\n",
    "# )\n",
    "\n",
    "# # Display sample results if available\n",
    "# if not df_risk_mentions_sample.empty:\n",
    "#     print(\"\\n--- Sample Risk Mentions ---\")\n",
    "#     print(df_risk_mentions_sample[['risk_factor', 'confidence_score', 'sentence_text']].head())\n",
    "#     print(\"-\" * 28)\n",
    "\n",
    "# --- To run on the FULL dataset ---\n",
    "# Make sure you have enough time and memory, as this will process all articles.\n",
    "\n",
    "df_risk_mentions_full = run_risk_extraction_pipeline(\n",
    "    data_dir='../data',\n",
    "    use_sample=False,  # Set this to False to process all data\n",
    "    # The 'sample_size' parameter will be ignored now\n",
    "    sentence_similarity_threshold=0.7,\n",
    "    classifier_confidence_threshold=0.95,\n",
    "    classifier_batch_size=512 # You might increase this if you have a powerful GPU\n",
    ")\n",
    "\n",
    "# Display a few results from the full run\n",
    "if not df_risk_mentions_full.empty:\n",
    "    print(\"\\n--- Full Run: Top Risk Mentions ---\")\n",
    "    print(df_risk_mentions_full[['risk_factor', 'confidence_score', 'sentence_text']].head())\n",
    "    print(\"-\" * 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7961ebd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>date</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>risk_factor</th>\n",
       "      <th>confidence_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5562</th>\n",
       "      <td>19373</td>\n",
       "      <td>2024-07-16</td>\n",
       "      <td>Pirates: Camden Janik, C, Illinois 355.</td>\n",
       "      <td>pirates</td>\n",
       "      <td>0.999301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5560</th>\n",
       "      <td>19373</td>\n",
       "      <td>2024-07-16</td>\n",
       "      <td>Pirates: Matt Ager, P, UC Santa Barbara 175.</td>\n",
       "      <td>pirates</td>\n",
       "      <td>0.999278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9307</th>\n",
       "      <td>40062</td>\n",
       "      <td>2024-06-26</td>\n",
       "      <td>Pirates 9, Reds 4.</td>\n",
       "      <td>pirates</td>\n",
       "      <td>0.999264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9301</th>\n",
       "      <td>40062</td>\n",
       "      <td>2024-06-26</td>\n",
       "      <td>Pirates 8, Reds 3.</td>\n",
       "      <td>pirates</td>\n",
       "      <td>0.999263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10237</th>\n",
       "      <td>44982</td>\n",
       "      <td>2024-07-11</td>\n",
       "      <td>Please donate today.</td>\n",
       "      <td>call for donations</td>\n",
       "      <td>0.999262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17555</th>\n",
       "      <td>129705</td>\n",
       "      <td>2024-07-10</td>\n",
       "      <td>الكرك..</td>\n",
       "      <td>flee</td>\n",
       "      <td>0.950227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19057</th>\n",
       "      <td>161768</td>\n",
       "      <td>2024-06-24</td>\n",
       "      <td>وسجّلت اسعار الدولار ارتفعت مع افتتاح بورصتي ا...</td>\n",
       "      <td>human rights abuses</td>\n",
       "      <td>0.950180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17496</th>\n",
       "      <td>127560</td>\n",
       "      <td>2024-06-24</td>\n",
       "      <td>اقرأ أيضاً : تقرير..</td>\n",
       "      <td>rising inflation</td>\n",
       "      <td>0.950134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18266</th>\n",
       "      <td>143735</td>\n",
       "      <td>2024-07-22</td>\n",
       "      <td>ونقلت وكالة الأنباء العراقية واع عن جهاز الأمن...</td>\n",
       "      <td>repression</td>\n",
       "      <td>0.950088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17317</th>\n",
       "      <td>123911</td>\n",
       "      <td>2024-07-17</td>\n",
       "      <td>والثلاثاء، أعلنت حكومة إقليم كردستان في شمال ا...</td>\n",
       "      <td>repression</td>\n",
       "      <td>0.950088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2238 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       article_id        date  \\\n",
       "5562        19373  2024-07-16   \n",
       "5560        19373  2024-07-16   \n",
       "9307        40062  2024-06-26   \n",
       "9301        40062  2024-06-26   \n",
       "10237       44982  2024-07-11   \n",
       "...           ...         ...   \n",
       "17555      129705  2024-07-10   \n",
       "19057      161768  2024-06-24   \n",
       "17496      127560  2024-06-24   \n",
       "18266      143735  2024-07-22   \n",
       "17317      123911  2024-07-17   \n",
       "\n",
       "                                           sentence_text          risk_factor  \\\n",
       "5562             Pirates: Camden Janik, C, Illinois 355.              pirates   \n",
       "5560        Pirates: Matt Ager, P, UC Santa Barbara 175.              pirates   \n",
       "9307                                  Pirates 9, Reds 4.              pirates   \n",
       "9301                                  Pirates 8, Reds 3.              pirates   \n",
       "10237                               Please donate today.   call for donations   \n",
       "...                                                  ...                  ...   \n",
       "17555                                            الكرك..                 flee   \n",
       "19057  وسجّلت اسعار الدولار ارتفعت مع افتتاح بورصتي ا...  human rights abuses   \n",
       "17496                               اقرأ أيضاً : تقرير..     rising inflation   \n",
       "18266  ونقلت وكالة الأنباء العراقية واع عن جهاز الأمن...           repression   \n",
       "17317  والثلاثاء، أعلنت حكومة إقليم كردستان في شمال ا...           repression   \n",
       "\n",
       "       confidence_score  \n",
       "5562           0.999301  \n",
       "5560           0.999278  \n",
       "9307           0.999264  \n",
       "9301           0.999263  \n",
       "10237          0.999262  \n",
       "...                 ...  \n",
       "17555          0.950227  \n",
       "19057          0.950180  \n",
       "17496          0.950134  \n",
       "18266          0.950088  \n",
       "17317          0.950088  \n",
       "\n",
       "[2238 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_risk_mentions_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f5e0c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-14 10:40:05,301 - INFO - Starting risk mention geotagging pipeline\n",
      "2025-09-14 10:40:05,302 - INFO - Step 1: Loading data\n",
      "2025-09-14 10:40:05,324 - INFO - Sentence length filter: removed 866 sentences with < 4 words\n",
      "2025-09-14 10:40:05,325 - INFO - Remaining sentences: 1,372 (was 2,238)\n",
      "2025-09-14 10:40:05,325 - INFO - Loaded 1,372 quality risk mention sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-14 10:40:12,447 - INFO - Loaded 84,970 English and 77,147 Arabic articles\n",
      "2025-09-14 10:40:12,477 - INFO - Step 2: Building location resolvers\n",
      "2025-09-14 10:40:12,484 - INFO - Built location resolvers with 918 aliases\n",
      "2025-09-14 10:40:12,485 - INFO - Step 3: Initializing NER pipeline\n",
      "2025-09-14 10:40:12,490 - INFO - GPU available: NVIDIA L4\n",
      "2025-09-14 10:40:12,495 - INFO - Loading NER model: Babelscape/wikineural-multilingual-ner\n",
      "Device set to use cuda:0\n",
      "2025-09-14 10:40:13,245 - INFO - NER pipeline initialized successfully\n",
      "2025-09-14 10:40:13,246 - INFO - Step 4: Performing hybrid geotagging\n",
      "2025-09-14 10:40:13,259 - INFO - Processing 1,148 articles with risk mentions\n",
      "2025-09-14 10:40:13,259 - INFO - Extracting article-level locations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689b687823e54c52bb7a315ea0ad62ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving article locations:   0%|          | 0/1148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-14 10:40:56,411 - INFO - Found 1,244 total locations in 658 articles\n",
      "2025-09-14 10:40:56,412 - INFO - Extracting sentence-level locations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a037bfd6028d4bc9911d9f87ab0caa3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving sentence locations:   0%|          | 0/1372 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-14 10:41:07,118 - INFO - Found 64 total locations in 56 sentences\n",
      "2025-09-14 10:41:07,119 - INFO - Step 5: Merging and finalizing data\n",
      "2025-09-14 10:41:07,120 - INFO - Applying hierarchical logic and finalizing data...\n",
      "/tmp/ipykernel_3277/1337983948.py:375: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if pd.isna(locations):\n",
      "/tmp/ipykernel_3277/1337983948.py:375: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if pd.isna(locations):\n",
      "2025-09-14 10:41:07,195 - INFO - Created 1,050 risk-location pairs\n",
      "2025-09-14 10:41:07,197 - INFO - Step 6: Saving results\n",
      "2025-09-14 10:41:07,211 - INFO - Saved 1,050 geotagged risk mentions to: ../data/03_models/risk_mentions_geotagged_FINAL.csv\n",
      "2025-09-14 10:41:07,212 - INFO - Pipeline completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample Geotagged Risk Mentions ---\n",
      "Showing 5 of 1,050 total records:\n",
      "\n",
      "Risk: call for donations (confidence: 1.00)\n",
      "Location: iraq (iq)\n",
      "Sentence: Donations welcome during these hours....\n",
      "Language: english\n",
      "--------------------------------------------------\n",
      "Risk: refugees (confidence: 1.00)\n",
      "Location: occupied palestinian territory (ps)\n",
      "Sentence: The refugees deserve time, dignified treatment and a path to return....\n",
      "Language: english\n",
      "--------------------------------------------------\n",
      "Risk: refugees (confidence: 1.00)\n",
      "Location: lebanon (lb)\n",
      "Sentence: The refugees deserve time, dignified treatment and a path to return....\n",
      "Language: english\n",
      "--------------------------------------------------\n",
      "Risk: rising food prices (confidence: 1.00)\n",
      "Location: gaza (ps_gz_2)\n",
      "Sentence: Food prices rose by 1.5% last March, 1.8% in February, 3% in January, 2.2% in December 2023, 0.8% in...\n",
      "Language: english\n",
      "--------------------------------------------------\n",
      "Risk: pirates (confidence: 1.00)\n",
      "Location: gaza (ps_gz_2)\n",
      "Sentence: Pittsburgh Pirates: Adding and subtracting....\n",
      "Language: english\n",
      "--------------------------------------------------\n",
      "\n",
      "Processed 1,050 risk-location pairs\n",
      "\n",
      "Risk factor distribution:\n",
      "  conflict: 184\n",
      "  repression: 96\n",
      "  weather extremes: 49\n",
      "  human rights abuses: 48\n",
      "  humanitarian situation: 47\n",
      "\n",
      "Top locations mentioned:\n",
      "  gaza: 239\n",
      "  iraq: 122\n",
      "  syria: 85\n",
      "  lebanon: 80\n",
      "  occupied palestinian territory: 65\n",
      "\n",
      "Sample sentence texts (quality filtered):\n",
      "  1. Donations welcome during these hours.\n",
      "  2. The refugees deserve time, dignified treatment and a path to return.\n",
      "  3. The refugees deserve time, dignified treatment and a path to return.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Risk mention geotagging pipeline for multilingual news articles.\n",
    "Extracts and resolves geographic locations from risk-related sentences using NER,\n",
    "applying hierarchical logic to maximize precision.\n",
    "Functional implementation without classes.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple, Optional\n",
    "from transformers import pipeline, Pipeline\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_device() -> int:\n",
    "    \"\"\"\n",
    "    Determine the best available device for processing.\n",
    "    \n",
    "    Returns:\n",
    "        Device ID (0 for GPU, -1 for CPU)\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "        return 0\n",
    "    else:\n",
    "        logger.info(\"No GPU found, using CPU\")\n",
    "        return -1\n",
    "\n",
    "\n",
    "def filter_short_sentences(df: pd.DataFrame, min_words: int = 3, text_column: str = 'sentence_text') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter out sentences with fewer than minimum number of words.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing sentences\n",
    "        min_words: Minimum number of words required (default: 3)\n",
    "        text_column: Name of column containing sentence text\n",
    "        \n",
    "    Returns:\n",
    "        Filtered DataFrame with sentences having at least min_words\n",
    "    \"\"\"\n",
    "    initial_count = len(df)\n",
    "    \n",
    "    def count_words(text: str) -> int:\n",
    "        \"\"\"Count words in text, handling various edge cases.\"\"\"\n",
    "        if pd.isna(text) or not isinstance(text, str):\n",
    "            return 0\n",
    "        # Remove extra whitespace and split on whitespace\n",
    "        words = text.strip().split()\n",
    "        # Filter out empty strings and very short tokens (like single punctuation)\n",
    "        meaningful_words = [word for word in words if len(word.strip()) > 0]\n",
    "        return len(meaningful_words)\n",
    "    \n",
    "    # Calculate word counts\n",
    "    df = df.copy()\n",
    "    df['word_count'] = df[text_column].apply(count_words)\n",
    "    \n",
    "    # Filter sentences with sufficient words\n",
    "    df_filtered = df[df['word_count'] >= min_words].copy()\n",
    "    \n",
    "    # Remove the temporary word_count column\n",
    "    df_filtered = df_filtered.drop('word_count', axis=1)\n",
    "    \n",
    "    filtered_count = len(df_filtered)\n",
    "    removed_count = initial_count - filtered_count\n",
    "    \n",
    "    logger.info(f\"Sentence length filter: removed {removed_count:,} sentences with < {min_words} words\")\n",
    "    logger.info(f\"Remaining sentences: {filtered_count:,} (was {initial_count:,})\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def load_risk_mentions(data_dir: str, sample_size: Optional[int] = None, min_words: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load risk mention sentences from processed data and filter by length.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing data folders\n",
    "        sample_size: Number of rows to process (None for all)\n",
    "        min_words: Minimum number of words required per sentence\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing filtered risk mentions\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If risk mentions file doesn't exist\n",
    "    \"\"\"\n",
    "    models_dir = Path(data_dir) / '03_models'\n",
    "    risk_path = models_dir / 'risk_mentions_FULL.csv'\n",
    "    \n",
    "    try:\n",
    "        df_risk = pd.read_csv(risk_path)\n",
    "        \n",
    "        # Apply sentence length filter first\n",
    "        df_risk = filter_short_sentences(df_risk, min_words=min_words)\n",
    "        \n",
    "        # Apply sampling after filtering if specified\n",
    "        if sample_size:\n",
    "            df_risk = df_risk.head(sample_size).copy()\n",
    "            logger.info(f\"Sampling {sample_size} rows for processing after filtering\")\n",
    "        \n",
    "        logger.info(f\"Loaded {len(df_risk):,} quality risk mention sentences\")\n",
    "        return df_risk\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Risk mentions file not found: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_articles(data_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and combine English and Arabic article datasets.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing data folders\n",
    "        \n",
    "    Returns:\n",
    "        Combined DataFrame with article content\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If article files don't exist\n",
    "    \"\"\"\n",
    "    processed_dir = Path(data_dir) / '02_processed'\n",
    "    eng_path = processed_dir / 'news_eng_processed.pkl'\n",
    "    ara_path = processed_dir / 'news_ara_processed.pkl'\n",
    "    \n",
    "    try:\n",
    "        # Load datasets\n",
    "        df_eng = pd.read_pickle(eng_path)\n",
    "        df_eng['language'] = 'english'\n",
    "        \n",
    "        df_ara = pd.read_pickle(ara_path)\n",
    "        df_ara['language'] = 'arabic'\n",
    "        \n",
    "        # Ensure article IDs exist\n",
    "        if 'article_id' not in df_eng.columns:\n",
    "            df_eng['article_id'] = df_eng.index\n",
    "        if 'article_id' not in df_ara.columns:\n",
    "            df_ara['article_id'] = df_ara.index\n",
    "        \n",
    "        # Combine datasets\n",
    "        df_articles = pd.concat([df_eng, df_ara], ignore_index=True)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(df_eng):,} English and {len(df_ara):,} Arabic articles\")\n",
    "        return df_articles\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Article file not found: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def build_location_resolvers(data_dir: str) -> Tuple[Dict[str, str], Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Build lookup dictionaries for location name resolution.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing raw data\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (location_lookup, id_to_english_name) dictionaries\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If location files don't exist\n",
    "    \"\"\"\n",
    "    raw_dir = Path(data_dir) / '01_raw'\n",
    "    eng_path = raw_dir / 'id_english_location_name.pkl'\n",
    "    ara_path = raw_dir / 'id_arabic_location_name.pkl'\n",
    "    \n",
    "    try:\n",
    "        # Load location dictionaries\n",
    "        with open(eng_path, 'rb') as f:\n",
    "            eng_locations = pickle.load(f)\n",
    "        with open(ara_path, 'rb') as f:\n",
    "            ara_locations = pickle.load(f)\n",
    "        \n",
    "        # Build name-to-ID lookup\n",
    "        location_lookup = {}\n",
    "        for location_dict in [eng_locations, ara_locations]:\n",
    "            for loc_id, names in location_dict.items():\n",
    "                for name in names:\n",
    "                    location_lookup[name.lower()] = loc_id\n",
    "        \n",
    "        # Build ID-to-English name lookup\n",
    "        id_to_english_name = {\n",
    "            loc_id: names[0] \n",
    "            for loc_id, names in eng_locations.items()\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Built location resolvers with {len(location_lookup):,} aliases\")\n",
    "        \n",
    "        return location_lookup, id_to_english_name\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Location dictionary not found: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def initialize_ner_pipeline(model_name: str = \"Babelscape/wikineural-multilingual-ner\", \n",
    "                           device: Optional[int] = None) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Initialize the NER pipeline for entity extraction.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the NER model to use\n",
    "        device: Device ID (None for auto-detection)\n",
    "        \n",
    "    Returns:\n",
    "        Initialized NER pipeline\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If model loading fails\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "        \n",
    "    logger.info(f\"Loading NER model: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        ner_pipeline = pipeline(\n",
    "            \"ner\",\n",
    "            model=model_name,\n",
    "            aggregation_strategy=\"simple\",\n",
    "            device=device\n",
    "        )\n",
    "        logger.info(\"NER pipeline initialized successfully\")\n",
    "        return ner_pipeline\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load NER model: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def resolve_locations_from_entities(entities: List[Dict], \n",
    "                                   location_lookup: Dict[str, str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract and resolve location IDs from NER entities.\n",
    "    \n",
    "    Args:\n",
    "        entities: List of entity dictionaries from NER pipeline\n",
    "        location_lookup: Dictionary mapping location names to IDs\n",
    "        \n",
    "    Returns:\n",
    "        List of resolved location IDs\n",
    "    \"\"\"\n",
    "    found_ids = set()\n",
    "    \n",
    "    for entity in entities:\n",
    "        if entity.get('entity_group') == 'LOC':\n",
    "            loc_name_lower = entity.get('word', '').lower()\n",
    "            if loc_name_lower in location_lookup:\n",
    "                found_ids.add(location_lookup[loc_name_lower])\n",
    "    \n",
    "    return list(found_ids)\n",
    "\n",
    "\n",
    "def extract_article_locations(df_articles: pd.DataFrame,\n",
    "                             ner_pipeline: Pipeline,\n",
    "                             location_lookup: Dict[str, str],\n",
    "                             batch_size: int = 128) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract locations from article bodies using batch NER processing.\n",
    "    \n",
    "    Args:\n",
    "        df_articles: DataFrame containing article bodies\n",
    "        ner_pipeline: Initialized NER pipeline\n",
    "        location_lookup: Dictionary mapping location names to IDs\n",
    "        batch_size: Batch size for NER processing\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with article_locations column added\n",
    "    \"\"\"\n",
    "    logger.info(\"Extracting article-level locations...\")\n",
    "    \n",
    "    # Prepare texts for processing\n",
    "    article_bodies = df_articles['body'].fillna('').tolist()\n",
    "    \n",
    "    # Batch process with NER\n",
    "    article_entities = ner_pipeline(\n",
    "        article_bodies, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Resolve locations for each article\n",
    "    article_locations = [\n",
    "        resolve_locations_from_entities(entities, location_lookup) \n",
    "        for entities in tqdm(article_entities, desc=\"Resolving article locations\")\n",
    "    ]\n",
    "    \n",
    "    df_articles = df_articles.copy()\n",
    "    df_articles['article_locations'] = article_locations\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_locations = sum(len(locs) for locs in article_locations)\n",
    "    articles_with_locations = sum(1 for locs in article_locations if locs)\n",
    "    \n",
    "    logger.info(f\"Found {total_locations:,} total locations in \"\n",
    "               f\"{articles_with_locations:,} articles\")\n",
    "    \n",
    "    return df_articles\n",
    "\n",
    "\n",
    "def extract_sentence_locations(df_sentences: pd.DataFrame,\n",
    "                              ner_pipeline: Pipeline,\n",
    "                              location_lookup: Dict[str, str],\n",
    "                              batch_size: int = 128) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract locations from individual sentences using batch NER processing.\n",
    "    \n",
    "    Args:\n",
    "        df_sentences: DataFrame containing sentence texts\n",
    "        ner_pipeline: Initialized NER pipeline\n",
    "        location_lookup: Dictionary mapping location names to IDs\n",
    "        batch_size: Batch size for NER processing\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with sentence_locations column added\n",
    "    \"\"\"\n",
    "    logger.info(\"Extracting sentence-level locations...\")\n",
    "    \n",
    "    # Prepare texts for processing\n",
    "    sentence_texts = df_sentences['sentence_text'].fillna('').tolist()\n",
    "    \n",
    "    # Batch process with NER\n",
    "    sentence_entities = ner_pipeline(\n",
    "        sentence_texts,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Resolve locations for each sentence\n",
    "    sentence_locations = [\n",
    "        resolve_locations_from_entities(entities, location_lookup)\n",
    "        for entities in tqdm(sentence_entities, desc=\"Resolving sentence locations\")\n",
    "    ]\n",
    "    \n",
    "    df_sentences = df_sentences.copy()\n",
    "    df_sentences['sentence_locations'] = sentence_locations\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_locations = sum(len(locs) for locs in sentence_locations)\n",
    "    sentences_with_locations = sum(1 for locs in sentence_locations if locs)\n",
    "    \n",
    "    logger.info(f\"Found {total_locations:,} total locations in \"\n",
    "               f\"{sentences_with_locations:,} sentences\")\n",
    "    \n",
    "    return df_sentences\n",
    "\n",
    "\n",
    "def safe_normalize_locations(locations) -> List[str]:\n",
    "    \"\"\"\n",
    "    Safely normalize location data to list format, handling various data types and edge cases.\n",
    "    \n",
    "    Args:\n",
    "        locations: Location data in various formats (list, NaN, None, etc.)\n",
    "        \n",
    "    Returns:\n",
    "        List of location strings (empty list if no valid locations)\n",
    "    \"\"\"\n",
    "    # Handle None or various NaN types\n",
    "    if locations is None:\n",
    "        return []\n",
    "    \n",
    "    # Handle pandas NaN - check if it's a scalar and NaN\n",
    "    try:\n",
    "        if pd.isna(locations):\n",
    "            return []\n",
    "    except (ValueError, TypeError):\n",
    "        # If pd.isna() fails (e.g., on arrays), continue with other checks\n",
    "        pass\n",
    "    \n",
    "    # Handle lists and tuples\n",
    "    if isinstance(locations, (list, tuple)):\n",
    "        return [str(loc) for loc in locations if loc is not None and str(loc).strip()]\n",
    "    \n",
    "    # Handle single string values\n",
    "    if isinstance(locations, str) and locations.strip():\n",
    "        return [locations.strip()]\n",
    "    \n",
    "    # Default case - return empty list\n",
    "    return []\n",
    "\n",
    "\n",
    "def apply_hierarchical_logic(row: pd.Series) -> List[str]:\n",
    "    \"\"\"\n",
    "    Apply hierarchical location selection logic with robust null handling.\n",
    "    \n",
    "    Priority order:\n",
    "    1. Specific sentence locations (ID length > 2)\n",
    "    2. Any sentence locations\n",
    "    3. Specific article locations (ID length > 2)\n",
    "    4. Any article locations\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row with location columns\n",
    "        \n",
    "    Returns:\n",
    "        List of selected location IDs\n",
    "    \"\"\"\n",
    "    # Get locations safely using the robust helper function\n",
    "    sentence_locs = safe_normalize_locations(row.get('sentence_locations', []))\n",
    "    article_locs = safe_normalize_locations(row.get('article_locations', []))\n",
    "    \n",
    "    # Check for specific sentence locations (ID length > 2)\n",
    "    sentence_specific = [\n",
    "        loc for loc in sentence_locs \n",
    "        if isinstance(loc, str) and len(loc) > 2\n",
    "    ]\n",
    "    if sentence_specific:\n",
    "        return sentence_specific\n",
    "    \n",
    "    # Use any sentence locations\n",
    "    if sentence_locs:\n",
    "        return sentence_locs\n",
    "    \n",
    "    # Check for specific article locations (ID length > 2)\n",
    "    article_specific = [\n",
    "        loc for loc in article_locs \n",
    "        if isinstance(loc, str) and len(loc) > 2\n",
    "    ]\n",
    "    if article_specific:\n",
    "        return article_specific\n",
    "    \n",
    "    # Fall back to any article locations\n",
    "    if article_locs:\n",
    "        return article_locs\n",
    "    \n",
    "    # No locations found\n",
    "    return []\n",
    "\n",
    "\n",
    "def merge_and_finalize(df_risk: pd.DataFrame,\n",
    "                      df_articles: pd.DataFrame,\n",
    "                      id_to_english_name: Dict[str, str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge risk mentions with locations and apply hierarchical logic.\n",
    "    \n",
    "    Args:\n",
    "        df_risk: DataFrame with risk mentions and sentence locations\n",
    "        df_articles: DataFrame with article locations\n",
    "        id_to_english_name: Dictionary mapping location IDs to English names\n",
    "        \n",
    "    Returns:\n",
    "        Final DataFrame with one row per risk-location pair\n",
    "    \"\"\"\n",
    "    logger.info(\"Applying hierarchical logic and finalizing data...\")\n",
    "    \n",
    "    # Create copies to avoid modifying original dataframes\n",
    "    df_articles = df_articles.copy()\n",
    "    df_risk = df_risk.copy()\n",
    "    \n",
    "    # Ensure location columns exist and normalize them\n",
    "    if 'article_locations' not in df_articles.columns:\n",
    "        df_articles['article_locations'] = [[] for _ in range(len(df_articles))]\n",
    "    else:\n",
    "        df_articles['article_locations'] = df_articles['article_locations'].apply(safe_normalize_locations)\n",
    "    \n",
    "    if 'sentence_locations' not in df_risk.columns:\n",
    "        df_risk['sentence_locations'] = [[] for _ in range(len(df_risk))]\n",
    "    else:\n",
    "        df_risk['sentence_locations'] = df_risk['sentence_locations'].apply(safe_normalize_locations)\n",
    "    \n",
    "    # Merge sentence and article data\n",
    "    df_merged = pd.merge(\n",
    "        df_risk,\n",
    "        df_articles[['article_id', 'language', 'article_locations']],\n",
    "        on='article_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Normalize article locations after merge (in case of new NaN values)\n",
    "    df_merged['article_locations'] = df_merged['article_locations'].apply(safe_normalize_locations)\n",
    "    \n",
    "    # Apply hierarchical location selection\n",
    "    df_merged['final_locations'] = df_merged.apply(\n",
    "        apply_hierarchical_logic, \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Filter out rows with empty location lists before exploding\n",
    "    df_merged = df_merged[df_merged['final_locations'].apply(len) > 0]\n",
    "    \n",
    "    if df_merged.empty:\n",
    "        logger.warning(\"No locations found after applying hierarchical logic\")\n",
    "        return pd.DataFrame(columns=[\n",
    "            'article_id', 'date', 'language', 'sentence_text', \n",
    "            'risk_factor', 'confidence_score', 'location_id', \n",
    "            'location_name_english'\n",
    "        ])\n",
    "    \n",
    "    # Explode to one row per location\n",
    "    df_exploded = df_merged.explode('final_locations').rename(\n",
    "        columns={'final_locations': 'location_id'}\n",
    "    )\n",
    "    \n",
    "    # Remove rows without valid location IDs\n",
    "    df_exploded = df_exploded.dropna(subset=['location_id'])\n",
    "    df_exploded = df_exploded[df_exploded['location_id'] != '']\n",
    "    \n",
    "    if df_exploded.empty:\n",
    "        logger.warning(\"No valid locations found after exploding\")\n",
    "        return pd.DataFrame(columns=[\n",
    "            'article_id', 'date', 'language', 'sentence_text', \n",
    "            'risk_factor', 'confidence_score', 'location_id', \n",
    "            'location_name_english'\n",
    "        ])\n",
    "    \n",
    "    # Add English location names\n",
    "    df_exploded['location_name_english'] = df_exploded['location_id'].map(\n",
    "        id_to_english_name\n",
    "    )\n",
    "    \n",
    "    # Select and order final columns\n",
    "    final_columns = [\n",
    "        'article_id', 'date', 'language', 'sentence_text', \n",
    "        'risk_factor', 'confidence_score', 'location_id', \n",
    "        'location_name_english'\n",
    "    ]\n",
    "    \n",
    "    # Ensure all required columns exist\n",
    "    missing_cols = [col for col in final_columns if col not in df_exploded.columns]\n",
    "    if missing_cols:\n",
    "        logger.warning(f\"Missing columns: {missing_cols}\")\n",
    "        for col in missing_cols:\n",
    "            df_exploded[col] = None\n",
    "    \n",
    "    df_final = df_exploded[final_columns].copy()\n",
    "    \n",
    "    logger.info(f\"Created {len(df_final):,} risk-location pairs\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "\n",
    "def save_geotagging_results(df_final: pd.DataFrame, data_dir: str) -> Path:\n",
    "    \"\"\"\n",
    "    Save geotagged results to CSV file.\n",
    "    \n",
    "    Args:\n",
    "        df_final: Final processed DataFrame\n",
    "        data_dir: Root directory for saving data\n",
    "        \n",
    "    Returns:\n",
    "        Path to saved file\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If saving fails\n",
    "    \"\"\"\n",
    "    models_dir = Path(data_dir) / '03_models'\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_path = models_dir / 'risk_mentions_geotagged_FINAL.csv'\n",
    "    \n",
    "    try:\n",
    "        df_final.to_csv(output_path, index=False)\n",
    "        logger.info(f\"Saved {len(df_final):,} geotagged risk mentions to: {output_path}\")\n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving results: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def display_geotagging_sample(df: pd.DataFrame, n: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Display sample results for verification.\n",
    "    \n",
    "    Args:\n",
    "        df: Final DataFrame\n",
    "        n: Number of samples to display\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        logger.warning(\"No data to display\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n--- Sample Geotagged Risk Mentions ---\")\n",
    "    print(f\"Showing {min(n, len(df))} of {len(df):,} total records:\")\n",
    "    print()\n",
    "    \n",
    "    sample = df.head(n)\n",
    "    for idx, row in sample.iterrows():\n",
    "        print(f\"Risk: {row['risk_factor']} (confidence: {row['confidence_score']:.2f})\")\n",
    "        print(f\"Location: {row['location_name_english']} ({row['location_id']})\")\n",
    "        print(f\"Sentence: {row['sentence_text'][:100]}...\")\n",
    "        print(f\"Language: {row['language']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "\n",
    "def run_geotagging_pipeline(data_dir: str = '../data',\n",
    "                           sample_size: Optional[int] = None,\n",
    "                           batch_size: int = 128,\n",
    "                           ner_model: str = \"Babelscape/wikineural-multilingual-ner\",\n",
    "                           min_words: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Execute the complete geotagging pipeline with sentence length filtering.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing data folders\n",
    "        sample_size: Number of rows to process (None for all)\n",
    "        batch_size: Batch size for NER processing\n",
    "        ner_model: Model name for NER pipeline\n",
    "        min_words: Minimum number of words required per sentence\n",
    "        \n",
    "    Returns:\n",
    "        Final geotagged DataFrame\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If required files don't exist\n",
    "        Exception: If processing fails\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting risk mention geotagging pipeline\")\n",
    "    \n",
    "    if sample_size:\n",
    "        logger.info(f\"Running on SAMPLE of {sample_size} rows\")\n",
    "    \n",
    "    # Step 1: Load all necessary data (with sentence filtering)\n",
    "    logger.info(\"Step 1: Loading data\")\n",
    "    df_risk = load_risk_mentions(data_dir, sample_size, min_words)\n",
    "    df_articles = load_articles(data_dir)\n",
    "    \n",
    "    # Step 2: Build location resolvers\n",
    "    logger.info(\"Step 2: Building location resolvers\")\n",
    "    location_lookup, id_to_english_name = build_location_resolvers(data_dir)\n",
    "    \n",
    "    # Step 3: Initialize NER pipeline\n",
    "    logger.info(\"Step 3: Initializing NER pipeline\")\n",
    "    ner_pipeline = initialize_ner_pipeline(ner_model)\n",
    "    \n",
    "    # Step 4: Extract locations (hybrid geotagging)\n",
    "    logger.info(\"Step 4: Performing hybrid geotagging\")\n",
    "    \n",
    "    # Filter articles to those with risk mentions\n",
    "    risk_article_ids = df_risk['article_id'].unique()\n",
    "    df_articles_filtered = df_articles[\n",
    "        df_articles['article_id'].isin(risk_article_ids)\n",
    "    ][['article_id', 'body', 'language']].copy()\n",
    "    \n",
    "    logger.info(f\"Processing {len(df_articles_filtered):,} articles with risk mentions\")\n",
    "    \n",
    "    # Extract article-level locations\n",
    "    df_articles_filtered = extract_article_locations(\n",
    "        df_articles_filtered, ner_pipeline, location_lookup, batch_size\n",
    "    )\n",
    "    \n",
    "    # Extract sentence-level locations\n",
    "    df_risk = extract_sentence_locations(\n",
    "        df_risk, ner_pipeline, location_lookup, batch_size\n",
    "    )\n",
    "    \n",
    "    # Step 5: Merge and apply hierarchical logic\n",
    "    logger.info(\"Step 5: Merging and finalizing data\")\n",
    "    df_final = merge_and_finalize(df_risk, df_articles_filtered, id_to_english_name)\n",
    "    \n",
    "    # Display sample for verification\n",
    "    display_geotagging_sample(df_final)\n",
    "    \n",
    "    # Step 6: Save results\n",
    "    logger.info(\"Step 6: Saving results\")\n",
    "    save_geotagging_results(df_final, data_dir)\n",
    "    \n",
    "    logger.info(\"Pipeline completed successfully\")\n",
    "    return df_final\n",
    "\n",
    "\n",
    "# Usage examples - now with sentence length filtering\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: Test with 100 quality rows, filtering sentences < 3 words\n",
    "    # df_geotagged = run_geotagging_pipeline(\n",
    "    #     data_dir='../data',\n",
    "    #     sample_size=100,\n",
    "    #     batch_size=512,\n",
    "    #     ner_model=\"Babelscape/wikineural-multilingual-ner\",\n",
    "    #     min_words=3  # Filter out sentences with fewer than 3 words\n",
    "    # )\n",
    "    \n",
    "    # Example 2: Process more rows with stricter filtering (4+ words)\n",
    "    # df_geotagged = run_geotagging_pipeline(\n",
    "    #     data_dir='../data',\n",
    "    #     sample_size=100,\n",
    "    #     batch_size=512,\n",
    "    #     min_words=4  # Even stricter filtering\n",
    "    # )\n",
    "    \n",
    "    # Example 3: Process entire FULL dataset with filtering\n",
    "    df_geotagged = run_geotagging_pipeline(\n",
    "        data_dir='../data',\n",
    "        sample_size=None,\n",
    "        batch_size=512,\n",
    "        min_words=4\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\nProcessed {len(df_geotagged):,} risk-location pairs\")\n",
    "\n",
    "    # Analyze distribution\n",
    "    if not df_geotagged.empty:\n",
    "        risk_distribution = df_geotagged['risk_factor'].value_counts()\n",
    "        print(f\"\\nRisk factor distribution:\")\n",
    "        for risk, count in risk_distribution.head().items():\n",
    "            print(f\"  {risk}: {count:,}\")\n",
    "\n",
    "        location_distribution = df_geotagged['location_name_english'].value_counts()\n",
    "        print(f\"\\nTop locations mentioned:\")\n",
    "        for location, count in location_distribution.head().items():\n",
    "            print(f\"  {location}: {count:,}\")\n",
    "    \n",
    "    # Display sample of sentence texts to verify quality\n",
    "    if not df_geotagged.empty:\n",
    "        print(f\"\\nSample sentence texts (quality filtered):\")\n",
    "        for i, text in enumerate(df_geotagged['sentence_text'].head(3)):\n",
    "            print(f\"  {i+1}. {text[:80]}{'...' if len(text) > 80 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a9a32ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>risk_factor</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>location_id</th>\n",
       "      <th>location_name_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16055</td>\n",
       "      <td>2024-07-15</td>\n",
       "      <td>english</td>\n",
       "      <td>Donations welcome during these hours.</td>\n",
       "      <td>call for donations</td>\n",
       "      <td>0.999252</td>\n",
       "      <td>iq</td>\n",
       "      <td>iraq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7788</td>\n",
       "      <td>2024-06-26</td>\n",
       "      <td>english</td>\n",
       "      <td>The refugees deserve time, dignified treatment...</td>\n",
       "      <td>refugees</td>\n",
       "      <td>0.999208</td>\n",
       "      <td>ps</td>\n",
       "      <td>occupied palestinian territory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7788</td>\n",
       "      <td>2024-06-26</td>\n",
       "      <td>english</td>\n",
       "      <td>The refugees deserve time, dignified treatment...</td>\n",
       "      <td>refugees</td>\n",
       "      <td>0.999208</td>\n",
       "      <td>lb</td>\n",
       "      <td>lebanon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8568</td>\n",
       "      <td>2024-07-08</td>\n",
       "      <td>english</td>\n",
       "      <td>Food prices rose by 1.5% last March, 1.8% in F...</td>\n",
       "      <td>rising food prices</td>\n",
       "      <td>0.999191</td>\n",
       "      <td>ps_gz_2</td>\n",
       "      <td>gaza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>35639</td>\n",
       "      <td>2024-07-23</td>\n",
       "      <td>english</td>\n",
       "      <td>Pittsburgh Pirates: Adding and subtracting.</td>\n",
       "      <td>pirates</td>\n",
       "      <td>0.999161</td>\n",
       "      <td>ps_gz_2</td>\n",
       "      <td>gaza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>161768</td>\n",
       "      <td>2024-06-24</td>\n",
       "      <td>arabic</td>\n",
       "      <td>وسجّلت اسعار الدولار ارتفعت مع افتتاح بورصتي ا...</td>\n",
       "      <td>human rights abuses</td>\n",
       "      <td>0.950180</td>\n",
       "      <td>ps_wb_1</td>\n",
       "      <td>hebron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>127560</td>\n",
       "      <td>2024-06-24</td>\n",
       "      <td>arabic</td>\n",
       "      <td>اقرأ أيضاً : تقرير..</td>\n",
       "      <td>rising inflation</td>\n",
       "      <td>0.950134</td>\n",
       "      <td>ps</td>\n",
       "      <td>occupied palestinian territory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>127560</td>\n",
       "      <td>2024-06-24</td>\n",
       "      <td>arabic</td>\n",
       "      <td>اقرأ أيضاً : تقرير..</td>\n",
       "      <td>rising inflation</td>\n",
       "      <td>0.950134</td>\n",
       "      <td>jo</td>\n",
       "      <td>jordan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>143735</td>\n",
       "      <td>2024-07-22</td>\n",
       "      <td>arabic</td>\n",
       "      <td>ونقلت وكالة الأنباء العراقية واع عن جهاز الأمن...</td>\n",
       "      <td>repression</td>\n",
       "      <td>0.950088</td>\n",
       "      <td>jo_am_3</td>\n",
       "      <td>amman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>123911</td>\n",
       "      <td>2024-07-17</td>\n",
       "      <td>arabic</td>\n",
       "      <td>والثلاثاء، أعلنت حكومة إقليم كردستان في شمال ا...</td>\n",
       "      <td>repression</td>\n",
       "      <td>0.950088</td>\n",
       "      <td>iq_sl_5</td>\n",
       "      <td>kalar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1050 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      article_id        date language  \\\n",
       "7          16055  2024-07-15  english   \n",
       "9           7788  2024-06-26  english   \n",
       "9           7788  2024-06-26  english   \n",
       "11          8568  2024-07-08  english   \n",
       "16         35639  2024-07-23  english   \n",
       "...          ...         ...      ...   \n",
       "1368      161768  2024-06-24   arabic   \n",
       "1369      127560  2024-06-24   arabic   \n",
       "1369      127560  2024-06-24   arabic   \n",
       "1370      143735  2024-07-22   arabic   \n",
       "1371      123911  2024-07-17   arabic   \n",
       "\n",
       "                                          sentence_text          risk_factor  \\\n",
       "7                 Donations welcome during these hours.   call for donations   \n",
       "9     The refugees deserve time, dignified treatment...             refugees   \n",
       "9     The refugees deserve time, dignified treatment...             refugees   \n",
       "11    Food prices rose by 1.5% last March, 1.8% in F...   rising food prices   \n",
       "16          Pittsburgh Pirates: Adding and subtracting.              pirates   \n",
       "...                                                 ...                  ...   \n",
       "1368  وسجّلت اسعار الدولار ارتفعت مع افتتاح بورصتي ا...  human rights abuses   \n",
       "1369                               اقرأ أيضاً : تقرير..     rising inflation   \n",
       "1369                               اقرأ أيضاً : تقرير..     rising inflation   \n",
       "1370  ونقلت وكالة الأنباء العراقية واع عن جهاز الأمن...           repression   \n",
       "1371  والثلاثاء، أعلنت حكومة إقليم كردستان في شمال ا...           repression   \n",
       "\n",
       "      confidence_score location_id           location_name_english  \n",
       "7             0.999252          iq                            iraq  \n",
       "9             0.999208          ps  occupied palestinian territory  \n",
       "9             0.999208          lb                         lebanon  \n",
       "11            0.999191     ps_gz_2                            gaza  \n",
       "16            0.999161     ps_gz_2                            gaza  \n",
       "...                ...         ...                             ...  \n",
       "1368          0.950180     ps_wb_1                          hebron  \n",
       "1369          0.950134          ps  occupied palestinian territory  \n",
       "1369          0.950134          jo                          jordan  \n",
       "1370          0.950088     jo_am_3                           amman  \n",
       "1371          0.950088     iq_sl_5                           kalar  \n",
       "\n",
       "[1050 rows x 8 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_geotagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "495c075d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 7: Enhanced Visualization ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_cri' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 309\u001b[0m\n\u001b[1;32m    306\u001b[0m location_to_plot \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaghdad\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# Create single timeline plot\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m success \u001b[38;5;241m=\u001b[39m plot_composite_risk_timeline(\u001b[43mdf_cri\u001b[49m, location_to_plot)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully created timeline plot for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation_to_plot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_cri' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Visualization functions for risk index analysis.\n",
    "Clean, reusable functions for plotting composite risk indices.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def plot_composite_risk_timeline(df_cri: pd.DataFrame,\n",
    "                                location_name: str,\n",
    "                                figsize: Tuple[int, int] = (15, 7),\n",
    "                                save_path: Optional[str] = None,\n",
    "                                show_plot: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Create a timeline plot for composite risk index of a specific location.\n",
    "    \n",
    "    Args:\n",
    "        df_cri: DataFrame containing composite risk index data\n",
    "        location_name: Name of the location to plot\n",
    "        figsize: Figure size as (width, height)\n",
    "        save_path: Path to save the plot (optional)\n",
    "        show_plot: Whether to display the plot\n",
    "        \n",
    "    Returns:\n",
    "        True if plot was created successfully, False otherwise\n",
    "    \"\"\"\n",
    "    # Filter data for the specified location\n",
    "    df_plot = df_cri[df_cri['location_name_english'] == location_name].copy()\n",
    "    \n",
    "    if df_plot.empty:\n",
    "        logger.warning(f\"No data found for location: {location_name}\")\n",
    "        available_locations = df_cri['location_name_english'].unique()[:10]\n",
    "        logger.info(f\"Available locations (sample): {list(available_locations)}\")\n",
    "        return False\n",
    "    \n",
    "    # Sort by date for proper timeline\n",
    "    df_plot = df_plot.sort_values('date')\n",
    "    \n",
    "    logger.info(f\"Creating timeline plot for {location_name} ({len(df_plot)} data points)\")\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Plot the timeline\n",
    "    ax.plot(df_plot['date'], df_plot['composite_risk_index'], \n",
    "           marker='o', linestyle='-', linewidth=2, markersize=4,\n",
    "           color='#d62728', alpha=0.8, label='Composite Risk Index')\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_title(f\"Daily Composite Risk Index for {location_name}\", \n",
    "                fontsize=16, fontweight='bold')\n",
    "    ax.set_ylabel(\"Risk Index (Normalized Score)\", fontsize=12)\n",
    "    ax.set_xlabel(\"Date\", fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Format dates on x-axis\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add summary statistics as text\n",
    "    mean_risk = df_plot['composite_risk_index'].mean()\n",
    "    max_risk = df_plot['composite_risk_index'].max()\n",
    "    min_risk = df_plot['composite_risk_index'].min()\n",
    "    \n",
    "    stats_text = f\"Mean: {mean_risk:.6f}\\nMax: {max_risk:.6f}\\nMin: {min_risk:.6f}\"\n",
    "    ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,\n",
    "           verticalalignment='top', fontsize=10,\n",
    "           bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        logger.info(f\"Plot saved to: {save_path}\")\n",
    "    \n",
    "    # Show if requested\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def plot_multiple_locations_comparison(df_cri: pd.DataFrame,\n",
    "                                     location_names: List[str],\n",
    "                                     figsize: Tuple[int, int] = (15, 8),\n",
    "                                     save_path: Optional[str] = None,\n",
    "                                     show_plot: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Compare composite risk indices across multiple locations on one plot.\n",
    "    \n",
    "    Args:\n",
    "        df_cri: DataFrame containing composite risk index data\n",
    "        location_names: List of location names to compare\n",
    "        figsize: Figure size as (width, height)\n",
    "        save_path: Path to save the plot (optional)\n",
    "        show_plot: Whether to display the plot\n",
    "        \n",
    "    Returns:\n",
    "        True if plot was created successfully, False otherwise\n",
    "    \"\"\"\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(location_names)))\n",
    "    plotted_locations = []\n",
    "    \n",
    "    for i, location in enumerate(location_names):\n",
    "        df_location = df_cri[df_cri['location_name_english'] == location].copy()\n",
    "        \n",
    "        if df_location.empty:\n",
    "            logger.warning(f\"No data found for location: {location}\")\n",
    "            continue\n",
    "        \n",
    "        df_location = df_location.sort_values('date')\n",
    "        \n",
    "        ax.plot(df_location['date'], df_location['composite_risk_index'],\n",
    "               marker='o', linestyle='-', linewidth=2, markersize=3,\n",
    "               color=colors[i], alpha=0.8, label=location)\n",
    "        \n",
    "        plotted_locations.append(location)\n",
    "    \n",
    "    if not plotted_locations:\n",
    "        logger.error(\"No valid locations found for plotting\")\n",
    "        return False\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_title(\"Composite Risk Index Comparison Across Locations\", \n",
    "                fontsize=16, fontweight='bold')\n",
    "    ax.set_ylabel(\"Risk Index (Normalized Score)\", fontsize=12)\n",
    "    ax.set_xlabel(\"Date\", fontsize=12)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        logger.info(f\"Comparison plot saved to: {save_path}\")\n",
    "    \n",
    "    # Show if requested\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    \n",
    "    logger.info(f\"Created comparison plot for {len(plotted_locations)} locations\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def plot_top_risk_locations_summary(df_cri: pd.DataFrame,\n",
    "                                   top_n: int = 10,\n",
    "                                   figsize: Tuple[int, int] = (12, 8),\n",
    "                                   save_path: Optional[str] = None,\n",
    "                                   show_plot: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Create a bar plot showing top N locations by average composite risk index.\n",
    "    \n",
    "    Args:\n",
    "        df_cri: DataFrame containing composite risk index data\n",
    "        top_n: Number of top locations to display\n",
    "        figsize: Figure size as (width, height)\n",
    "        save_path: Path to save the plot (optional)\n",
    "        show_plot: Whether to display the plot\n",
    "        \n",
    "    Returns:\n",
    "        True if plot was created successfully, False otherwise\n",
    "    \"\"\"\n",
    "    if df_cri.empty:\n",
    "        logger.error(\"Empty DataFrame provided\")\n",
    "        return False\n",
    "    \n",
    "    # Calculate average risk per location\n",
    "    location_avg_risk = df_cri.groupby('location_name_english')['composite_risk_index'].agg([\n",
    "        'mean', 'count', 'std'\n",
    "    ]).reset_index()\n",
    "    \n",
    "    # Sort by mean risk and get top N\n",
    "    top_locations = location_avg_risk.nlargest(top_n, 'mean')\n",
    "    \n",
    "    logger.info(f\"Creating summary plot for top {len(top_locations)} locations\")\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    bars = ax.barh(range(len(top_locations)), top_locations['mean'].values,\n",
    "                   color='#d62728', alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_yticks(range(len(top_locations)))\n",
    "    ax.set_yticklabels(top_locations['location_name_english'].values, fontsize=11)\n",
    "    ax.set_xlabel('Average Composite Risk Index', fontsize=12)\n",
    "    ax.set_title(f'Top {top_n} Locations by Average Composite Risk Index', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, value, count) in enumerate(zip(bars, top_locations['mean'], top_locations['count'])):\n",
    "        width = bar.get_width()\n",
    "        ax.text(width + width * 0.01, bar.get_y() + bar.get_height()/2,\n",
    "               f'{value:.6f}\\n({count} days)', ha='left', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        logger.info(f\"Summary plot saved to: {save_path}\")\n",
    "    \n",
    "    # Show if requested\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def create_risk_visualization_suite(df_cri: pd.DataFrame,\n",
    "                                  primary_location: str = 'Baghdad',\n",
    "                                  comparison_locations: Optional[List[str]] = None,\n",
    "                                  output_dir: Optional[str] = None,\n",
    "                                  show_plots: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Create a complete suite of risk visualization plots.\n",
    "    \n",
    "    Args:\n",
    "        df_cri: DataFrame containing composite risk index data\n",
    "        primary_location: Main location for detailed timeline\n",
    "        comparison_locations: List of locations for comparison plot\n",
    "        output_dir: Directory to save plots (optional)\n",
    "        show_plots: Whether to display plots\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating complete risk visualization suite\")\n",
    "    \n",
    "    # Set default comparison locations if not provided\n",
    "    if comparison_locations is None:\n",
    "        # Get top 5 locations by average risk (excluding primary if it's already top)\n",
    "        top_locations = df_cri.groupby('location_name_english')['composite_risk_index'].mean().nlargest(5)\n",
    "        comparison_locations = [loc for loc in top_locations.index if loc != primary_location][:4]\n",
    "        if primary_location not in top_locations.index:\n",
    "            comparison_locations = [primary_location] + comparison_locations[:3]\n",
    "        else:\n",
    "            comparison_locations = [primary_location] + comparison_locations\n",
    "    \n",
    "    # Create output directory if saving\n",
    "    if output_dir:\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Primary location timeline\n",
    "    timeline_path = f\"{output_dir}/{primary_location.replace(' ', '_')}_timeline.png\" if output_dir else None\n",
    "    plot_composite_risk_timeline(df_cri, primary_location, save_path=timeline_path, show_plot=show_plots)\n",
    "    \n",
    "    # 2. Multiple locations comparison\n",
    "    comparison_path = f\"{output_dir}/locations_comparison.png\" if output_dir else None\n",
    "    plot_multiple_locations_comparison(df_cri, comparison_locations, \n",
    "                                     save_path=comparison_path, show_plot=show_plots)\n",
    "    \n",
    "    # 3. Top locations summary\n",
    "    summary_path = f\"{output_dir}/top_locations_summary.png\" if output_dir else None\n",
    "    plot_top_risk_locations_summary(df_cri, save_path=summary_path, show_plot=show_plots)\n",
    "    \n",
    "    logger.info(\"Risk visualization suite completed\")\n",
    "\n",
    "\n",
    "# Usage examples (replace with your actual data):\n",
    "\n",
    "# --- Example 1: Basic single location plot ---\n",
    "# plot_composite_risk_timeline(df_cri, 'Baghdad')\n",
    "\n",
    "# --- Example 2: Compare multiple locations ---\n",
    "# locations_to_compare = ['Baghdad', 'Damascus', 'Beirut', 'Cairo']\n",
    "# plot_multiple_locations_comparison(df_cri, locations_to_compare)\n",
    "\n",
    "# --- Example 3: Top locations summary ---\n",
    "# plot_top_risk_locations_summary(df_cri, top_n=15)\n",
    "\n",
    "# --- Example 4: Complete visualization suite ---\n",
    "# create_risk_visualization_suite(\n",
    "#     df_cri, \n",
    "#     primary_location='Baghdad',\n",
    "#     comparison_locations=['Damascus', 'Beirut', 'Cairo', 'Aleppo'],\n",
    "#     output_dir='../data/05_reporting/risk_plots',\n",
    "#     show_plots=True\n",
    "# )\n",
    "\n",
    "# --- Replace the original Step 7 code with this: ---\n",
    "print(\"--- Step 7: Enhanced Visualization ---\")\n",
    "location_to_plot = 'Baghdad'\n",
    "\n",
    "# Create single timeline plot\n",
    "success = plot_composite_risk_timeline(df_cri, location_to_plot)\n",
    "\n",
    "if success:\n",
    "    print(f\"Successfully created timeline plot for {location_to_plot}\")\n",
    "else:\n",
    "    print(f\"Could not create plot for {location_to_plot}\")\n",
    "    \n",
    "    # Show available locations\n",
    "    available_locations = df_cri['location_name_english'].value_counts().head(10)\n",
    "    print(\"Available locations with most data points:\")\n",
    "    for location, count in available_locations.items():\n",
    "        print(f\"  {location}: {count} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f304389f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualitative",
   "metadata": {},
   "source": [
    "# Part 2: Reflection\n",
    "\n",
    "Please outline (1) some of the limitations of your approach and (2) how you would tackle these if you had more time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49236dab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbcfd532",
   "metadata": {},
   "source": [
    "i napisze na poczatku ze to jest moj pomysl a ponizej w ramach prototyupu przetestujemy rozwiazanie z gotowymi modelami. \n",
    "\n",
    "tutaj mysle ze mozna by to zastapic tak naprawde dwoma customowymi modelami\n",
    "\n",
    "custom trained model ktory klasyfikuje czy zdanie posiada informacje zwiazane z food insecurity.\n",
    "modelem klasyfikacyjnym ktory przyjmowalby jako sentence jako input i klasyfikowalby czy jest to ktorys z tych 167 risk factorow lub tez nie :) \n",
    "\n",
    "napisz tez ze to ze wzgledu na ograniczona moc obliczeniowa \n",
    "\n",
    "    sentence_similarity_threshold=0.7,\n",
    "    classifier_confidence_threshold=0.95,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
