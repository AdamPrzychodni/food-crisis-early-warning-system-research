{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ce3648",
   "metadata": {},
   "source": [
    "# [Take-home Assessment] Food Crisis Early Warning \n",
    "\n",
    "Welcome to the assessment. You will showcase your modeling and research skills by investigating news articles (in English and Arabic) as well as a set of food insecurity risk factors. \n",
    "\n",
    "We suggest planning to spend **~6–8 hours** on this assessment. **Please submit your response by Monday, September 15th, 9:00 AM EST via email to dime_ai@worldbank.org**. Please document your code with comments and explanations of design choices. There is one question on reflecting upon your approach at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cf2966",
   "metadata": {},
   "source": [
    "**Name:** Adam Przychodni\n",
    "\n",
    "**Email:** adam.przychodni@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09329a02",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "# Part 1: Technical Assessment\n",
    "\n",
    "\n",
    "## Task:\n",
    "\n",
    "We invite you to approach the challenge of understanding (and potentially predicting) food insecurity using the provided (limited) data. Your response should demonstrate how you tackle open-ended problems in data-scarce environments.\n",
    "\n",
    "Some example questions to consider:\n",
    "- What is the added value of geospatial data?\n",
    "- How can we address the lack of ground-truth information on food insecurity levels?\n",
    "- What are the benefits and challenges of working with multilingual data?\n",
    "- ...\n",
    "\n",
    "These are just guiding examples — you are free to explore any relevant angles to this topic/data.\n",
    "\n",
    "**Note:** There is no single \"right\" approach. Instead, we want to understand how you approach and structure open-ended problems in data-scarce environments. Given the large number of applicants, we will preselect the most impressive and complete submissions. Please take effort in structuring your response, as selection will depend on its depth and originality.\n",
    "\n",
    "\n",
    "## Provided Data:\n",
    "\n",
    "1. **Risk Factors:** A file containing 167 risk factors (unigrams, bigrams, and trigrams) in the `english_keywords` column and an empty `keywords_arabic` column. A separate file with the mapping of English risk factors to pre-defined thematic cluster assignments.\n",
    "\n",
    "\n",
    "2. **News Articles:** Two files containing one month of news articles from the Mashriq region:\n",
    "   - `news-articles-eng.csv`\n",
    "   - `news-articles-ara.csv`\n",
    "   - **Note:** You may work on a sample subset during development.\n",
    "   \n",
    "   \n",
    "3. **Geographic Taxonomy:** A file containing the names of the countries, provinces, and districts for the subset of Mashriq countries that is covered by the news articles. The files are a dictionary mapping from a key to the geographic name.\n",
    "   - `id_arabic_location_name.pkl`\n",
    "   - `id_english_location_name.pkl`\n",
    "   - **Note:** Each unique country/province/district is assigned a key (e.g. `iq`,`iq_bg` and `iq_bg_1` for country Iraq, province Baghdad, and district 1 in Baghdad respectively).\n",
    "   - The key of country names is a two character abbreviation as follows.\n",
    "       - 'iq': 'Iraq'\n",
    "       - 'jo': 'Jordan'\n",
    "       - 'lb': 'Lebanon'\n",
    "       - 'ps': 'Palestine'\n",
    "       - 'sy': 'Syria'\n",
    "       \n",
    "   - The key of provinces is a two-character abbreviation of the country followed by two-character abbreviation of the province **`{country_abbreviation}_{province_abbreviation}`**, and the key of districts is **`{country_abbreviation}_{province_abbreviation}_{unique_number}`**.\n",
    "       \n",
    "\n",
    "\n",
    "## Submission Guidelines:\n",
    "\n",
    "- **Code:** Follow best coding practices and ensure clear documentation. All notebook cells should be executed with outputs saved, and the notebook should run correctly on its own. Name your file **`solution_{FIRSTNAME}_{LASTNAME}.ipynb`**. If your solution relies on additional open-access data, either include it in your submission (e.g., as part of a ZIP file) or provide clear data-loading code/instructions as part of the nottebook. \n",
    "- **Report:** Submit a separate markdown file communicating your approach to this research problem. We expect you to detail the models, methods, or (additional) data you are using.\n",
    "\n",
    "Good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f9934a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task1",
   "metadata": {},
   "source": [
    "## Your Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b241d89f",
   "metadata": {},
   "source": [
    "parts of code generated and also formatted using LLMs, Gemini 2.5 Pro and Claude Opus 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd3ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 15:21:58,853 - INFO - ============================================================\n",
      "2025-09-13 15:21:58,854 - INFO - Starting news article processing pipeline\n",
      "2025-09-13 15:21:58,854 - INFO - ============================================================\n",
      "2025-09-13 15:21:58,855 - INFO - Geographic filtering: ENABLED\n",
      "2025-09-13 15:21:58,856 - INFO -   - Pre-filtering: ENABLED\n",
      "2025-09-13 15:21:58,856 - INFO -   - NER filtering: DISABLED\n",
      "2025-09-13 15:21:58,857 - INFO -   - Using all keywords: True\n",
      "2025-09-13 15:21:58,857 - INFO -   - Keywords sample size: 50\n",
      "2025-09-13 15:21:58,858 - INFO - Loading news articles from CSV files...\n",
      "2025-09-13 15:22:14,999 - INFO - Loaded 86,660 English and 85,511 Arabic articles\n",
      "2025-09-13 15:22:15,044 - INFO - Loaded 918 unique location aliases\n",
      "2025-09-13 15:22:15,046 - INFO - Using all 918 location keywords for pre-filtering\n",
      "2025-09-13 15:22:15,067 - INFO - Starting pre-filtering...\n",
      "2025-09-13 15:32:21,843 - INFO - Pre-filtering complete: 162117/172171 articles contain location keywords (94.2%)\n",
      "2025-09-13 15:32:21,862 - INFO - NER filtering disabled.\n",
      "2025-09-13 15:32:22,311 - INFO - Final counts: 84,970 English, 77,147 Arabic articles\n",
      "2025-09-13 15:32:22,355 - INFO - Processing English articles...\n",
      "2025-09-13 15:32:47,469 - INFO - Cleaned 84,970 English articles\n",
      "2025-09-13 15:32:59,383 - INFO - Tokenized English articles: avg 35.2 sentences per article\n",
      "2025-09-13 15:32:59,386 - INFO - Processing Arabic articles...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample Tokenization Results ---\n",
      "Article split into 124 sentences\n",
      "First 3 sentences:\n",
      "  1. Hussam al-Mahmoud | Yamen Moghrabi | Hassan Ibrahim On October 7, 2023, the world and the Middle East awoke to the drums of war beating in the Gaza Strip.\n",
      "  2. Over time, it turned into a reality that American efforts, Qatari and Egyptian mediation, condemnations, statements, summits, and conferences could not stop.\n",
      "  3. While Israel continues its war in the besieged Gaza Strip, attention is turning towards the potential outbreak of another war.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 15:33:12,567 - INFO - Cleaned 77,147 Arabic articles\n",
      "2025-09-13 15:33:17,889 - INFO - Tokenized Arabic articles: avg 11.8 sentences per article\n",
      "2025-09-13 15:33:28,650 - INFO - Saved English data to: ../data/02_processed/news_eng_processed.pkl\n",
      "2025-09-13 15:33:28,651 - INFO - Saved Arabic data to: ../data/02_processed/news_ara_processed.pkl\n",
      "2025-09-13 15:33:28,652 - INFO - ============================================================\n",
      "2025-09-13 15:33:28,653 - INFO - Pipeline completed successfully!\n",
      "2025-09-13 15:33:28,653 - INFO - ============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Strategy 4 - Pre-filtering Only:\n",
      "English articles: 84,970\n",
      "Arabic articles: 77,147\n",
      "\n",
      "Memory cleaned up!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "News article processing pipeline for multilingual text data.\n",
    "Balanced version with improved pre-filtering strategy.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Set, Tuple\n",
    "from transformers import pipeline, Pipeline\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Regex patterns as module constants\n",
    "HTML_PATTERN = re.compile(r'<.*?>')\n",
    "URL_PATTERN = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "WHITESPACE_PATTERN = re.compile(r'\\s+')\n",
    "SENTENCE_SPLIT_PATTERN = re.compile(r'(?<=[.!?۔])\\s+')\n",
    "LETTER_PATTERN = re.compile(r'[a-zA-Zء-ي]')\n",
    "\n",
    "\n",
    "def get_device() -> int:\n",
    "    \"\"\"\n",
    "    Determine the best available device for processing.\n",
    "    \n",
    "    Returns:\n",
    "        int: Device ID (0 for GPU, -1 for CPU).\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "        return 0\n",
    "    else:\n",
    "        logger.info(\"No GPU found, using CPU\")\n",
    "        return -1\n",
    "\n",
    "\n",
    "def load_location_lookup(data_dir: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Load location dictionaries for geographic filtering.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing raw data.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary mapping location names to IDs.\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If location files don't exist.\n",
    "    \"\"\"\n",
    "    raw_dir = Path(data_dir) / '01_raw'\n",
    "    eng_path = raw_dir / 'id_english_location_name.pkl'\n",
    "    ara_path = raw_dir / 'id_arabic_location_name.pkl'\n",
    "    \n",
    "    location_lookup = {}\n",
    "    \n",
    "    try:\n",
    "        with open(eng_path, 'rb') as f:\n",
    "            eng_locations = pickle.load(f)\n",
    "        with open(ara_path, 'rb') as f:\n",
    "            ara_locations = pickle.load(f)\n",
    "        \n",
    "        # Build lookup dictionary\n",
    "        for location_dict in [eng_locations, ara_locations]:\n",
    "            for loc_id, names in location_dict.items():\n",
    "                for name in names:\n",
    "                    location_lookup[name.lower()] = loc_id\n",
    "        \n",
    "        logger.info(f\"Loaded {len(location_lookup):,} unique location aliases\")\n",
    "        return location_lookup\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Location file not found: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def prefilter_by_keywords(df: pd.DataFrame, location_lookup: Dict[str, str], \n",
    "                         use_all_keywords: bool = False, sample_size: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Balanced keyword-based pre-filtering before expensive NER.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with articles.\n",
    "        location_lookup: Dictionary of location names.\n",
    "        use_all_keywords: If True, use all keywords. If False, use sample_size.\n",
    "        sample_size: Number of most common locations to use if not using all.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Pre-filtered dataframe.\n",
    "    \"\"\"\n",
    "    location_keywords = list(location_lookup.keys())\n",
    "    \n",
    "    if not use_all_keywords and len(location_keywords) > sample_size:\n",
    "        # Prioritize important locations (countries and major cities)\n",
    "        # Sort by length - shorter names are often more important\n",
    "        location_keywords = sorted(location_keywords, key=len)[:sample_size]\n",
    "        logger.info(f\"Using top {sample_size} location keywords for pre-filtering\")\n",
    "    else:\n",
    "        logger.info(f\"Using all {len(location_keywords)} location keywords for pre-filtering\")\n",
    "    \n",
    "    # Create more flexible regex pattern\n",
    "    # Use word boundaries for whole word matching\n",
    "    escaped_keywords = [re.escape(loc) for loc in location_keywords]\n",
    "    # Create pattern in chunks to avoid regex size limits\n",
    "    chunk_size = 100\n",
    "    patterns = []\n",
    "    \n",
    "    for i in range(0, len(escaped_keywords), chunk_size):\n",
    "        chunk = escaped_keywords[i:i+chunk_size]\n",
    "        pattern = r'\\b(' + '|'.join(chunk) + r')\\b'\n",
    "        patterns.append(re.compile(pattern, re.IGNORECASE))\n",
    "    \n",
    "    def contains_location(text):\n",
    "        if not isinstance(text, str):\n",
    "            return False\n",
    "        # Check against all pattern chunks\n",
    "        for pattern in patterns:\n",
    "            if pattern.search(text):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    # Apply pre-filter\n",
    "    logger.info(\"Starting pre-filtering...\")\n",
    "    mask = df['body'].apply(contains_location)\n",
    "    df_prefiltered = df[mask].copy()\n",
    "    \n",
    "    logger.info(f\"Pre-filtering complete: {len(df_prefiltered)}/{len(df)} articles \"\n",
    "               f\"contain location keywords ({len(df_prefiltered)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # If pre-filtering is too aggressive, warn\n",
    "    if len(df_prefiltered) < len(df) * 0.01:  # Less than 1%\n",
    "        logger.warning(\"Pre-filtering may be too aggressive! Consider adjusting parameters.\")\n",
    "    \n",
    "    return df_prefiltered\n",
    "\n",
    "\n",
    "def initialize_ner_pipeline(model_name: str = \"Babelscape/wikineural-multilingual-ner\", \n",
    "                           device: Optional[int] = None) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Initialize NER pipeline for geographic filtering.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the NER model to use.\n",
    "        device: Device ID (None for auto-detection).\n",
    "        \n",
    "    Returns:\n",
    "        Pipeline: Initialized NER pipeline.\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If model loading fails.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "        \n",
    "    logger.info(f\"Loading NER model: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        ner_pipeline = pipeline(\n",
    "            \"ner\",\n",
    "            model=model_name,\n",
    "            aggregation_strategy=\"simple\",\n",
    "            device=device\n",
    "        )\n",
    "        logger.info(\"NER pipeline initialized successfully\")\n",
    "        return ner_pipeline\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load NER model: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def filter_by_geography(df: pd.DataFrame, location_lookup: Dict[str, str], \n",
    "                       ner_pipeline: Pipeline, max_text_length: int = 1000,\n",
    "                       batch_size: int = 256, chunk_size: int = 5000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter articles containing target geographic locations.\n",
    "    \n",
    "    Args:\n",
    "        df: Combined dataframe of articles.\n",
    "        location_lookup: Dictionary mapping location names to IDs.\n",
    "        ner_pipeline: Initialized NER pipeline.\n",
    "        max_text_length: Maximum text length for NER processing.\n",
    "        batch_size: Batch size for NER processing.\n",
    "        chunk_size: Size of chunks for progress tracking.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered dataframe with location metadata.\n",
    "    \"\"\"\n",
    "    # Convert lookup keys to a set for O(1) lookups\n",
    "    location_set = set(location_lookup.keys())\n",
    "    \n",
    "    # Prepare texts for NER\n",
    "    article_bodies = df['body'].fillna('').tolist()\n",
    "    texts_to_process = [text[:max_text_length] for text in article_bodies]\n",
    "    \n",
    "    total_articles = len(texts_to_process)\n",
    "    logger.info(f\"Running NER on {total_articles:,} articles...\")\n",
    "    logger.info(f\"Batch size: {batch_size}, Chunk size: {chunk_size}\")\n",
    "    logger.info(f\"Max text length: {max_text_length} chars\")\n",
    "    \n",
    "    # Process in chunks to show progress and manage memory\n",
    "    all_relevance = []\n",
    "    all_locations = []\n",
    "    \n",
    "    with tqdm(total=total_articles, desc=\"Processing articles\") as pbar:\n",
    "        for i in range(0, total_articles, chunk_size):\n",
    "            chunk_end = min(i + chunk_size, total_articles)\n",
    "            chunk_texts = texts_to_process[i:chunk_end]\n",
    "            \n",
    "            # Extract entities for this chunk\n",
    "            try:\n",
    "                chunk_entities = ner_pipeline(chunk_texts, batch_size=batch_size)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error processing chunk {i//chunk_size + 1}: {e}\")\n",
    "                # If batch fails, try smaller batches\n",
    "                logger.info(\"Retrying with smaller batch size...\")\n",
    "                chunk_entities = ner_pipeline(chunk_texts, batch_size=batch_size//2)\n",
    "            \n",
    "            # Process results\n",
    "            for article_entities in chunk_entities:\n",
    "                found_locations = []\n",
    "                \n",
    "                # Use set operations for faster checking\n",
    "                for entity in article_entities:\n",
    "                    if entity.get('entity_group') == 'LOC':\n",
    "                        word_lower = entity.get('word', '').lower()\n",
    "                        if word_lower in location_set:\n",
    "                            found_locations.append(entity['word'])\n",
    "                \n",
    "                all_relevance.append(len(found_locations) > 0)\n",
    "                all_locations.append(found_locations)\n",
    "            \n",
    "            # Update progress\n",
    "            pbar.update(chunk_end - i)\n",
    "            \n",
    "            # Clear GPU cache periodically to prevent OOM\n",
    "            if i % (chunk_size * 5) == 0 and i > 0:\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                logger.info(f\"Cleared memory at article {i}\")\n",
    "    \n",
    "    # Add metadata and filter\n",
    "    df['matched_locations'] = all_locations\n",
    "    df_filtered = df[all_relevance].copy()\n",
    "    \n",
    "    # Log statistics\n",
    "    logger.info(f\"Geographic filtering complete:\")\n",
    "    logger.info(f\"  - Original articles: {len(df):,}\")\n",
    "    logger.info(f\"  - Relevant articles: {len(df_filtered):,}\")\n",
    "    logger.info(f\"  - Retention rate: {len(df_filtered)/len(df)*100:.1f}%\")\n",
    "    \n",
    "    # Calculate location frequency\n",
    "    if len(df_filtered) > 0:\n",
    "        all_matched_locs = [loc for locs in df_filtered['matched_locations'] for loc in locs]\n",
    "        if all_matched_locs:\n",
    "            from collections import Counter\n",
    "            loc_freq = Counter(all_matched_locs)\n",
    "            logger.info(f\"  - Top 5 locations: {loc_freq.most_common(5)}\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def load_news_data(data_dir: str, enable_geo_filter: bool = True,\n",
    "                  max_text_length: int = 1000, batch_size: int = 256,\n",
    "                  ner_model: str = \"Babelscape/wikineural-multilingual-ner\",\n",
    "                  use_prefilter: bool = True, run_ner_filter: bool = True,\n",
    "                  use_all_keywords: bool = False,\n",
    "                  prefilter_sample: int = 100) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load English and Arabic news datasets with optional geographic filtering.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing raw data.\n",
    "        enable_geo_filter: Whether to apply geographic filtering.\n",
    "        max_text_length: Maximum text length for NER processing.\n",
    "        batch_size: Batch size for NER processing.\n",
    "        ner_model: Model name for NER pipeline.\n",
    "        use_prefilter: Whether to use keyword pre-filtering.\n",
    "        run_ner_filter: Whether to run the NER model after pre-filtering.\n",
    "        use_all_keywords: Whether to use all location keywords.\n",
    "        prefilter_sample: Number of keywords to use if not using all.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: Tuple of (english_df, arabic_df).\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If dataset files don't exist.\n",
    "    \"\"\"\n",
    "    raw_dir = Path(data_dir) / '01_raw'\n",
    "    eng_path = raw_dir / 'news-articles-eng.csv'\n",
    "    ara_path = raw_dir / 'news-articles-ara.csv'\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"Loading news articles from CSV files...\")\n",
    "        df_eng = pd.read_csv(eng_path)\n",
    "        df_ara = pd.read_csv(ara_path)\n",
    "        \n",
    "        # Add language column for tracking\n",
    "        df_eng['language'] = 'english'\n",
    "        df_ara['language'] = 'arabic'\n",
    "        \n",
    "        logger.info(f\"Loaded {len(df_eng):,} English and {len(df_ara):,} Arabic articles\")\n",
    "        \n",
    "        # Combine for geographic filtering\n",
    "        df_combined = pd.concat([df_eng, df_ara], ignore_index=True)\n",
    "        \n",
    "        # Apply geographic filtering if enabled\n",
    "        if enable_geo_filter:\n",
    "            location_lookup = load_location_lookup(data_dir)\n",
    "            \n",
    "            # Apply keyword pre-filtering if enabled\n",
    "            if use_prefilter:\n",
    "                df_combined = prefilter_by_keywords(\n",
    "                    df_combined, location_lookup, use_all_keywords, prefilter_sample\n",
    "                )\n",
    "                if df_combined.empty:\n",
    "                    logger.warning(\"Pre-filtering removed all articles! No data to process further.\")\n",
    "                    return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "            # Apply NER-based filtering if enabled\n",
    "            if run_ner_filter:\n",
    "                if not df_combined.empty:\n",
    "                    ner_pipeline = initialize_ner_pipeline(ner_model)\n",
    "                    df_combined = filter_by_geography(\n",
    "                        df_combined, location_lookup, ner_pipeline, \n",
    "                        max_text_length, batch_size, chunk_size=5000\n",
    "                    )\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                else:\n",
    "                    logger.warning(\"Skipping NER as no articles remain after pre-filtering.\")\n",
    "            else:\n",
    "                logger.info(\"NER filtering disabled.\")\n",
    "                # Ensure 'matched_locations' column exists for consistency\n",
    "                df_combined['matched_locations'] = [[] for _ in range(len(df_combined))]\n",
    "            \n",
    "            # Handle edge case where both filters are off but geo_filter is on\n",
    "            if not use_prefilter and not run_ner_filter:\n",
    "                logger.warning(\"Geo-filtering enabled, but both pre-filter and NER are off. All articles will be processed.\")\n",
    "\n",
    "        else:\n",
    "            logger.info(\"Geographic filtering disabled, using all articles\")\n",
    "            df_combined['matched_locations'] = [[] for _ in range(len(df_combined))]\n",
    "        \n",
    "        # Split back into language-specific dataframes\n",
    "        df_eng_filtered = df_combined[df_combined['language'] == 'english'].copy()\n",
    "        df_ara_filtered = df_combined[df_combined['language'] == 'arabic'].copy()\n",
    "        \n",
    "        # Remove the language column\n",
    "        df_eng_filtered = df_eng_filtered.drop('language', axis=1)\n",
    "        df_ara_filtered = df_ara_filtered.drop('language', axis=1)\n",
    "        \n",
    "        logger.info(f\"Final counts: {len(df_eng_filtered):,} English, \"\n",
    "                   f\"{len(df_ara_filtered):,} Arabic articles\")\n",
    "        \n",
    "        return df_eng_filtered, df_ara_filtered\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Dataset file not found: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def clean_text(text: Optional[str]) -> str:\n",
    "    \"\"\"\n",
    "    Clean raw text by removing HTML tags, URLs, and normalizing whitespace.\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text string to clean.\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text string.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = HTML_PATTERN.sub('', text)\n",
    "    # Remove URLs\n",
    "    text = URL_PATTERN.sub('', text)\n",
    "    # Normalize whitespace\n",
    "    text = WHITESPACE_PATTERN.sub(' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize_sentences(text: Optional[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences using regex and filter non-textual results.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to tokenize.\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of sentence strings.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return []\n",
    "    \n",
    "    # Split on sentence-ending punctuation\n",
    "    sentences = SENTENCE_SPLIT_PATTERN.split(text)\n",
    "    \n",
    "    # Filter empty strings and non-textual content\n",
    "    valid_sentences = [\n",
    "        s for s in sentences \n",
    "        if s and LETTER_PATTERN.search(s)\n",
    "    ]\n",
    "    \n",
    "    return valid_sentences\n",
    "\n",
    "\n",
    "def process_dataframe(df: pd.DataFrame, language: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply cleaning and tokenization to a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'body' column containing article text.\n",
    "        language: Language identifier for logging.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with cleaned text and sentences.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Processing {language} articles...\")\n",
    "    \n",
    "    # Clean text\n",
    "    df['body_cleaned'] = df['body'].apply(clean_text)\n",
    "    logger.info(f\"Cleaned {len(df):,} {language} articles\")\n",
    "    \n",
    "    # Tokenize into sentences\n",
    "    df['sentences'] = df['body_cleaned'].apply(tokenize_sentences)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    sentence_counts = df['sentences'].apply(len)\n",
    "    if len(sentence_counts) > 0:\n",
    "        logger.info(f\"Tokenized {language} articles: \"\n",
    "                   f\"avg {sentence_counts.mean():.1f} sentences per article\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def save_processed_data(df_eng: pd.DataFrame, df_ara: pd.DataFrame, data_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Save processed dataframes to pickle files.\n",
    "    \n",
    "    Args:\n",
    "        df_eng: Processed English dataframe.\n",
    "        df_ara: Processed Arabic dataframe.\n",
    "        data_dir: Root directory for saving data.\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If saving fails.\n",
    "    \"\"\"\n",
    "    processed_dir = Path(data_dir) / '02_processed'\n",
    "    processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    eng_path = processed_dir / 'news_eng_processed.pkl'\n",
    "    ara_path = processed_dir / 'news_ara_processed.pkl'\n",
    "    \n",
    "    try:\n",
    "        df_eng.to_pickle(eng_path)\n",
    "        df_ara.to_pickle(ara_path)\n",
    "        logger.info(f\"Saved English data to: {eng_path}\")\n",
    "        logger.info(f\"Saved Arabic data to: {ara_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving processed data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def display_sample(df: pd.DataFrame, num_sentences: int = 3) -> None:\n",
    "    \"\"\"\n",
    "    Display sample processed sentences for verification.\n",
    "    \n",
    "    Args:\n",
    "        df: Processed dataframe.\n",
    "        num_sentences: Number of sentences to display.\n",
    "    \"\"\"\n",
    "    if df.empty or 'sentences' not in df.columns:\n",
    "        logger.warning(\"No sentences to display\")\n",
    "        return\n",
    "    \n",
    "    first_article = df.iloc[0]\n",
    "    sentences = first_article['sentences']\n",
    "    \n",
    "    print(f\"\\n--- Sample Tokenization Results ---\")\n",
    "    print(f\"Article split into {len(sentences)} sentences\")\n",
    "    print(f\"First {min(num_sentences, len(sentences))} sentences:\")\n",
    "    \n",
    "    for i, sentence in enumerate(sentences[:num_sentences], 1):\n",
    "        print(f\"  {i}. {sentence}\")\n",
    "    \n",
    "    # Show matched locations if available\n",
    "    if 'matched_locations' in df.columns and first_article['matched_locations']:\n",
    "        print(f\"\\nMatched locations: {', '.join(first_article['matched_locations'])}\")\n",
    "\n",
    "\n",
    "def run_news_processing_pipeline(data_dir: str = '../data', \n",
    "                                enable_geo_filter: bool = True,\n",
    "                                use_prefilter: bool = True,\n",
    "                                run_ner_filter: bool = True,\n",
    "                                max_text_length: int = 1000,\n",
    "                                batch_size: int = 256,\n",
    "                                ner_model: str = \"Babelscape/wikineural-multilingual-ner\",\n",
    "                                use_all_keywords: bool = False,\n",
    "                                prefilter_sample: int = 50) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Execute the complete news processing pipeline with balanced filtering.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing raw and processed data folders.\n",
    "        enable_geo_filter: Whether to apply geographic filtering.\n",
    "        use_prefilter: Whether to use keyword pre-filtering.\n",
    "        run_ner_filter: Whether to run the NER model for filtering.\n",
    "        max_text_length: Maximum text length for NER processing.\n",
    "        batch_size: Batch size for NER processing.\n",
    "        ner_model: Model name for NER pipeline.\n",
    "        use_all_keywords: Whether to use all location keywords.\n",
    "        prefilter_sample: Number of keywords to use if not using all.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: Tuple of processed (english_df, arabic_df).\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If required files don't exist.\n",
    "        Exception: If processing fails.\n",
    "    \"\"\"\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"Starting news article processing pipeline\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    \n",
    "    if enable_geo_filter:\n",
    "        logger.info(\"Geographic filtering: ENABLED\")\n",
    "        logger.info(f\"  - Pre-filtering: {'ENABLED' if use_prefilter else 'DISABLED'}\")\n",
    "        logger.info(f\"  - NER filtering: {'ENABLED' if run_ner_filter else 'DISABLED'}\")\n",
    "        if use_prefilter:\n",
    "            logger.info(f\"  - Using all keywords: {use_all_keywords}\")\n",
    "            logger.info(f\"  - Keywords sample size: {prefilter_sample}\")\n",
    "        if run_ner_filter:\n",
    "            logger.info(f\"  - Max text length: {max_text_length} chars\")\n",
    "            logger.info(f\"  - Batch size: {batch_size}\")\n",
    "    else:\n",
    "        logger.info(\"Geographic filtering: DISABLED\")\n",
    "    \n",
    "    # Load data (with geographic filtering if enabled)\n",
    "    df_eng, df_ara = load_news_data(\n",
    "        data_dir=data_dir, \n",
    "        enable_geo_filter=enable_geo_filter, \n",
    "        max_text_length=max_text_length, \n",
    "        batch_size=batch_size, \n",
    "        ner_model=ner_model, \n",
    "        use_prefilter=use_prefilter, \n",
    "        run_ner_filter=run_ner_filter,\n",
    "        use_all_keywords=use_all_keywords, \n",
    "        prefilter_sample=prefilter_sample\n",
    "    )\n",
    "    \n",
    "    # Check if we have data to process\n",
    "    if df_eng.empty and df_ara.empty:\n",
    "        logger.warning(\"No articles to process after filtering!\")\n",
    "        return df_eng, df_ara\n",
    "    \n",
    "    # Process each dataset\n",
    "    if not df_eng.empty:\n",
    "        df_eng = process_dataframe(df_eng, \"English\")\n",
    "        display_sample(df_eng)\n",
    "    \n",
    "    if not df_ara.empty:\n",
    "        df_ara = process_dataframe(df_ara, \"Arabic\")\n",
    "    \n",
    "    # Save processed data\n",
    "    save_processed_data(df_eng, df_ara, data_dir)\n",
    "    \n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"Pipeline completed successfully!\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    \n",
    "    return df_eng, df_ara\n",
    "\n",
    "# ============================================================\n",
    "# STRATEGY 1: NO FILTERING (Process all articles)\n",
    "# ============================================================\n",
    "# Use this to get ALL articles for comprehensive analysis\n",
    "# df_eng_all, df_ara_all = run_news_processing_pipeline(\n",
    "#     data_dir='../data',\n",
    "#     enable_geo_filter=False  # Disable geographic filtering entirely\n",
    "# )\n",
    "\n",
    "# print(f\"\\nStrategy 1 - No Filtering:\")\n",
    "# print(f\"English articles: {len(df_eng_all):,}\")\n",
    "# print(f\"Arabic articles: {len(df_ara_all):,}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STRATEGY 2: BALANCED FILTERING (Recommended: Pre-filter + NER)\n",
    "# ============================================================\n",
    "# Use focused keywords for pre-filtering, then apply NER\n",
    "# df_eng_balanced, df_ara_balanced = run_news_processing_pipeline(\n",
    "#     data_dir='../data',\n",
    "#     enable_geo_filter=True,\n",
    "#     use_prefilter=True,\n",
    "#     run_ner_filter=True,      # Ensure NER is also on\n",
    "#     use_all_keywords=True,  \n",
    "#     # prefilter_sample=50,    # Focus on 50 most important locations\n",
    "#     max_text_length=1500,     # Look at more text\n",
    "#     batch_size=512            # Balanced batch size\n",
    "# )\n",
    "\n",
    "# print(f\"\\nStrategy 2 - Balanced Filtering (Prefilter + NER):\")\n",
    "# print(f\"English articles: {len(df_eng_balanced):,}\")\n",
    "# print(f\"Arabic articles: {len(df_ara_balanced):,}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STRATEGY 3: NER-ONLY FILTERING (More inclusive, slower)\n",
    "# ============================================================\n",
    "# Skip pre-filtering but still use NER\n",
    "# df_eng_relaxed, df_ara_relaxed = run_news_processing_pipeline(\n",
    "#     data_dir='../data',\n",
    "#     enable_geo_filter=True,\n",
    "#     use_prefilter=False,      # Skip pre-filtering\n",
    "#     run_ner_filter=True,      # Run NER\n",
    "#     max_text_length=1000,     # Reduced for speed\n",
    "#     batch_size=512            # Larger batches for efficiency\n",
    "# )\n",
    "\n",
    "# print(f\"\\nStrategy 3 - NER-Only Filtering:\")\n",
    "# print(f\"English articles: {len(df_eng_relaxed):,}\")\n",
    "# print(f\"Arabic articles: {len(df_ara_relaxed):,}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STRATEGY 4: PRE-FILTERING ONLY (Fastest geographic filtering)\n",
    "# ============================================================\n",
    "# Use this for a quick, less precise filtering based on keywords.\n",
    "df_eng_pre, df_ara_pre = run_news_processing_pipeline(\n",
    "    data_dir='../data',\n",
    "    enable_geo_filter=True,\n",
    "    use_prefilter=True,\n",
    "    run_ner_filter=False,     # The key change: disable NER\n",
    "    use_all_keywords=True,    # Use all available location keywords\n",
    ")\n",
    "\n",
    "print(f\"\\nStrategy 4 - Pre-filtering Only:\")\n",
    "print(f\"English articles: {len(df_eng_pre):,}\")\n",
    "print(f\"Arabic articles: {len(df_ara_pre):,}\")\n",
    "\n",
    "# ============================================================\n",
    "# MEMORY CLEANUP\n",
    "# ============================================================\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"\\nMemory cleaned up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c276ddd3",
   "metadata": {},
   "source": [
    "The Critical 'But': Acknowledging the Risks and Trade-offs\n",
    "This is where you demonstrate senior-level thinking in your report. A great submission won't just do the filtering; it will discuss the limitations.\n",
    "\n",
    "The Risk of Information Loss (False Negatives) 🗑️\n",
    "This is the biggest drawback. You are throwing data away. What if a crucial article from Reuters discusses a regional drought impacting \"the Levant\" but never explicitly names \"Syria,\" \"Lebanon,\" or any city from your list?\n",
    "\n",
    "Your NER model isn't perfect: It might fail to recognize a location name.\n",
    "\n",
    "Your location list isn't exhaustive: It might miss alternative spellings or aliases.\n",
    "Your current implementation correctly identifies relevant articles but will inevitably discard some relevant ones. This is a classic precision vs. recall trade-off. You are choosing to build a high-precision dataset at the cost of lower recall.\n",
    "\n",
    "Lack of Contextual Understanding 🧐\n",
    "Your current filter confirms the presence of a location name but not its context. An article could mention \"Baghdad\" in a purely historical context or as the location of a financial conference unrelated to food security. While this is a minor issue compared to the noise reduction benefits, it's a limitation worth mentioning in your reflection.\n",
    "\n",
    "State your choice: \"A crucial preprocessing step was to filter the corpus to include only articles geographically relevant to the Mashriq region.\"\n",
    "\n",
    "Justify it: Explain that this was done to enhance the signal-to-noise ratio, reduce computational load, and enable granular, location-specific risk analysis, which is the primary goal.\n",
    "\n",
    "Acknowledge the limitations: Explicitly discuss the risk of discarding relevant articles (false negatives) due to model or list imperfections. Frame it as a deliberate choice to prioritize precision for this high-stakes early warning system.\n",
    "\n",
    "Suggest future improvements (the \"if I had more time\" part): Propose a \"two-funnel\" approach for a real-world system:\n",
    "\n",
    "Funnel 1 (High-Precision): Your current pipeline for generating specific, geotagged alerts.\n",
    "\n",
    "Funnel 2 (High-Recall): A separate, lightweight process that monitors the discarded articles for spikes in key risk factors (e.g., \"drought,\" \"wheat prices\"). A sudden spike in this \"unfiltered\" stream could trigger a manual review and potentially reveal a systemic event your geofilter is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d4ad82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 15:51:14,340 - INFO - ============================================================\n",
      "2025-09-13 15:51:14,341 - INFO - Starting risk factor extraction pipeline\n",
      "2025-09-13 15:51:14,342 - INFO - ============================================================\n",
      "2025-09-13 15:51:14,343 - INFO - Running in FULL mode (all articles)\n",
      "2025-09-13 15:51:17,134 - INFO - Loaded 84,970 English articles.\n",
      "2025-09-13 15:51:21,288 - INFO - Loaded 77,147 Arabic articles.\n",
      "2025-09-13 15:51:21,346 - INFO - Combined data: 162,117 total articles.\n",
      "2025-09-13 15:51:21,895 - INFO - Loaded 167 risk factors\n",
      "2025-09-13 15:51:21,948 - INFO - GPU available: NVIDIA L4\n",
      "Device set to use cuda:0\n",
      "2025-09-13 15:51:23,498 - INFO - Classifier initialized: MoritzLaurer/deberta-v3-xsmall-zeroshot-v1.1-all-33\n",
      "2025-09-13 15:51:23,500 - INFO - GPU available: NVIDIA L4\n",
      "2025-09-13 15:51:23,503 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n",
      "2025-09-13 15:51:26,732 - INFO - Embedder initialized: paraphrase-multilingual-MiniLM-L12-v2\n",
      "2025-09-13 15:51:34,140 - INFO - Prepared 3,897,406 sentences from articles\n",
      "2025-09-13 15:51:34,241 - INFO - Pre-filtering 3,897,406 sentences...\n",
      "2025-09-13 15:51:34,242 - INFO - Similarity threshold: 0.55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d0ef4bc7a841f0ae9f55a3019755fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9caaf7f4724150b792097769655677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/121794 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 16:22:51,533 - INFO - Reduced to 151,225 relevant sentences (3.9% retention)\n",
      "2025-09-13 16:22:51,990 - INFO - Classifying 151,225 sentences...\n",
      "2025-09-13 16:22:51,991 - INFO - Confidence threshold: 0.9\n",
      "2025-09-13 17:29:51,814 - ERROR - Pipeline failed: CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacity of 22.28 GiB of which 2.36 GiB is free. Process 78491 has 19.91 GiB memory in use. Of the allocated memory 16.85 GiB is allocated by PyTorch, and 2.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacity of 22.28 GiB of which 2.36 GiB is free. Process 78491 has 19.91 GiB memory in use. Of the allocated memory 16.85 GiB is allocated by PyTorch, and 2.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 571\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m# --- SCRIPT EXECUTION ---\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \n\u001b[1;32m    551\u001b[0m \u001b[38;5;66;03m# Usage example - Sample mode (for testing)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# --- To run on the FULL dataset ---\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# Make sure you have enough time and memory, as this will process all articles.\u001b[39;00m\n\u001b[0;32m--> 571\u001b[0m df_risk_mentions_full \u001b[38;5;241m=\u001b[39m \u001b[43mrun_risk_extraction_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set this to False to process all data\u001b[39;49;00m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# The 'sample_size' parameter will be ignored now\u001b[39;49;00m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43msentence_similarity_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.55\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier_confidence_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.90\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# You might increase this if you have a powerful GPU\u001b[39;49;00m\n\u001b[1;32m    578\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;66;03m# Display a few results from the full run\u001b[39;00m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df_risk_mentions_full\u001b[38;5;241m.\u001b[39mempty:\n",
      "Cell \u001b[0;32mIn[1], line 520\u001b[0m, in \u001b[0;36mrun_risk_extraction_pipeline\u001b[0;34m(data_dir, classifier_model, embedder_model, classifier_batch_size, sentence_similarity_threshold, classifier_confidence_threshold, use_sample, sample_size)\u001b[0m\n\u001b[1;32m    517\u001b[0m filtered_sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df_filtered)\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# Classify sentences\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m df_mentions \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_sentences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_filtered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrisk_factor_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier_confidence_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier_batch_size\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m risk_mentions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df_mentions)\n\u001b[1;32m    526\u001b[0m \u001b[38;5;66;03m# Refine results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 301\u001b[0m, in \u001b[0;36mclassify_sentences\u001b[0;34m(df_sentences, classifier, risk_factor_labels, confidence_threshold, batch_size)\u001b[0m\n\u001b[1;32m    297\u001b[0m results_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# Run classification in batches\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, result \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[0;32m--> 301\u001b[0m     \u001b[38;5;28menumerate\u001b[39m(\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrisk_factor_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m),\n\u001b[1;32m    307\u001b[0m     total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(sentences),\n\u001b[1;32m    308\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassifying sentences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m ):\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# Check each label's confidence\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m label, score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m], result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m confidence_threshold:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/zero_shot_classification.py:209\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline.__call__\u001b[0;34m(self, sequences, *args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to understand extra arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/base.py:1448\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1445\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1446\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1447\u001b[0m     )\n\u001b[0;32m-> 1448\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:126\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:271\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 271\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/base.py:1374\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1373\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1374\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/zero_shot_classification.py:232\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline._forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters:\n\u001b[1;32m    231\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate_label\u001b[39m\u001b[38;5;124m\"\u001b[39m: candidate_label,\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m: sequence,\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_last\u001b[39m\u001b[38;5;124m\"\u001b[39m: inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_last\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutputs,\n\u001b[1;32m    239\u001b[0m }\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_outputs\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1079\u001b[0m, in \u001b[0;36mDebertaV2ForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1079\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m encoder_layer \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(encoder_layer)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:786\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    776\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    778\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    779\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    780\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    784\u001b[0m )\n\u001b[0;32m--> 786\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    793\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:659\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    657\u001b[0m rel_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_rel_embedding()\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer):\n\u001b[0;32m--> 659\u001b[0m     output_states, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    669\u001b[0m         all_attentions \u001b[38;5;241m=\u001b[39m all_attentions \u001b[38;5;241m+\u001b[39m (attn_weights,)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:438\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    431\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    436\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    437\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, Optional[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m--> 438\u001b[0m     attention_output, att_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m    447\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:371\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    364\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m     rel_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    370\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, Optional[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m--> 371\u001b[0m     self_output, att_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m         query_states \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:251\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_attention:\n\u001b[1;32m    250\u001b[0m     rel_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_dropout(rel_embeddings)\n\u001b[0;32m--> 251\u001b[0m     rel_att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisentangled_attention_bias\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rel_att \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m rel_att\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:344\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.disentangled_attention_bias\u001b[0;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[1;32m    342\u001b[0m     p2c_pos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m-\u001b[39mr_pos \u001b[38;5;241m+\u001b[39m att_span, \u001b[38;5;241m0\u001b[39m, att_span \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    343\u001b[0m     p2c_att \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(key_layer, pos_query_layer\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m--> 344\u001b[0m     p2c_att \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mp2c_att\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp2c_pos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    349\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p2c_att \u001b[38;5;241m/\u001b[39m scale\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mp2c_att\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacity of 22.28 GiB of which 2.36 GiB is free. Process 78491 has 19.91 GiB memory in use. Of the allocated memory 16.85 GiB is allocated by PyTorch, and 2.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Risk factor extraction pipeline for geographically filtered news articles.\n",
    "Uses zero-shot classification to identify risk factors in article sentences.\n",
    "Functional implementation without classes.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from transformers import pipeline, Pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_device() -> int:\n",
    "    \"\"\"\n",
    "    Determine the best available device for processing.\n",
    "    \n",
    "    Returns:\n",
    "        int: Device ID (0 for GPU, -1 for CPU).\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device_name = torch.cuda.get_device_name(0)\n",
    "        logger.info(f\"GPU available: {device_name}\")\n",
    "        return 0\n",
    "    else:\n",
    "        logger.info(\"No GPU found, using CPU\")\n",
    "        return -1\n",
    "\n",
    "\n",
    "def load_filtered_data(data_dir: str, use_sample: bool = True, sample_size: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load geographically filtered English and Arabic article data and combine them.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing processed data.\n",
    "        use_sample: Whether to use a sample of articles for testing.\n",
    "        sample_size: Number of articles to sample if use_sample is True.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A combined DataFrame of filtered articles.\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If no processed data files are found.\n",
    "    \"\"\"\n",
    "    processed_dir = Path(data_dir) / '02_processed'\n",
    "    eng_path = processed_dir / 'news_eng_processed.pkl'\n",
    "    ara_path = processed_dir / 'news_ara_processed.pkl'\n",
    "    \n",
    "    df_list = []\n",
    "    \n",
    "    # Load English data if it exists and is not empty\n",
    "    try:\n",
    "        df_eng = pd.read_pickle(eng_path)\n",
    "        if not df_eng.empty:\n",
    "            df_list.append(df_eng)\n",
    "            logger.info(f\"Loaded {len(df_eng):,} English articles.\")\n",
    "    except FileNotFoundError:\n",
    "        logger.warning(f\"English data file not found at: {eng_path}\")\n",
    "\n",
    "    # Load Arabic data if it exists and is not empty\n",
    "    try:\n",
    "        df_ara = pd.read_pickle(ara_path)\n",
    "        if not df_ara.empty:\n",
    "            df_list.append(df_ara)\n",
    "            logger.info(f\"Loaded {len(df_ara):,} Arabic articles.\")\n",
    "    except FileNotFoundError:\n",
    "        logger.warning(f\"Arabic data file not found at: {ara_path}\")\n",
    "\n",
    "    if not df_list:\n",
    "        error_msg = \"No processed data found. Please run the news processing pipeline first.\"\n",
    "        logger.error(error_msg)\n",
    "        raise FileNotFoundError(error_msg)\n",
    "        \n",
    "    # Combine the dataframes\n",
    "    df_combined = pd.concat(df_list, ignore_index=True)\n",
    "    logger.info(f\"Combined data: {len(df_combined):,} total articles.\")\n",
    "\n",
    "    if use_sample and not df_combined.empty:\n",
    "        # Ensure sample size is not larger than the dataframe\n",
    "        sample_size = min(sample_size, len(df_combined))\n",
    "        df_combined = df_combined.sample(n=sample_size, random_state=42).copy()\n",
    "        logger.info(f\"Using a random sample of {len(df_combined)} articles for processing.\")\n",
    "    \n",
    "    return df_combined\n",
    "\n",
    "\n",
    "def load_risk_factors(data_dir: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load risk factor labels from Excel file.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing raw data.\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of risk factor labels.\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If risk factors file doesn't exist.\n",
    "    \"\"\"\n",
    "    raw_dir = Path(data_dir) / '01_raw'\n",
    "    risk_path = raw_dir / 'risk-factors.xlsx'\n",
    "    \n",
    "    try:\n",
    "        df_risk = pd.read_excel(risk_path)\n",
    "        df_risk.dropna(subset=['risk_factor_english'], inplace=True)\n",
    "        \n",
    "        risk_factor_labels = df_risk['risk_factor_english'].tolist()\n",
    "        logger.info(f\"Loaded {len(risk_factor_labels)} risk factors\")\n",
    "        \n",
    "        return risk_factor_labels\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Risk factors file not found: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def initialize_classifier(model_name: str = 'MoritzLaurer/deberta-v3-xsmall-zeroshot-v1.1-all-33',\n",
    "                         device: Optional[int] = None) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Initialize zero-shot classification pipeline.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the classification model to use.\n",
    "        device: Device ID (None for auto-detection).\n",
    "        \n",
    "    Returns:\n",
    "        Pipeline: Initialized classification pipeline.\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If model loading fails.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "        \n",
    "    try:\n",
    "        classifier = pipeline(\n",
    "            \"zero-shot-classification\",\n",
    "            model=model_name,\n",
    "            device=device\n",
    "        )\n",
    "        logger.info(f\"Classifier initialized: {model_name}\")\n",
    "        return classifier\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize classifier: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def initialize_embedder(model_name: str = 'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "                       device: Optional[int] = None) -> SentenceTransformer:\n",
    "    \"\"\"\n",
    "    Initialize sentence embedding model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the embedding model to use.\n",
    "        device: Device ID (None for auto-detection).\n",
    "        \n",
    "    Returns:\n",
    "        SentenceTransformer: Initialized sentence transformer.\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If model loading fails.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "        \n",
    "    try:\n",
    "        embedder = SentenceTransformer(\n",
    "            model_name,\n",
    "            device='cuda' if device == 0 else 'cpu'\n",
    "        )\n",
    "        logger.info(f\"Embedder initialized: {model_name}\")\n",
    "        return embedder\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize embedder: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def prepare_sentences(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare sentences from articles for processing.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with articles containing 'sentences' column.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with individual sentences.\n",
    "    \"\"\"\n",
    "    # Add article ID if not present\n",
    "    if 'article_id' not in df.columns:\n",
    "        df['article_id'] = df.index\n",
    "    \n",
    "    # Explode sentences into individual rows\n",
    "    df_sentences = df.explode('sentences').rename(columns={'sentences': 'sentence_text'})\n",
    "    df_sentences = df_sentences[['article_id', 'date', 'sentence_text']].dropna(subset=['sentence_text'])\n",
    "    \n",
    "    logger.info(f\"Prepared {len(df_sentences):,} sentences from articles\")\n",
    "    \n",
    "    return df_sentences\n",
    "\n",
    "\n",
    "def filter_relevant_sentences(df_sentences: pd.DataFrame, \n",
    "                             embedder: SentenceTransformer,\n",
    "                             risk_factor_labels: List[str],\n",
    "                             similarity_threshold: float = 0.55) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pre-filter sentences using semantic similarity to risk factors.\n",
    "    \n",
    "    Args:\n",
    "        df_sentences: DataFrame with sentence data.\n",
    "        embedder: Initialized sentence transformer.\n",
    "        risk_factor_labels: List of risk factor labels.\n",
    "        similarity_threshold: Minimum similarity score for filtering.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with filtered relevant sentences.\n",
    "    \"\"\"\n",
    "    if df_sentences.empty:\n",
    "        return df_sentences\n",
    "    \n",
    "    sentences = df_sentences['sentence_text'].tolist()\n",
    "    \n",
    "    logger.info(f\"Pre-filtering {len(sentences):,} sentences...\")\n",
    "    logger.info(f\"Similarity threshold: {similarity_threshold}\")\n",
    "    \n",
    "    # Pre-compute risk factor embeddings\n",
    "    risk_factor_embeddings = embedder.encode(\n",
    "        risk_factor_labels,\n",
    "        convert_to_tensor=True\n",
    "    )\n",
    "    \n",
    "    # Encode sentences\n",
    "    sentence_embeddings = embedder.encode(\n",
    "        sentences,\n",
    "        convert_to_tensor=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Find semantically similar sentences to risk factors\n",
    "    hits = util.semantic_search(\n",
    "        sentence_embeddings,\n",
    "        risk_factor_embeddings,\n",
    "        top_k=1\n",
    "    )\n",
    "    \n",
    "    # Filter by similarity threshold\n",
    "    relevant_indices = [\n",
    "        i for i, hit_list in enumerate(hits)\n",
    "        if hit_list and hit_list[0]['score'] >= similarity_threshold\n",
    "    ]\n",
    "    \n",
    "    df_filtered = df_sentences.iloc[relevant_indices].copy()\n",
    "    \n",
    "    logger.info(f\"Reduced to {len(df_filtered):,} relevant sentences \"\n",
    "               f\"({len(df_filtered)/len(sentences)*100:.1f}% retention)\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def classify_sentences(df_sentences: pd.DataFrame,\n",
    "                      classifier: Pipeline,\n",
    "                      risk_factor_labels: List[str],\n",
    "                      confidence_threshold: float = 0.90,\n",
    "                      batch_size: int = 128) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Classify filtered sentences for risk factors.\n",
    "    \n",
    "    Args:\n",
    "        df_sentences: DataFrame with filtered sentences.\n",
    "        classifier: Initialized classification pipeline.\n",
    "        risk_factor_labels: List of risk factor labels.\n",
    "        confidence_threshold: Minimum confidence score for classification.\n",
    "        batch_size: Batch size for classification.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with risk factor classifications.\n",
    "    \"\"\"\n",
    "    if df_sentences.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    sentences = df_sentences['sentence_text'].tolist()\n",
    "    \n",
    "    logger.info(f\"Classifying {len(sentences):,} sentences...\")\n",
    "    logger.info(f\"Confidence threshold: {confidence_threshold}\")\n",
    "    \n",
    "    results_list = []\n",
    "    \n",
    "    # Run classification in batches\n",
    "    for i, result in tqdm(\n",
    "        enumerate(classifier(\n",
    "            sentences,\n",
    "            risk_factor_labels,\n",
    "            multi_label=True,\n",
    "            batch_size=batch_size\n",
    "        )),\n",
    "        total=len(sentences),\n",
    "        desc=\"Classifying sentences\"\n",
    "    ):\n",
    "        # Check each label's confidence\n",
    "        for label, score in zip(result['labels'], result['scores']):\n",
    "            if score >= confidence_threshold:\n",
    "                original_row = df_sentences.iloc[i]\n",
    "                results_list.append({\n",
    "                    'article_id': original_row['article_id'],\n",
    "                    'date': original_row['date'],\n",
    "                    'sentence_text': result['sequence'],\n",
    "                    'risk_factor': label,\n",
    "                    'confidence_score': score\n",
    "                })\n",
    "    \n",
    "    df_results = pd.DataFrame(results_list)\n",
    "    \n",
    "    logger.info(f\"Found {len(df_results):,} risk factor mentions\")\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "\n",
    "def refine_results(df_mentions: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Post-process results to keep only highest confidence mention per sentence.\n",
    "    \n",
    "    Args:\n",
    "        df_mentions: DataFrame with all risk mentions.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with refined risk mentions.\n",
    "    \"\"\"\n",
    "    if df_mentions.empty:\n",
    "        return df_mentions\n",
    "    \n",
    "    logger.info(\"Refining results to highest confidence per sentence...\")\n",
    "    \n",
    "    # Keep only the highest confidence label for each sentence\n",
    "    idx = df_mentions.groupby('sentence_text')['confidence_score'].idxmax()\n",
    "    df_refined = df_mentions.loc[idx].copy()\n",
    "    \n",
    "    # Sort by confidence score\n",
    "    df_refined = df_refined.sort_values('confidence_score', ascending=False)\n",
    "    \n",
    "    logger.info(f\"Refined from {len(df_mentions):,} to {len(df_refined):,} unique mentions\")\n",
    "    \n",
    "    return df_refined\n",
    "\n",
    "\n",
    "def save_results(df_results: pd.DataFrame, data_dir: str, use_sample: bool = True) -> Path:\n",
    "    \"\"\"\n",
    "    Save extraction results to CSV.\n",
    "    \n",
    "    Args:\n",
    "        df_results: DataFrame with risk factor mentions.\n",
    "        data_dir: Root directory for saving data.\n",
    "        use_sample: Whether this is sample data (affects filename).\n",
    "        \n",
    "    Returns:\n",
    "        Path: Path to saved file.\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If saving fails.\n",
    "    \"\"\"\n",
    "    models_dir = Path(data_dir) / '03_models'\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    filename = 'risk_mentions_SAMPLE.csv' if use_sample else 'risk_mentions_FULL.csv'\n",
    "    \n",
    "    output_path = models_dir / filename\n",
    "    \n",
    "    try:\n",
    "        df_results.to_csv(output_path, index=False)\n",
    "        logger.info(f\"Saved {len(df_results):,} risk mentions to {output_path}\")\n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save results: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def calculate_statistics(df_results: pd.DataFrame, \n",
    "                        total_articles: int,\n",
    "                        total_sentences: int,\n",
    "                        filtered_sentences: int,\n",
    "                        risk_mentions: int) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate statistics about the extraction.\n",
    "    \n",
    "    Args:\n",
    "        df_results: DataFrame with extraction results.\n",
    "        total_articles: Total number of articles processed.\n",
    "        total_sentences: Total number of sentences extracted.\n",
    "        filtered_sentences: Number of sentences after filtering.\n",
    "        risk_mentions: Number of risk mentions found.\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Dictionary of statistics.\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'total_articles': total_articles,\n",
    "        'total_sentences': total_sentences,\n",
    "        'filtered_sentences': filtered_sentences,\n",
    "        'risk_mentions': risk_mentions,\n",
    "        'unique_mentions': len(df_results)\n",
    "    }\n",
    "    \n",
    "    if not df_results.empty:\n",
    "        stats.update({\n",
    "            'unique_risk_factors': df_results['risk_factor'].nunique(),\n",
    "            'avg_confidence': df_results['confidence_score'].mean(),\n",
    "            'min_confidence': df_results['confidence_score'].min(),\n",
    "            'max_confidence': df_results['confidence_score'].max(),\n",
    "            'top_risk_factors': df_results['risk_factor'].value_counts().head(5).to_dict()\n",
    "        })\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "def display_statistics(stats: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Display extraction statistics.\n",
    "    \n",
    "    Args:\n",
    "        stats: Dictionary containing statistics.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EXTRACTION STATISTICS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total articles processed: {stats['total_articles']:,}\")\n",
    "    print(f\"Total sentences extracted: {stats['total_sentences']:,}\")\n",
    "    print(f\"Sentences after filtering: {stats['filtered_sentences']:,}\")\n",
    "    print(f\"Risk mentions found: {stats['risk_mentions']:,}\")\n",
    "    print(f\"Unique mentions (refined): {stats['unique_mentions']:,}\")\n",
    "    \n",
    "    if 'unique_risk_factors' in stats:\n",
    "        print(f\"\\nUnique risk factors: {stats['unique_risk_factors']}\")\n",
    "        print(f\"Average confidence: {stats['avg_confidence']:.3f}\")\n",
    "        print(f\"Confidence range: {stats['min_confidence']:.3f} - {stats['max_confidence']:.3f}\")\n",
    "        \n",
    "        if stats.get('top_risk_factors'):\n",
    "            print(\"\\nTop 5 risk factors:\")\n",
    "            for risk, count in stats['top_risk_factors'].items():\n",
    "                print(f\"  - {risk}: {count} mentions\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "def run_risk_extraction_pipeline(data_dir: str = '../data',\n",
    "                               classifier_model: str = 'MoritzLaurer/deberta-v3-xsmall-zeroshot-v1.1-all-33',\n",
    "                               embedder_model: str = 'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "                               classifier_batch_size: int = 128,\n",
    "                               sentence_similarity_threshold: float = 0.55,\n",
    "                               classifier_confidence_threshold: float = 0.90,\n",
    "                               use_sample: bool = True,\n",
    "                               sample_size: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Execute the complete risk factor extraction pipeline.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing data folders.\n",
    "        classifier_model: Name of the classification model to use.\n",
    "        embedder_model: Name of the embedding model to use.\n",
    "        classifier_batch_size: Batch size for classification.\n",
    "        sentence_similarity_threshold: Minimum similarity score for sentence filtering.\n",
    "        classifier_confidence_threshold: Minimum confidence score for classification.\n",
    "        use_sample: Whether to use a sample of articles for testing.\n",
    "        sample_size: Number of articles to sample if use_sample is True.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with refined risk factor mentions.\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If required files don't exist.\n",
    "        Exception: If processing fails.\n",
    "    \"\"\"\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"Starting risk factor extraction pipeline\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    \n",
    "    if use_sample:\n",
    "        logger.info(f\"Running in SAMPLE mode ({sample_size} articles)\")\n",
    "    else:\n",
    "        logger.info(\"Running in FULL mode (all articles)\")\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        df_articles = load_filtered_data(data_dir, use_sample, sample_size)\n",
    "        if df_articles.empty:\n",
    "            logger.warning(\"No articles to process after loading.\")\n",
    "            return pd.DataFrame()\n",
    "        total_articles = len(df_articles)\n",
    "        \n",
    "        # Load risk factors\n",
    "        risk_factor_labels = load_risk_factors(data_dir)\n",
    "        \n",
    "        # Initialize models\n",
    "        classifier = initialize_classifier(classifier_model)\n",
    "        embedder = initialize_embedder(embedder_model)\n",
    "        \n",
    "        # Prepare sentences\n",
    "        df_sentences = prepare_sentences(df_articles)\n",
    "        if df_sentences.empty:\n",
    "            logger.warning(\"No sentences to process after preparation.\")\n",
    "            return pd.DataFrame()\n",
    "        total_sentences = len(df_sentences)\n",
    "        \n",
    "        # Filter relevant sentences\n",
    "        df_filtered = filter_relevant_sentences(\n",
    "            df_sentences, embedder, risk_factor_labels, sentence_similarity_threshold\n",
    "        )\n",
    "        filtered_sentences = len(df_filtered)\n",
    "        \n",
    "        # Classify sentences\n",
    "        df_mentions = classify_sentences(\n",
    "            df_filtered, classifier, risk_factor_labels, \n",
    "            classifier_confidence_threshold, classifier_batch_size\n",
    "        )\n",
    "        risk_mentions = len(df_mentions)\n",
    "        \n",
    "        # Refine results\n",
    "        df_refined = refine_results(df_mentions)\n",
    "        \n",
    "        # Save results\n",
    "        save_results(df_refined, data_dir, use_sample)\n",
    "        \n",
    "        # Display statistics\n",
    "        stats = calculate_statistics(\n",
    "            df_refined, total_articles, total_sentences, \n",
    "            filtered_sentences, risk_mentions\n",
    "        )\n",
    "        display_statistics(stats)\n",
    "        \n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"Pipeline completed successfully!\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        return df_refined\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline failed: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# --- SCRIPT EXECUTION ---\n",
    "\n",
    "# Usage example - Sample mode (for testing)\n",
    "# This will now correctly load your Arabic data and process a small sample.\n",
    "# df_risk_mentions_sample = run_risk_extraction_pipeline(\n",
    "#     data_dir='../data',\n",
    "#     use_sample=True,\n",
    "#     sample_size=20, # Increased sample size a bit\n",
    "#     sentence_similarity_threshold=0.55,\n",
    "#     classifier_confidence_threshold=0.90,\n",
    "#     classifier_batch_size=128\n",
    "# )\n",
    "\n",
    "# # Display sample results if available\n",
    "# if not df_risk_mentions_sample.empty:\n",
    "#     print(\"\\n--- Sample Risk Mentions ---\")\n",
    "#     print(df_risk_mentions_sample[['risk_factor', 'confidence_score', 'sentence_text']].head())\n",
    "#     print(\"-\" * 28)\n",
    "\n",
    "# --- To run on the FULL dataset ---\n",
    "# Make sure you have enough time and memory, as this will process all articles.\n",
    "\n",
    "df_risk_mentions_full = run_risk_extraction_pipeline(\n",
    "    data_dir='../data',\n",
    "    use_sample=False,  # Set this to False to process all data\n",
    "    # The 'sample_size' parameter will be ignored now\n",
    "    sentence_similarity_threshold=0.55,\n",
    "    classifier_confidence_threshold=0.90,\n",
    "    classifier_batch_size=1024 # You might increase this if you have a powerful GPU\n",
    ")\n",
    "\n",
    "# Display a few results from the full run\n",
    "if not df_risk_mentions_full.empty:\n",
    "    print(\"\\n--- Full Run: Top Risk Mentions ---\")\n",
    "    print(df_risk_mentions_full[['risk_factor', 'confidence_score', 'sentence_text']].head())\n",
    "    print(\"-\" * 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ae2308",
   "metadata": {},
   "source": [
    "tutaj mysle ze mozna by to zastapic tak naprawde jednym modelem klasyfikacyjnym ktory przyjmowalby jako sentence jako input i klasyfikowalby czy jest to ktorys z tych 167 risk factorow lub tez nie :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f5e0c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 14:34:32,535 - INFO - Starting risk mention geotagging pipeline\n",
      "2025-09-13 14:34:32,537 - INFO - Step 1: Loading data\n",
      "2025-09-13 14:34:32,735 - INFO - Loaded 34,275 risk mention sentences\n",
      "2025-09-13 14:34:32,769 - INFO - Loaded 0 English and 352 Arabic articles\n",
      "2025-09-13 14:34:32,770 - INFO - Step 2: Building location resolvers\n",
      "2025-09-13 14:34:32,773 - INFO - Built location resolvers with 918 aliases\n",
      "2025-09-13 14:34:32,773 - INFO - Step 3: Initializing NER pipeline\n",
      "2025-09-13 14:34:32,774 - INFO - GPU available: NVIDIA L4\n",
      "2025-09-13 14:34:32,775 - INFO - Loading NER model: Babelscape/wikineural-multilingual-ner\n",
      "Device set to use cuda:0\n",
      "2025-09-13 14:34:33,494 - INFO - NER pipeline initialized successfully\n",
      "2025-09-13 14:34:33,495 - INFO - Step 4: Performing hybrid geotagging\n",
      "2025-09-13 14:34:33,501 - INFO - Processing 123 articles with risk mentions\n",
      "2025-09-13 14:34:33,502 - INFO - Extracting article-level locations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96759586779540ecadfb1f869623c96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving article locations:   0%|          | 0/123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 14:34:38,480 - INFO - Found 204 total locations in 123 articles\n",
      "2025-09-13 14:34:38,481 - INFO - Extracting sentence-level locations...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:188\u001b[0m, in \u001b[0;36mPipelineChunkIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# Try to return next item\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubiterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m# When a preprocess iterator ends, we can start looking at the next item\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;66;03m# ChunkIterator will keep feeding until ALL elements of iterator\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Another way to look at it, is we're basically flattening lists of lists\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# into a single list, but with generators\u001b[39;00m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 535\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_final\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Usage example with sampling for quick testing\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m df_geotagged \u001b[38;5;241m=\u001b[39m \u001b[43mrun_geotagging_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# sample_size=100,  # Set to None to process all data\u001b[39;49;00m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mner_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBabelscape/wikineural-multilingual-ner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    540\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# Example of accessing the results\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_geotagged)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m risk-location pairs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 515\u001b[0m, in \u001b[0;36mrun_geotagging_pipeline\u001b[0;34m(data_dir, sample_size, batch_size, ner_model)\u001b[0m\n\u001b[1;32m    510\u001b[0m df_articles_filtered \u001b[38;5;241m=\u001b[39m extract_article_locations(\n\u001b[1;32m    511\u001b[0m     df_articles_filtered, ner_pipeline, location_lookup, batch_size\n\u001b[1;32m    512\u001b[0m )\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# Extract sentence-level locations\u001b[39;00m\n\u001b[0;32m--> 515\u001b[0m df_risk \u001b[38;5;241m=\u001b[39m \u001b[43mextract_sentence_locations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_risk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mner_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation_lookup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# Step 5: Merge and apply hierarchical logic\u001b[39;00m\n\u001b[1;32m    520\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep 5: Merging and finalizing data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 287\u001b[0m, in \u001b[0;36mextract_sentence_locations\u001b[0;34m(df_sentences, ner_pipeline, location_lookup, batch_size)\u001b[0m\n\u001b[1;32m    284\u001b[0m sentence_texts \u001b[38;5;241m=\u001b[39m df_sentences[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# Batch process with NER\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m sentence_entities \u001b[38;5;241m=\u001b[39m \u001b[43mner_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43msentence_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# Resolve locations for each sentence\u001b[39;00m\n\u001b[1;32m    293\u001b[0m sentence_locations \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    294\u001b[0m     resolve_locations_from_entities(entities, location_lookup)\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entities \u001b[38;5;129;01min\u001b[39;00m tqdm(sentence_entities, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResolving sentence locations\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    296\u001b[0m ]\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:279\u001b[0m, in \u001b[0;36mTokenClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset_mapping:\n\u001b[1;32m    277\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m offset_mapping\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/base.py:1448\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1445\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1446\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1447\u001b[0m     )\n\u001b[0;32m-> 1448\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:126\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:271\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 271\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:33\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:197\u001b[0m, in \u001b[0;36mPipelineChunkIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m# When a preprocess iterator ends, we can start looking at the next item\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;66;03m# ChunkIterator will keep feeding until ALL elements of iterator\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Another way to look at it, is we're basically flattening lists of lists\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# into a single list, but with generators\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubiterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m--> 197\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubiterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m processed\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:309\u001b[0m, in \u001b[0;36mTokenClassificationPipeline.preprocess\u001b[0;34m(self, sentence, offset_mapping, **preprocess_params)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `is_split_into_words=False`, `sentence` must be an untokenized string.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    307\u001b[0m     text_to_tokenize \u001b[38;5;241m=\u001b[39m sentence\n\u001b[0;32m--> 309\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_to_tokenize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_fast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_split_into_words \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mis_fast:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words=True is only supported with fast tokenizers.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2911\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2909\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2910\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2911\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2913\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3021\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2999\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   3000\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   3001\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3018\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3019\u001b[0m     )\n\u001b[1;32m   3020\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3021\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3024\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3034\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3035\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3040\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3041\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3042\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3096\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3067\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3068\u001b[0m \u001b[38;5;124;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[1;32m   3069\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;124;03m        method).\u001b[39;00m\n\u001b[1;32m   3085\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3087\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3088\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3089\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3093\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3094\u001b[0m )\n\u001b[0;32m-> 3096\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3099\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3115\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3116\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3117\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:627\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    605\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    625\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    626\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 627\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:553\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m!=\u001b[39m split_special_tokens:\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m=\u001b[39m split_special_tokens\n\u001b[0;32m--> 553\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: tuple[\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m#                       list[dict[str, list[list[int]]]] or list[dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m#                       list[EncodingFast]\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    565\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    567\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    577\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Risk mention geotagging pipeline for multilingual news articles.\n",
    "Extracts and resolves geographic locations from risk-related sentences using NER,\n",
    "applying hierarchical logic to maximize precision.\n",
    "Functional implementation without classes.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple, Optional\n",
    "from transformers import pipeline, Pipeline\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_device() -> int:\n",
    "    \"\"\"\n",
    "    Determine the best available device for processing.\n",
    "    \n",
    "    Returns:\n",
    "        Device ID (0 for GPU, -1 for CPU)\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "        return 0\n",
    "    else:\n",
    "        logger.info(\"No GPU found, using CPU\")\n",
    "        return -1\n",
    "\n",
    "\n",
    "def load_risk_mentions(data_dir: str, sample_size: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load risk mention sentences from processed data.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing data folders\n",
    "        sample_size: Number of rows to process (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing risk mentions\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If risk mentions file doesn't exist\n",
    "    \"\"\"\n",
    "    models_dir = Path(data_dir) / '03_models'\n",
    "    risk_path = models_dir / 'risk_mentions_SAMPLE_FINAL.csv'\n",
    "    \n",
    "    try:\n",
    "        df_risk = pd.read_csv(risk_path)\n",
    "        \n",
    "        # Apply sampling if specified\n",
    "        if sample_size:\n",
    "            df_risk = df_risk.head(sample_size).copy()\n",
    "            logger.info(f\"Sampling {sample_size} rows for processing\")\n",
    "        \n",
    "        logger.info(f\"Loaded {len(df_risk):,} risk mention sentences\")\n",
    "        return df_risk\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Risk mentions file not found: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_articles(data_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and combine English and Arabic article datasets.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing data folders\n",
    "        \n",
    "    Returns:\n",
    "        Combined DataFrame with article content\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If article files don't exist\n",
    "    \"\"\"\n",
    "    processed_dir = Path(data_dir) / '02_processed'\n",
    "    eng_path = processed_dir / 'news_eng_processed.pkl'\n",
    "    ara_path = processed_dir / 'news_ara_processed.pkl'\n",
    "    \n",
    "    try:\n",
    "        # Load datasets\n",
    "        df_eng = pd.read_pickle(eng_path)\n",
    "        df_eng['language'] = 'english'\n",
    "        \n",
    "        df_ara = pd.read_pickle(ara_path)\n",
    "        df_ara['language'] = 'arabic'\n",
    "        \n",
    "        # Ensure article IDs exist\n",
    "        if 'article_id' not in df_eng.columns:\n",
    "            df_eng['article_id'] = df_eng.index\n",
    "        if 'article_id' not in df_ara.columns:\n",
    "            df_ara['article_id'] = df_ara.index\n",
    "        \n",
    "        # Combine datasets\n",
    "        df_articles = pd.concat([df_eng, df_ara], ignore_index=True)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(df_eng):,} English and {len(df_ara):,} Arabic articles\")\n",
    "        return df_articles\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Article file not found: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def build_location_resolvers(data_dir: str) -> Tuple[Dict[str, str], Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Build lookup dictionaries for location name resolution.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing raw data\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (location_lookup, id_to_english_name) dictionaries\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If location files don't exist\n",
    "    \"\"\"\n",
    "    raw_dir = Path(data_dir) / '01_raw'\n",
    "    eng_path = raw_dir / 'id_english_location_name.pkl'\n",
    "    ara_path = raw_dir / 'id_arabic_location_name.pkl'\n",
    "    \n",
    "    try:\n",
    "        # Load location dictionaries\n",
    "        with open(eng_path, 'rb') as f:\n",
    "            eng_locations = pickle.load(f)\n",
    "        with open(ara_path, 'rb') as f:\n",
    "            ara_locations = pickle.load(f)\n",
    "        \n",
    "        # Build name-to-ID lookup\n",
    "        location_lookup = {}\n",
    "        for location_dict in [eng_locations, ara_locations]:\n",
    "            for loc_id, names in location_dict.items():\n",
    "                for name in names:\n",
    "                    location_lookup[name.lower()] = loc_id\n",
    "        \n",
    "        # Build ID-to-English name lookup\n",
    "        id_to_english_name = {\n",
    "            loc_id: names[0] \n",
    "            for loc_id, names in eng_locations.items()\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Built location resolvers with {len(location_lookup):,} aliases\")\n",
    "        \n",
    "        return location_lookup, id_to_english_name\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Location dictionary not found: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def initialize_ner_pipeline(model_name: str = \"Babelscape/wikineural-multilingual-ner\", \n",
    "                           device: Optional[int] = None) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Initialize the NER pipeline for entity extraction.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the NER model to use\n",
    "        device: Device ID (None for auto-detection)\n",
    "        \n",
    "    Returns:\n",
    "        Initialized NER pipeline\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If model loading fails\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "        \n",
    "    logger.info(f\"Loading NER model: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        ner_pipeline = pipeline(\n",
    "            \"ner\",\n",
    "            model=model_name,\n",
    "            aggregation_strategy=\"simple\",\n",
    "            device=device\n",
    "        )\n",
    "        logger.info(\"NER pipeline initialized successfully\")\n",
    "        return ner_pipeline\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load NER model: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def resolve_locations_from_entities(entities: List[Dict], \n",
    "                                   location_lookup: Dict[str, str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract and resolve location IDs from NER entities.\n",
    "    \n",
    "    Args:\n",
    "        entities: List of entity dictionaries from NER pipeline\n",
    "        location_lookup: Dictionary mapping location names to IDs\n",
    "        \n",
    "    Returns:\n",
    "        List of resolved location IDs\n",
    "    \"\"\"\n",
    "    found_ids = set()\n",
    "    \n",
    "    for entity in entities:\n",
    "        if entity.get('entity_group') == 'LOC':\n",
    "            loc_name_lower = entity.get('word', '').lower()\n",
    "            if loc_name_lower in location_lookup:\n",
    "                found_ids.add(location_lookup[loc_name_lower])\n",
    "    \n",
    "    return list(found_ids)\n",
    "\n",
    "\n",
    "def extract_article_locations(df_articles: pd.DataFrame,\n",
    "                             ner_pipeline: Pipeline,\n",
    "                             location_lookup: Dict[str, str],\n",
    "                             batch_size: int = 128) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract locations from article bodies using batch NER processing.\n",
    "    \n",
    "    Args:\n",
    "        df_articles: DataFrame containing article bodies\n",
    "        ner_pipeline: Initialized NER pipeline\n",
    "        location_lookup: Dictionary mapping location names to IDs\n",
    "        batch_size: Batch size for NER processing\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with article_locations column added\n",
    "    \"\"\"\n",
    "    logger.info(\"Extracting article-level locations...\")\n",
    "    \n",
    "    # Prepare texts for processing\n",
    "    article_bodies = df_articles['body'].fillna('').tolist()\n",
    "    \n",
    "    # Batch process with NER\n",
    "    article_entities = ner_pipeline(\n",
    "        article_bodies, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Resolve locations for each article\n",
    "    article_locations = [\n",
    "        resolve_locations_from_entities(entities, location_lookup) \n",
    "        for entities in tqdm(article_entities, desc=\"Resolving article locations\")\n",
    "    ]\n",
    "    \n",
    "    df_articles = df_articles.copy()\n",
    "    df_articles['article_locations'] = article_locations\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_locations = sum(len(locs) for locs in article_locations)\n",
    "    articles_with_locations = sum(1 for locs in article_locations if locs)\n",
    "    \n",
    "    logger.info(f\"Found {total_locations:,} total locations in \"\n",
    "               f\"{articles_with_locations:,} articles\")\n",
    "    \n",
    "    return df_articles\n",
    "\n",
    "\n",
    "def extract_sentence_locations(df_sentences: pd.DataFrame,\n",
    "                              ner_pipeline: Pipeline,\n",
    "                              location_lookup: Dict[str, str],\n",
    "                              batch_size: int = 128) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract locations from individual sentences using batch NER processing.\n",
    "    \n",
    "    Args:\n",
    "        df_sentences: DataFrame containing sentence texts\n",
    "        ner_pipeline: Initialized NER pipeline\n",
    "        location_lookup: Dictionary mapping location names to IDs\n",
    "        batch_size: Batch size for NER processing\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with sentence_locations column added\n",
    "    \"\"\"\n",
    "    logger.info(\"Extracting sentence-level locations...\")\n",
    "    \n",
    "    # Prepare texts for processing\n",
    "    sentence_texts = df_sentences['sentence_text'].fillna('').tolist()\n",
    "    \n",
    "    # Batch process with NER\n",
    "    sentence_entities = ner_pipeline(\n",
    "        sentence_texts,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Resolve locations for each sentence\n",
    "    sentence_locations = [\n",
    "        resolve_locations_from_entities(entities, location_lookup)\n",
    "        for entities in tqdm(sentence_entities, desc=\"Resolving sentence locations\")\n",
    "    ]\n",
    "    \n",
    "    df_sentences = df_sentences.copy()\n",
    "    df_sentences['sentence_locations'] = sentence_locations\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_locations = sum(len(locs) for locs in sentence_locations)\n",
    "    sentences_with_locations = sum(1 for locs in sentence_locations if locs)\n",
    "    \n",
    "    logger.info(f\"Found {total_locations:,} total locations in \"\n",
    "               f\"{sentences_with_locations:,} sentences\")\n",
    "    \n",
    "    return df_sentences\n",
    "\n",
    "\n",
    "def apply_hierarchical_logic(row: pd.Series) -> List[str]:\n",
    "    \"\"\"\n",
    "    Apply hierarchical location selection logic.\n",
    "    \n",
    "    Priority order:\n",
    "    1. Specific sentence locations (ID length > 2)\n",
    "    2. Any sentence locations\n",
    "    3. Specific article locations (ID length > 2)\n",
    "    4. Any article locations\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row with location columns\n",
    "        \n",
    "    Returns:\n",
    "        List of selected location IDs\n",
    "    \"\"\"\n",
    "    # Check for specific sentence locations\n",
    "    sentence_specific = {\n",
    "        loc for loc in row['sentence_locations'] \n",
    "        if len(loc) > 2\n",
    "    }\n",
    "    if sentence_specific:\n",
    "        return list(sentence_specific)\n",
    "    \n",
    "    # Use any sentence locations\n",
    "    if row['sentence_locations']:\n",
    "        return row['sentence_locations']\n",
    "    \n",
    "    # Check for specific article locations\n",
    "    article_specific = {\n",
    "        loc for loc in row['article_locations'] \n",
    "        if len(loc) > 2\n",
    "    }\n",
    "    if article_specific:\n",
    "        return list(article_specific)\n",
    "    \n",
    "    # Fall back to any article locations\n",
    "    return row['article_locations']\n",
    "\n",
    "\n",
    "def merge_and_finalize(df_risk: pd.DataFrame,\n",
    "                      df_articles: pd.DataFrame,\n",
    "                      id_to_english_name: Dict[str, str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge risk mentions with locations and apply hierarchical logic.\n",
    "    \n",
    "    Args:\n",
    "        df_risk: DataFrame with risk mentions and sentence locations\n",
    "        df_articles: DataFrame with article locations\n",
    "        id_to_english_name: Dictionary mapping location IDs to English names\n",
    "        \n",
    "    Returns:\n",
    "        Final DataFrame with one row per risk-location pair\n",
    "    \"\"\"\n",
    "    logger.info(\"Applying hierarchical logic and finalizing data...\")\n",
    "    \n",
    "    # Merge sentence and article data\n",
    "    df_merged = pd.merge(\n",
    "        df_risk,\n",
    "        df_articles[['article_id', 'language', 'article_locations']],\n",
    "        on='article_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Apply hierarchical location selection\n",
    "    df_merged['final_locations'] = df_merged.apply(\n",
    "        apply_hierarchical_logic, \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Explode to one row per location\n",
    "    df_exploded = df_merged.explode('final_locations').rename(\n",
    "        columns={'final_locations': 'location_id'}\n",
    "    )\n",
    "    \n",
    "    # Remove rows without locations\n",
    "    df_exploded = df_exploded.dropna(subset=['location_id'])\n",
    "    \n",
    "    # Add English location names\n",
    "    df_exploded['location_name_english'] = df_exploded['location_id'].map(\n",
    "        id_to_english_name\n",
    "    )\n",
    "    \n",
    "    # Select and order final columns\n",
    "    final_columns = [\n",
    "        'article_id', 'date', 'language', 'sentence_text', \n",
    "        'risk_factor', 'confidence_score', 'location_id', \n",
    "        'location_name_english'\n",
    "    ]\n",
    "    df_final = df_exploded[final_columns].copy()\n",
    "    \n",
    "    logger.info(f\"Created {len(df_final):,} risk-location pairs\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "\n",
    "def save_geotagging_results(df_final: pd.DataFrame, data_dir: str) -> Path:\n",
    "    \"\"\"\n",
    "    Save geotagged results to CSV file.\n",
    "    \n",
    "    Args:\n",
    "        df_final: Final processed DataFrame\n",
    "        data_dir: Root directory for saving data\n",
    "        \n",
    "    Returns:\n",
    "        Path to saved file\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If saving fails\n",
    "    \"\"\"\n",
    "    models_dir = Path(data_dir) / '03_models'\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_path = models_dir / 'risk_mentions_geotagged_FINAL.csv'\n",
    "    \n",
    "    try:\n",
    "        df_final.to_csv(output_path, index=False)\n",
    "        logger.info(f\"Saved {len(df_final):,} geotagged risk mentions to: {output_path}\")\n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving results: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def display_geotagging_sample(df: pd.DataFrame, n: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Display sample results for verification.\n",
    "    \n",
    "    Args:\n",
    "        df: Final DataFrame\n",
    "        n: Number of samples to display\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        logger.warning(\"No data to display\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n--- Sample Geotagged Risk Mentions ---\")\n",
    "    print(f\"Showing {min(n, len(df))} of {len(df):,} total records:\")\n",
    "    print()\n",
    "    \n",
    "    sample = df.head(n)\n",
    "    for idx, row in sample.iterrows():\n",
    "        print(f\"Risk: {row['risk_factor']} (confidence: {row['confidence_score']:.2f})\")\n",
    "        print(f\"Location: {row['location_name_english']} ({row['location_id']})\")\n",
    "        print(f\"Sentence: {row['sentence_text'][:100]}...\")\n",
    "        print(f\"Language: {row['language']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "\n",
    "def run_geotagging_pipeline(data_dir: str = '../data',\n",
    "                           sample_size: Optional[int] = None,\n",
    "                           batch_size: int = 128,\n",
    "                           ner_model: str = \"Babelscape/wikineural-multilingual-ner\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Execute the complete geotagging pipeline.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Root directory containing data folders\n",
    "        sample_size: Number of rows to process (None for all)\n",
    "        batch_size: Batch size for NER processing\n",
    "        ner_model: Model name for NER pipeline\n",
    "        \n",
    "    Returns:\n",
    "        Final geotagged DataFrame\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If required files don't exist\n",
    "        Exception: If processing fails\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting risk mention geotagging pipeline\")\n",
    "    \n",
    "    if sample_size:\n",
    "        logger.info(f\"Running on SAMPLE of {sample_size} rows\")\n",
    "    \n",
    "    # Step 1: Load all necessary data\n",
    "    logger.info(\"Step 1: Loading data\")\n",
    "    df_risk = load_risk_mentions(data_dir, sample_size)\n",
    "    df_articles = load_articles(data_dir)\n",
    "    \n",
    "    # Step 2: Build location resolvers\n",
    "    logger.info(\"Step 2: Building location resolvers\")\n",
    "    location_lookup, id_to_english_name = build_location_resolvers(data_dir)\n",
    "    \n",
    "    # Step 3: Initialize NER pipeline\n",
    "    logger.info(\"Step 3: Initializing NER pipeline\")\n",
    "    ner_pipeline = initialize_ner_pipeline(ner_model)\n",
    "    \n",
    "    # Step 4: Extract locations (hybrid geotagging)\n",
    "    logger.info(\"Step 4: Performing hybrid geotagging\")\n",
    "    \n",
    "    # Filter articles to those with risk mentions\n",
    "    risk_article_ids = df_risk['article_id'].unique()\n",
    "    df_articles_filtered = df_articles[\n",
    "        df_articles['article_id'].isin(risk_article_ids)\n",
    "    ][['article_id', 'body', 'language']].copy()\n",
    "    \n",
    "    logger.info(f\"Processing {len(df_articles_filtered):,} articles with risk mentions\")\n",
    "    \n",
    "    # Extract article-level locations\n",
    "    df_articles_filtered = extract_article_locations(\n",
    "        df_articles_filtered, ner_pipeline, location_lookup, batch_size\n",
    "    )\n",
    "    \n",
    "    # Extract sentence-level locations\n",
    "    df_risk = extract_sentence_locations(\n",
    "        df_risk, ner_pipeline, location_lookup, batch_size\n",
    "    )\n",
    "    \n",
    "    # Step 5: Merge and apply hierarchical logic\n",
    "    logger.info(\"Step 5: Merging and finalizing data\")\n",
    "    df_final = merge_and_finalize(df_risk, df_articles_filtered, id_to_english_name)\n",
    "    \n",
    "    # Display sample for verification\n",
    "    display_geotagging_sample(df_final)\n",
    "    \n",
    "    # Step 6: Save results\n",
    "    logger.info(\"Step 6: Saving results\")\n",
    "    save_geotagging_results(df_final, data_dir)\n",
    "    \n",
    "    logger.info(\"Pipeline completed successfully\")\n",
    "    return df_final\n",
    "\n",
    "\n",
    "# Usage example with sampling for quick testing\n",
    "df_geotagged = run_geotagging_pipeline(\n",
    "    data_dir='../data',\n",
    "    # sample_size=100,  # Set to None to process all data\n",
    "    batch_size=128,\n",
    "    ner_model=\"Babelscape/wikineural-multilingual-ner\"\n",
    ")\n",
    "\n",
    "# Example of accessing the results\n",
    "print(f\"\\nProcessed {len(df_geotagged):,} risk-location pairs\")\n",
    "\n",
    "# Analyze distribution\n",
    "if not df_geotagged.empty:\n",
    "    risk_distribution = df_geotagged['risk_factor'].value_counts()\n",
    "    print(f\"\\nRisk factor distribution:\")\n",
    "    for risk, count in risk_distribution.head().items():\n",
    "        print(f\"  {risk}: {count:,}\")\n",
    "\n",
    "    location_distribution = df_geotagged['location_name_english'].value_counts()\n",
    "    print(f\"\\nTop locations mentioned:\")\n",
    "    for location, count in location_distribution.head().items():\n",
    "        print(f\"  {location}: {count:,}\")\n",
    "\n",
    "\n",
    "# Alternative usage examples:\n",
    "\n",
    "# Full processing mode\n",
    "# df_geotagged_full = run_geotagging_pipeline(\n",
    "#     data_dir='../data',\n",
    "#     sample_size=None,  # Process all data\n",
    "#     batch_size=256,\n",
    "#     ner_model=\"Babelscape/wikineural-multilingual-ner\"\n",
    "# )\n",
    "\n",
    "# Custom configuration with different batch size and model\n",
    "# df_geotagged_custom = run_geotagging_pipeline(\n",
    "#     data_dir='../data',\n",
    "#     sample_size=50,\n",
    "#     batch_size=64,\n",
    "#     ner_model=\"dbmdz/bert-large-cased-finetuned-conll03-english\"  # Alternative NER model\n",
    "# )\n",
    "\n",
    "# Process specific number of records for testing\n",
    "# df_geotagged_test = run_geotagging_pipeline(\n",
    "#     data_dir='../data',\n",
    "#     sample_size=20,\n",
    "#     batch_size=32\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495c075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1. Load Necessary Data (Corrected) ---\n",
    "print(\"--- Step 1: Loading Processed Data ---\")\n",
    "DATA_DIR = '../data'\n",
    "MODELS_DIR = os.path.join(DATA_DIR, '03_models')\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, '01_raw')\n",
    "\n",
    "# CORRECTED: Load the file you just created from notebook 05\n",
    "df_final_exploded = pd.read_csv(os.path.join(MODELS_DIR, 'risk_mentions_geotagged_FINAL.csv'))\n",
    "\n",
    "# CORRECTED: Load the risk factor cluster file with the correct name\n",
    "df_clusters = pd.read_excel(os.path.join(RAW_DATA_DIR, 'risk-factors-categories.xlsx'))\n",
    "\n",
    "# This file is needed for normalization\n",
    "df_geo_articles = pd.read_pickle(os.path.join(DATA_DIR, '02_processed', 'news_geographically_filtered.pkl'))\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "# --- 2. Prepare the Data (Corrected) ---\n",
    "print(\"--- Step 2: Preparing Data for Aggregation ---\")\n",
    "\n",
    "# --- HELPFUL DEBUGGING STEP ---\n",
    "# Print the column names to see what they are actually called\n",
    "print(\"Columns in df_clusters (from risk-factors-categories.xlsx):\")\n",
    "print(df_clusters.columns)\n",
    "# -----------------------------\n",
    "\n",
    "# Ensure the 'date' column is in datetime format\n",
    "df_final_exploded['date'] = pd.to_datetime(df_final_exploded['date'])\n",
    "\n",
    "# Merge the risk mentions with their thematic clusters\n",
    "# CORRECTED: Changed 'right_on' to the correct column name. It's likely 'risk_factor'.\n",
    "df_merged = pd.merge(df_final_exploded, df_clusters, on='risk_factor') # Using 'on' is cleaner when column names match\n",
    "\n",
    "print(\"\\nMerged risk mentions with thematic clusters.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 3. Aggregate Risk Mentions (Corrected) ---\n",
    "print(\"--- Step 3: Aggregating Daily Risk Counts ---\")\n",
    "\n",
    "# CORRECTED: Changed 'theme' to 'cluster' to match the actual column name\n",
    "df_daily_counts = df_merged.groupby([pd.Grouper(key='date', freq='D'), 'location_id', 'location_name_english', 'cluster']).size().reset_index(name='risk_mention_count')\n",
    "\n",
    "print(\"Calculated raw daily counts of risk mentions per location and theme.\")\n",
    "display(df_daily_counts.head())\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 4. Normalization (Crucial Step) ---\n",
    "# To avoid bias from varying news volume, we normalize by the number of articles published per day.\n",
    "# NOTE: This is a simplified normalization. A more advanced approach would be to get article counts *per location*,\n",
    "# which would require re-running the geotagger on ALL articles, not just those with risks.\n",
    "# For this assessment, normalizing by total daily articles is a reasonable simplification.\n",
    "\n",
    "print(\"--- Step 4: Normalizing Risk Counts ---\")\n",
    "df_geo_articles['date'] = pd.to_datetime(df_geo_articles['date'])\n",
    "daily_article_volume = df_geo_articles.groupby(pd.Grouper(key='date', freq='D')).size().reset_index(name='total_articles_published')\n",
    "\n",
    "# Merge the risk counts with the total article volume for normalization\n",
    "df_normalized = pd.merge(df_daily_counts, daily_article_volume, on='date')\n",
    "df_normalized['normalized_risk'] = df_normalized['risk_mention_count'] / df_normalized['total_articles_published']\n",
    "\n",
    "print(\"Normalized risk scores by total daily article volume.\")\n",
    "display(df_normalized.head())\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 5. Calculate Thematic and Composite Risk Indices ---\n",
    "print(\"--- Step 5: Constructing Risk Indices ---\")\n",
    "# The 'normalized_risk' is already our Thematic Risk Index for each theme.\n",
    "# Now, we calculate the Composite Risk Index (CRI) by averaging themes per day/location.\n",
    "\n",
    "df_cri = df_normalized.groupby(['date', 'location_id', 'location_name_english'])['normalized_risk'].mean().reset_index(name='composite_risk_index')\n",
    "\n",
    "print(\"Calculated Composite Risk Index (CRI).\")\n",
    "display(df_cri.head())\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 6. Save the Final Time-Series Data ---\n",
    "print(\"--- Step 6: Saving Final Index Data ---\")\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, '04_feature')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Save the thematic and composite indices\n",
    "df_normalized.to_csv(os.path.join(OUTPUT_DIR, 'thematic_risk_indices.csv'), index=False)\n",
    "df_cri.to_csv(os.path.join(OUTPUT_DIR, 'composite_risk_index.csv'), index=False)\n",
    "\n",
    "print(f\"Final time-series data saved to: {OUTPUT_DIR}\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 7. Example Visualization ---\n",
    "print(\"--- Step 7: Example Visualization ---\")\n",
    "# Let's visualize the CRI for a specific location, e.g., Baghdad\n",
    "location_to_plot = 'Baghdad'\n",
    "df_plot = df_cri[df_cri['location_name_english'] == location_to_plot]\n",
    "\n",
    "if not df_plot.empty:\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(15, 7))\n",
    "    \n",
    "    ax.plot(df_plot['date'], df_plot['composite_risk_index'], marker='o', linestyle='-', label='Composite Risk Index')\n",
    "    ax.set_title(f\"Daily Composite Risk Index for {location_to_plot}\", fontsize=16)\n",
    "    ax.set_ylabel(\"Risk Index (Normalized Score)\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"No data found for location: {location_to_plot}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f304389f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualitative",
   "metadata": {},
   "source": [
    "# Part 2: Reflection\n",
    "\n",
    "Please outline (1) some of the limitations of your approach and (2) how you would tackle these if you had more time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49236dab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d211d3b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
