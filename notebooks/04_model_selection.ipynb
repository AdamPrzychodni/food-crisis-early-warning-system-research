{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading Geographically Filtered Data ---\n",
      "Geographically filtered data loaded successfully.\n",
      "  - Total relevant articles: 96,516 articles\n",
      "------------------------------ \n",
      "\n",
      "--- Step 2: Loading Risk Factors ---\n",
      "Loaded 167 English risk factors.\n",
      "------------------------------ \n",
      "\n",
      "--- Step 3: Initializing Models ---\n",
      "GPU found. Models will run on the GPU for maximum speed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main classifier initialized: MoritzLaurer/deberta-v3-xsmall-zeroshot-v1.1-all-33\n",
      "Fast pre-filtering model initialized: paraphrase-multilingual-MiniLM-L12-v2\n",
      "------------------------------ \n",
      "\n",
      "--- Step 4: Defining the Extraction Function ---\n",
      "Risk factor embeddings pre-computed.\n",
      "\n",
      "--- Step 5: Running on a SAMPLE of 10 Articles ---\n",
      "Processing a sample of 10 articles...\n",
      "\n",
      "Original sentence count: 1,294\n",
      "Pre-filtering sentences with threshold: 0.55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033732ebf1bc4d66b4613a8cd998bd45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced to 80 sentences after filtering.\n",
      "Running classifier with confidence threshold: 0.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17037f57335a485081ce19fc05de6ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample risk factor extraction complete.\n",
      "Found 188 potential risk mentions in the sample.\n",
      "------------------------------ \n",
      "\n",
      "--- Step 6: Refining Sample Results (Post-Processing) ---\n",
      "Original number of mentions: 188\n",
      "Refined to 45 high-confidence, unique mentions.\n",
      "------------------------------ \n",
      "\n",
      "--- Step 7: Saving SAMPLE Results ---\n",
      "Successfully saved 45 sample risk mentions to: ../data/03_models/risk_mentions_SAMPLE_FINAL.csv\n",
      "------------------------------ \n",
      "\n",
      "--- Final Extracted Risk Factors (from Sample) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>date</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>risk_factor</th>\n",
       "      <th>confidence_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>6</td>\n",
       "      <td>2024-07-15</td>\n",
       "      <td>A progressive U.S.</td>\n",
       "      <td>rise</td>\n",
       "      <td>0.950273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6</td>\n",
       "      <td>2024-07-15</td>\n",
       "      <td>And the Middle East presents a special challen...</td>\n",
       "      <td>conflict</td>\n",
       "      <td>0.993686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>6</td>\n",
       "      <td>2024-07-15</td>\n",
       "      <td>Create direct American diplomatic channels wit...</td>\n",
       "      <td>conflict</td>\n",
       "      <td>0.996039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>8</td>\n",
       "      <td>2024-07-03</td>\n",
       "      <td>Even maverick figures such as Muqtada al-Sadr ...</td>\n",
       "      <td>conflict</td>\n",
       "      <td>0.997864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>6</td>\n",
       "      <td>2024-07-15</td>\n",
       "      <td>Fund an international climate finance agency.</td>\n",
       "      <td>call for donations</td>\n",
       "      <td>0.986623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     article_id        date  \\\n",
       "87            6  2024-07-15   \n",
       "136           6  2024-07-15   \n",
       "101           6  2024-07-15   \n",
       "172           8  2024-07-03   \n",
       "153           6  2024-07-15   \n",
       "\n",
       "                                         sentence_text         risk_factor  \\\n",
       "87                                  A progressive U.S.                rise   \n",
       "136  And the Middle East presents a special challen...            conflict   \n",
       "101  Create direct American diplomatic channels wit...            conflict   \n",
       "172  Even maverick figures such as Muqtada al-Sadr ...            conflict   \n",
       "153      Fund an international climate finance agency.  call for donations   \n",
       "\n",
       "     confidence_score  \n",
       "87           0.950273  \n",
       "136          0.993686  \n",
       "101          0.996039  \n",
       "172          0.997864  \n",
       "153          0.986623  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- 1. Load GEOGRAPHICALLY FILTERED Data ---\n",
    "print(\"--- Step 1: Loading Geographically Filtered Data ---\")\n",
    "DATA_DIR = '../data'\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, '02_processed')\n",
    "\n",
    "filtered_data_path = os.path.join(PROCESSED_DATA_DIR, 'news_geographically_filtered.pkl')\n",
    "df_filtered = pd.read_pickle(filtered_data_path)\n",
    "\n",
    "print(\"Geographically filtered data loaded successfully.\")\n",
    "print(f\"  - Total relevant articles: {len(df_filtered):,} articles\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 2. Load Risk Factors ---\n",
    "print(\"--- Step 2: Loading Risk Factors ---\")\n",
    "risk_factors_path = os.path.join(DATA_DIR, '01_raw/risk-factors.xlsx')\n",
    "df_risk_factors_eng = pd.read_excel(risk_factors_path)\n",
    "df_risk_factors_eng.dropna(subset=['risk_factor_english'], inplace=True)\n",
    "risk_factor_labels = df_risk_factors_eng['risk_factor_english'].tolist()\n",
    "print(f\"Loaded {len(risk_factor_labels)} English risk factors.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 3. Initialize Models ---\n",
    "print(\"--- Step 3: Initializing Models ---\")\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "if device == 0:\n",
    "    print(\"GPU found. Models will run on the GPU for maximum speed.\")\n",
    "else:\n",
    "    print(\"No GPU found. Models will run on the CPU.\")\n",
    "\n",
    "MODEL_NAME = 'MoritzLaurer/deberta-v3-xsmall-zeroshot-v1.1-all-33'\n",
    "classifier = pipeline(\"zero-shot-classification\", model=MODEL_NAME, device=device)\n",
    "print(f\"Main classifier initialized: {MODEL_NAME}\")\n",
    "\n",
    "FAST_MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "fast_embedder = SentenceTransformer(FAST_MODEL_NAME, device=device)\n",
    "print(f\"Fast pre-filtering model initialized: {FAST_MODEL_NAME}\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 4. Define the Extraction Function ---\n",
    "# (No changes needed in this section)\n",
    "print(\"--- Step 4: Defining the Extraction Function ---\")\n",
    "risk_factor_embeddings = fast_embedder.encode(risk_factor_labels, convert_to_tensor=True)\n",
    "print(\"Risk factor embeddings pre-computed.\")\n",
    "\n",
    "def extract_risk_factors_optimized(\n",
    "    df, classifier, labels, threshold, batch_size,\n",
    "    sentence_embedder, risk_factor_embeddings, sentence_similarity_threshold\n",
    "):\n",
    "    if 'article_id' not in df.columns:\n",
    "        df['article_id'] = df.index\n",
    "    df_sentences = df.explode('sentences').rename(columns={'sentences': 'sentence_text'})\n",
    "    df_sentences = df_sentences[['article_id', 'date', 'sentence_text']].dropna(subset=['sentence_text'])\n",
    "    all_sentences = df_sentences['sentence_text'].tolist()\n",
    "    if not all_sentences: return pd.DataFrame()\n",
    "\n",
    "    print(f\"\\nOriginal sentence count: {len(all_sentences):,}\")\n",
    "    print(f\"Pre-filtering sentences with threshold: {sentence_similarity_threshold}\")\n",
    "    sentence_embeddings = sentence_embedder.encode(all_sentences, convert_to_tensor=True, show_progress_bar=True)\n",
    "    hits = util.semantic_search(sentence_embeddings, risk_factor_embeddings, top_k=1)\n",
    "    relevant_indices = [i for i, hit_list in enumerate(hits) if hit_list and hit_list[0]['score'] >= sentence_similarity_threshold]\n",
    "    filtered_sentences_df = df_sentences.iloc[relevant_indices]\n",
    "    sentence_list = filtered_sentences_df['sentence_text'].tolist()\n",
    "    if not sentence_list: return pd.DataFrame()\n",
    "\n",
    "    print(f\"Reduced to {len(sentence_list):,} sentences after filtering.\")\n",
    "    print(f\"Running classifier with confidence threshold: {threshold}\")\n",
    "    results_list = []\n",
    "    for i, result in tqdm(enumerate(classifier(sentence_list, labels, multi_label=True, batch_size=batch_size)), total=len(sentence_list)):\n",
    "        for label, score in zip(result['labels'], result['scores']):\n",
    "            if score >= threshold:\n",
    "                original_row = filtered_sentences_df.iloc[i]\n",
    "                results_list.append({\n",
    "                    'article_id': original_row['article_id'],\n",
    "                    'date': original_row['date'],\n",
    "                    'sentence_text': result['sequence'],\n",
    "                    'risk_factor': label,\n",
    "                    'confidence_score': score\n",
    "                })\n",
    "    return pd.DataFrame(results_list)\n",
    "\n",
    "# --- 5. Set Parameters and Run on a SAMPLE (CHANGED) ---\n",
    "print(\"\\n--- Step 5: Running on a SAMPLE of 10 Articles ---\")\n",
    "CLASSIFIER_BATCH_SIZE = 128\n",
    "SENTENCE_SIMILARITY_THRESHOLD = 0.55\n",
    "CLASSIFIER_CONFIDENCE_THRESHOLD = 0.90\n",
    "\n",
    "# Create a small sample to test the pipeline\n",
    "df_sample = df_filtered.head(10).copy()\n",
    "\n",
    "print(f\"Processing a sample of {len(df_sample):,} articles...\")\n",
    "\n",
    "all_risk_mentions = extract_risk_factors_optimized(\n",
    "    df_sample,  # Use the sample DataFrame\n",
    "    classifier,\n",
    "    risk_factor_labels,\n",
    "    threshold=CLASSIFIER_CONFIDENCE_THRESHOLD,\n",
    "    batch_size=CLASSIFIER_BATCH_SIZE,\n",
    "    sentence_embedder=fast_embedder,\n",
    "    risk_factor_embeddings=risk_factor_embeddings,\n",
    "    sentence_similarity_threshold=SENTENCE_SIMILARITY_THRESHOLD\n",
    ")\n",
    "print(\"\\nSample risk factor extraction complete.\")\n",
    "print(f\"Found {len(all_risk_mentions):,} potential risk mentions in the sample.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 6. Post-Processing: Keep Only the Top Label per Sentence ---\n",
    "print(\"--- Step 6: Refining Sample Results (Post-Processing) ---\")\n",
    "if not all_risk_mentions.empty:\n",
    "    print(f\"Original number of mentions: {len(all_risk_mentions):,}\")\n",
    "    idx = all_risk_mentions.groupby('sentence_text')['confidence_score'].idxmax()\n",
    "    all_risk_mentions_refined = all_risk_mentions.loc[idx]\n",
    "    print(f\"Refined to {len(all_risk_mentions_refined):,} high-confidence, unique mentions.\")\n",
    "else:\n",
    "    all_risk_mentions_refined = pd.DataFrame()\n",
    "    print(\"No risk mentions found to refine.\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "\n",
    "# --- 7. Save the SAMPLE Results (CHANGED) ---\n",
    "print(\"--- Step 7: Saving SAMPLE Results ---\")\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, '03_models')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Save the sample results to a separate file\n",
    "output_path = os.path.join(OUTPUT_DIR, 'risk_mentions_SAMPLE_FINAL.csv')\n",
    "all_risk_mentions_refined.to_csv(output_path, index=False)\n",
    "print(f\"Successfully saved {len(all_risk_mentions_refined):,} sample risk mentions to: {output_path}\")\n",
    "print(\"-\" * 30, \"\\n\")\n",
    "\n",
    "print(\"--- Final Extracted Risk Factors (from Sample) ---\")\n",
    "if all_risk_mentions_refined.empty:\n",
    "    print(\"No risk factors found with the current settings.\")\n",
    "else:\n",
    "    display(all_risk_mentions_refined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
